/**
 * Service RAG Pipeline - Types publics et pipeline de gÃ©nÃ©ration LLM
 *
 * Ce module gÃ¨re:
 * 1. Les types publics exportÃ©s (ChatSource, ChatResponse, ChatOptions, ConversationMessage, StreamChunk)
 * 2. Les clients LLM (Ollama, Anthropic, Groq, DeepSeek)
 * 3. La fonction principale answerQuestion()
 * 4. La fonction de streaming answerQuestionStream()
 */

import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'
import { db } from '@/lib/db/postgres'
import {
  aiConfig,
  SYSTEM_PROMPTS,
  isChatEnabled,
  getChatProvider,
} from './config'
import {
  getSystemPromptForContext,
  PROMPT_CONFIG,
  type PromptContextType,
  type SupportedLanguage,
  type LegalStance,
} from './legal-reasoning-prompts'
import { detectLanguage } from './language-utils'
import {
  validateCitationFirst,
  enforceCitationFirst,
  CITATION_FIRST_SYSTEM_PROMPT,
} from './citation-first-enforcer'
import {
  getConversationContext,
  triggerSummaryGenerationIfNeeded,
  SUMMARY_CONFIG,
} from './conversation-summary-service'
import { RAGLogger } from '@/lib/logging/rag-logger'
import { createLogger } from '@/lib/logger'
import {
  callLLMWithFallback,
  callLLMStream,
  LLMMessage,
  LLMResponse,
  type StreamTokenUsage,
} from './llm-fallback-service'
import { type OperationName, getOperationProvider, getOperationModel } from './operations-config'
import {
  validateArticleCitations,
  formatValidationWarnings,
  verifyClaimSourceAlignment,
  verifyBranchAlignment,
} from './citation-validator-service'
import {
  detectAbrogatedReferences,
  formatAbrogationWarnings,
  type AbrogationWarning,
} from './abrogation-detector-service'
import { recordRAGMetric } from '@/lib/metrics/rag-metrics'
import { buildContextFromSources, sanitizeCitations, computeSourceQualityMetrics } from './rag-context-builder'
import {
  searchRelevantContext,
  searchRelevantContextBilingual,
  ENABLE_QUERY_EXPANSION,
  BILINGUAL_SEARCH_TIMEOUT_MS,
  type SearchResult,
  type ChatSource,
  type ChatOptions,
  type ConversationMessage,
} from './rag-search-service'

const log = createLogger('RAG')

// =============================================================================
// CLIENTS LLM (Ollama prioritaire, puis Groq, puis Anthropic)
// =============================================================================

let anthropicClient: Anthropic | null = null
let groqClient: OpenAI | null = null
let ollamaClient: OpenAI | null = null
let deepseekClient: OpenAI | null = null

function getOllamaClient(): OpenAI {
  if (!ollamaClient) {
    ollamaClient = new OpenAI({
      apiKey: 'ollama', // Ollama n'a pas besoin de clÃ©
      baseURL: `${aiConfig.ollama.baseUrl}/v1`,
      timeout: 120000,
    })
  }
  return ollamaClient
}

function getAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    if (!aiConfig.anthropic.apiKey) {
      throw new Error('ANTHROPIC_API_KEY non configurÃ©')
    }
    anthropicClient = new Anthropic({ apiKey: aiConfig.anthropic.apiKey })
  }
  return anthropicClient
}

function getGroqClient(): OpenAI {
  if (!groqClient) {
    if (!aiConfig.groq.apiKey) {
      throw new Error('GROQ_API_KEY non configurÃ©')
    }
    groqClient = new OpenAI({
      apiKey: aiConfig.groq.apiKey,
      baseURL: aiConfig.groq.baseUrl,
    })
  }
  return groqClient
}

function getDeepSeekClient(): OpenAI {
  if (!deepseekClient) {
    if (!aiConfig.deepseek.apiKey) {
      throw new Error('DEEPSEEK_API_KEY non configurÃ©')
    }
    deepseekClient = new OpenAI({
      apiKey: aiConfig.deepseek.apiKey,
      baseURL: aiConfig.deepseek.baseUrl,
    })
  }
  return deepseekClient
}

// =============================================================================
// TYPES PROPRES AU PIPELINE
// =============================================================================

export interface ChatResponse {
  answer: string
  sources: ChatSource[]
  tokensUsed: {
    input: number
    output: number
    total: number
  }
  model: string
  conversationId?: string
  citationWarnings?: string[] // Phase 2.2 - Citations non vÃ©rifiÃ©es
  abrogationWarnings?: import('./abrogation-detector-service').AbrogationWarning[] // Phase 2.3 - Lois abrogÃ©es
  qualityIndicator?: 'high' | 'medium' | 'low'
  averageSimilarity?: number
  abstentionReason?: string // Sprint 1 B1 - Raison de l'abstention si sources insuffisantes
  /** Sprint 3 RAG Audit-Proof : true si la rÃ©ponse a Ã©tÃ© rÃ©gÃ©nÃ©rÃ©e aprÃ¨s dÃ©tection cross-domaine */
  wasRegenerated?: boolean
  /** Sprint 3 : statut de validation des sources aprÃ¨s gÃ©nÃ©ration */
  validationStatus?: 'passed' | 'regenerated' | 'insufficient_sources'
}

// Templates bilingues pour le message utilisateur (utilisÃ© dans answerQuestion et answerQuestionStream)
const USER_MESSAGE_TEMPLATES = {
  ar: {
    prefix: 'ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø±Ø¬Ø¹ÙŠØ©:',
    questionLabel: 'Ø§Ù„Ø³Ø¤Ø§Ù„:',
    analysisHint: 'ØªØ¹Ù„ÙŠÙ…Ø§Øª: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù† ÙƒÙ„ ÙØµÙ„ØŒ Ø­Ø¯Ù‘Ø¯ Ø§Ù„Ø¢Ø¬Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ ÙˆØ§Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.',
    followUpHint: 'Ù…ØªØ§Ø¨Ø¹Ø©: Ù„Ù‚Ø¯ Ø£Ø¬Ø¨Øª Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©. Ù„Ø§ ØªÙƒØ±Ø± Ù…Ø§ Ø³Ø¨Ù‚ Ø°ÙƒØ±Ù‡. Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø£Ùˆ Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ø®ØªØµØ±.',
  },
  fr: {
    prefix: 'Documents du dossier:',
    questionLabel: 'Question:',
    analysisHint: 'Instructions: extraire les conditions lÃ©gales de chaque article, identifier les dÃ©lais et procÃ©dures, relier les textes entre eux.',
    followUpHint: 'SUIVI : Tu as dÃ©jÃ  rÃ©pondu aux questions prÃ©cÃ©dentes dans cette conversation. NE PAS RÃ‰PÃ‰TER ce qui a dÃ©jÃ  Ã©tÃ© expliquÃ©. RÃ©pondre UNIQUEMENT Ã  ce qui est nouveau ou spÃ©cifiquement demandÃ© dans cette question, de faÃ§on directe et ciblÃ©e.',
  },
}

// =============================================================================
// HELPERS CONVERSATION
// =============================================================================

/**
 * RÃ©cupÃ¨re l'historique de conversation pour le contexte (version simple)
 */
async function getConversationHistory(
  conversationId: string,
  limit: number = 10
): Promise<ConversationMessage[]> {
  const result = await db.query(
    `SELECT role, content
     FROM chat_messages
     WHERE conversation_id = $1
       AND role IN ('user', 'assistant')
     ORDER BY created_at DESC
     LIMIT $2`,
    [conversationId, limit]
  )

  // Inverser pour avoir l'ordre chronologique
  return result.rows
    .reverse()
    .map((row) => ({
      role: row.role as 'user' | 'assistant',
      content: row.content,
    }))
}

/**
 * RÃ©cupÃ¨re l'historique de conversation avec rÃ©sumÃ© si disponible
 * Retourne le rÃ©sumÃ© + les messages rÃ©cents pour un contexte optimal
 */
async function getConversationHistoryWithSummary(
  conversationId: string,
  recentLimit: number = SUMMARY_CONFIG.recentMessagesLimit
): Promise<{
  summary: string | null
  messages: ConversationMessage[]
  totalCount: number
}> {
  const context = await getConversationContext(conversationId, recentLimit)

  return {
    summary: context.summary,
    messages: context.messages.map((m) => ({
      role: m.role,
      content: m.content,
    })),
    totalCount: context.totalCount,
  }
}

// =============================================================================
// STREAMING NATIF GEMINI
// =============================================================================

/**
 * Ã‰vÃ©nements Ã©mis par answerQuestionStream()
 */
export type StreamChunk =
  | { type: 'metadata'; sources: ChatSource[]; model: string; qualityIndicator: 'high' | 'medium' | 'low'; averageSimilarity: number }
  | { type: 'chunk'; text: string }
  | { type: 'done'; tokensUsed: { input: number; output: number; total: number } }
  | { type: 'error'; message: string }
  | { type: 'progress'; step: 'searching' | 'sources_found' | 'generating'; count?: number; avgSimilarity?: number; quality?: string }

// =============================================================================
// FONCTION PRINCIPALE: RÃ‰PONDRE Ã€ UNE QUESTION
// =============================================================================

/**
 * RÃ©pond Ã  une question en utilisant le pipeline RAG complet
 */
export async function answerQuestion(
  question: string,
  userId: string,
  options: ChatOptions = {}
): Promise<ChatResponse> {
  const startTotal = Date.now()

  // Initialiser logger structurÃ© (ou utiliser celui fourni)
  const logger = options.logger || new RAGLogger(undefined, { userId, operation: options.operationName })
  logger.addContext('question', question.substring(0, 100)) // Truncate pour Ã©viter logs massifs
  logger.info('search', 'Pipeline RAG dÃ©marrÃ©', {
    enableExpansion: ENABLE_QUERY_EXPANSION,
    operationName: options.operationName,
  })

  if (!isChatEnabled()) {
    logger.error('search', 'Chat IA dÃ©sactivÃ©')
    throw new Error('Chat IA dÃ©sactivÃ© (activer OLLAMA_ENABLED ou configurer GROQ_API_KEY)')
  }

  const provider = getChatProvider()
  logger.addContext('provider', provider)

  // MÃ©triques RAG
  let searchTimeMs = 0
  let cacheHit = false

  // 1. Rechercher le contexte pertinent (bilingue si activÃ©) avec fallback dÃ©gradÃ©
  let sources: ChatSource[] = []
  let isDegradedMode = false

  let lastSearchResult: SearchResult | null = null
  const startSearch = Date.now()
  try {
    lastSearchResult = ENABLE_QUERY_EXPANSION
      ? await searchRelevantContextBilingual(question, userId, options)
      : await searchRelevantContext(question, userId, options)
    sources = lastSearchResult.sources
    cacheHit = lastSearchResult.cacheHit
    searchTimeMs = Date.now() - startSearch
  } catch (error) {
    // Mode dÃ©gradÃ©: retourner une erreur claire au lieu de continuer sans contexte
    // Ã‰vite les hallucinations juridiques en mode sans source
    logger.error('search', 'Erreur recherche contexte - Sources indisponibles', error)
    isDegradedMode = true
    sources = []
    searchTimeMs = Date.now() - startSearch
  }

  // 2. Si la recherche a rÃ©ussi mais n'a trouvÃ© aucune source pertinente,
  // retourner un message clair au lieu d'appeler le LLM (Ã©vite les hallucinations)
  if (!isDegradedMode && sources.length === 0) {
    const noSourcesLang = detectLanguage(question)
    log.warn(`[RAG Diagnostic] ğŸ” Aucune source trouvÃ©e pour requÃªte:`, {
      queryLength: question.length,
      queryWords: question.trim().split(/\s+/).length,
      language: noSourcesLang,
      condensationOccurred: lastSearchResult?.embeddingQuestion !== undefined && lastSearchResult.embeddingQuestion !== question,
      condensedQuery: lastSearchResult?.embeddingQuestion?.substring(0, 100),
      failureReason: lastSearchResult?.reason || 'unknown',
      queryPreview: question.substring(0, 100) + (question.length > 100 ? '...' : ''),
      searchTimeMs,
      enableExpansion: ENABLE_QUERY_EXPANSION,
    })
    const noSourcesMessage = noSourcesLang === 'fr'
      ? 'Ma base de connaissances ne contient pas de rÃ©fÃ©rences directement applicables Ã  cette question. Je vous oriente vers les textes officiels publiÃ©s au JORT ou vers un confrÃ¨re spÃ©cialisÃ© dans ce domaine.'
      : 'Ù„Ø§ ØªØªÙˆÙØ± Ù„Ø¯ÙŠÙ‘ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù†ØµÙˆØµ Ø£Ùˆ Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©. Ø£Ù†ØµØ­Ùƒ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØ´Ø±ÙŠØ¹Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„ØµØ§Ø¯Ø±Ø© ÙÙŠ Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠØŒ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.'

    return {
      answer: noSourcesMessage,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'none',
      conversationId: options.conversationId,
    }
  }

  // 3. Construire le contexte (bloquer si mode dÃ©gradÃ© pour Ã©viter les hallucinations)
  if (isDegradedMode) {
    // Enregistrer la mÃ©trique d'erreur
    const totalTimeMs = Date.now() - startTotal
    recordRAGMetric({
      searchTimeMs,
      llmTimeMs: 0,
      totalTimeMs,
      inputTokens: 0,
      outputTokens: 0,
      resultsCount: 0,
      cacheHit: false,
      degradedMode: true,
      provider: provider || 'unknown',
      error: 'Sources indisponibles - mode dÃ©gradÃ© bloquÃ©',
    })

    // Retourner un message explicite au lieu de throw (Ã©vite 500 error)
    const degradedLang = detectLanguage(question)
    const degradedMessage = degradedLang === 'fr'
      ? 'Les sources juridiques sont temporairement indisponibles. Veuillez rÃ©essayer dans quelques instants.'
      : 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù…Ø¤Ù‚ØªÙ‹Ø§. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„.'

    return {
      answer: degradedMessage,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'degraded',
      conversationId: options.conversationId,
    }
  }

  // DÃ©tecter la langue de la question pour adapter les labels du contexte
  const questionLang = detectLanguage(question)
  const context = await buildContextFromSources(sources, questionLang)

  // Calculer mÃ©triques qualitÃ© et injecter avertissement si nÃ©cessaire
  const qualityMetrics = computeSourceQualityMetrics(sources)

  // B1: Abstention progressive â€” quality gate Ã  3 niveaux (zone grise 0.30-0.40)
  // < 0.30 â†’ abstention directe (sources non pertinentes)
  // 0.30-0.40 â†’ zone grise : acceptÃ© si â‰¥2 sources, sinon abstention
  // â‰¥ 0.40 â†’ acceptÃ© (comportement standard)
  const avg = qualityMetrics.averageSimilarity
  const isHardAbstention = avg < 0.30
  const isGreyZone = avg >= 0.30 && avg < 0.40 && qualityMetrics.qualityLevel === 'low'
  const isGreyZoneAbstention = isGreyZone && sources.length < 2

  if (isHardAbstention || isGreyZoneAbstention) {
    const abstentionReason = isHardAbstention
      ? `SimilaritÃ© ${Math.round(avg * 100)}% < 30% (sources non pertinentes)`
      : `Zone grise: similaritÃ© ${Math.round(avg * 100)}% (30-40%) avec seulement ${sources.length} source(s)`
    log.info(`[RAG] Abstention: ${abstentionReason}`)
    const abstentionMsg = questionLang === 'fr'
      ? 'Les documents disponibles ne traitent pas cette problÃ©matique avec suffisamment de prÃ©cision pour formuler un avis juridique fiable. Je vous recommande de consulter directement les textes lÃ©gislatifs applicables ou un confrÃ¨re spÃ©cialisÃ© dans ce domaine.'
      : 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ø§ ØªØ¹Ø§Ù„Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ Ù„Ø¥Ø¨Ø¯Ø§Ø¡ Ø±Ø£ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ÙˆØ«ÙˆÙ‚. Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØ´Ø±ÙŠØ¹ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©ØŒ Ø£Ùˆ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ Ù„Ù„ØªØ¹Ù…Ù‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©.'
    return {
      answer: abstentionMsg,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'abstained',
      conversationId: options.conversationId,
      qualityIndicator: 'low',
      averageSimilarity: avg,
      abstentionReason,
    }
  }

  // Zone grise acceptÃ©e (0.30-0.40 avec â‰¥2 sources) â†’ log avertissement
  if (isGreyZone) {
    log.info(`[RAG] Zone grise acceptÃ©e: similaritÃ© ${Math.round(avg * 100)}%, ${sources.length} sources â€” rÃ©ponse avec avertissement`)
  }

  let contextWithWarning = context
  if (qualityMetrics.warningMessage) {
    contextWithWarning = `${qualityMetrics.warningMessage}\n\n---\n\n${context}`
    logger.warn('search', 'Low quality sources', {
      averageSimilarity: qualityMetrics.averageSimilarity,
      qualityLevel: qualityMetrics.qualityLevel,
    })
  }

  // Phase 3b: Avertissement conditionnel si domaine principal non couvert par les sources
  // DÃ©tection rapide basÃ©e sur les catÃ©gories des sources (pas d'appel LLM supplÃ©mentaire)
  const sourceCategories = new Set(
    sources.map(s => (s.metadata as Record<string, unknown>)?.category).filter(Boolean) as string[]
  )
  // Si toutes les sources proviennent d'un seul domaine et que la qualitÃ© est moyenne/basse,
  // injecter un avertissement pour dÃ©clencher le raisonnement conditionnel
  if (sources.length > 0 && qualityMetrics.qualityLevel !== 'high' && sourceCategories.size <= 1) {
    contextWithWarning = `âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù…Ø­Ø¯ÙˆØ¯Ø© Ø§Ù„Ù†Ø·Ø§Ù‚. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ· ÙˆÙ‚Ø¯Ù‘Ù… Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø¨Ø¯ÙŠÙ„Ø© Ø¥Ù† Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±.\n\n${contextWithWarning}`
  }

  // 3b. Multi-Chain Reasoning (optionnel â€” activÃ© via ENABLE_MULTI_CHAIN_CONSULTATION=true)
  // DÃ©clenchÃ© uniquement pour les consultations formelles avec suffisamment de sources
  if (
    process.env.ENABLE_MULTI_CHAIN_CONSULTATION === 'true' &&
    options.operationName === 'dossiers-consultation' &&
    sources.length >= 3
  ) {
    try {
      const { multiChainReasoning } = await import('./multi-chain-legal-reasoning')
      const multiChainSources = sources.map((s) => ({
        id: s.documentId,
        content: s.chunkContent,
        category: (s.metadata?.category as string) || 'autre',
        metadata: s.metadata as Parameters<typeof multiChainReasoning>[0]['sources'][0]['metadata'],
      }))
      const mcResult = await multiChainReasoning({
        question,
        sources: multiChainSources,
        language: questionLang === 'fr' ? 'fr' : 'ar',
        usePremiumModel: options.usePremiumModel ?? false,
      })
      // PrÃ©fixer le contexte RAG avec l'analyse multi-chain
      contextWithWarning = `## Analyse Multi-Chain (Raisonnement juridique structurÃ©)\n\n${mcResult.finalResponse}\n\n---\n\n${contextWithWarning}`
      logger.info('search', '[MultiChain] Raisonnement multi-chain intÃ©grÃ©', {
        confidence: mcResult.overallConfidence,
        durationMs: mcResult.totalDurationMs,
        chains: mcResult.metadata.chainsExecuted,
      })
    } catch (mcError) {
      // Non-bloquant : si le multi-chain Ã©choue, on continue sans lui
      log.error('[MultiChain] Erreur (non-bloquant):', mcError instanceof Error ? mcError.message : mcError)
    }
  }

  // 3. RÃ©cupÃ©rer l'historique avec rÃ©sumÃ© si conversation existante
  let conversationHistory: ConversationMessage[] = []
  let conversationSummary: string | null = null
  let totalMessageCount = 0

  if (options.conversationId) {
    const historyContext = await getConversationHistoryWithSummary(
      options.conversationId,
      SUMMARY_CONFIG.recentMessagesLimit
    )
    conversationHistory = historyContext.messages
    conversationSummary = historyContext.summary
    totalMessageCount = historyContext.totalCount
  }

  // SÃ©lectionner le prompt systÃ¨me appropriÃ© selon le contexte
  // Par dÃ©faut: 'chat' si conversation, 'consultation' sinon
  const contextType: PromptContextType = options.contextType || (options.conversationId ? 'chat' : 'consultation')
  const supportedLang: SupportedLanguage = questionLang === 'fr' ? 'fr' : 'ar'
  const stance = options.stance ?? 'defense'
  const baseSystemPrompt = getSystemPromptForContext(contextType, supportedLang, stance)

  // 4. Construire les messages (format OpenAI-compatible pour Ollama/Groq)
  const messagesOpenAI: Array<{ role: 'user' | 'assistant' | 'system'; content: string }> = []

  // Injecter le rÃ©sumÃ© de la conversation si disponible (pour Ollama/Groq)
  if (conversationSummary) {
    messagesOpenAI.push({
      role: 'system',
      content: `[RÃ©sumÃ© de la conversation prÃ©cÃ©dente]\n${conversationSummary}`,
    })
  }

  // Ajouter l'historique de conversation rÃ©cent
  for (const msg of conversationHistory) {
    messagesOpenAI.push({ role: msg.role, content: msg.content })
  }

  // Ajouter la nouvelle question avec le contexte (template bilingue)
  const msgTemplate = USER_MESSAGE_TEMPLATES[supportedLang]
  // DÃ©tecter si c'est un follow-up (au moins 1 Ã©change Q+R prÃ©cÃ©dent)
  const isFollowUp = contextType === 'chat' && conversationHistory.length >= 2
  const questionNumber = isFollowUp ? Math.floor(conversationHistory.length / 2) + 1 : null
  // Pour les follow-ups : instruction anti-rÃ©pÃ©tition. Pour Q1 : instruction d'analyse.
  let analysisLine = ''
  if (contextType === 'chat') {
    if (isFollowUp) {
      analysisLine = `\n${msgTemplate.followUpHint}\n`
    } else {
      analysisLine = `\n${msgTemplate.analysisHint}\n`
    }
  }
  const questionPrefix = questionNumber ? `[Question ${questionNumber}]\n` : ''
  messagesOpenAI.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${questionPrefix}${msgTemplate.questionLabel} ${question}`,
  })

  // Messages format Anthropic (sans 'system' dans les messages)
  const messagesAnthropic: Array<{ role: 'user' | 'assistant'; content: string }> = []
  for (const msg of conversationHistory) {
    messagesAnthropic.push({ role: msg.role, content: msg.content })
  }
  messagesAnthropic.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
  })

  // Log si rÃ©sumÃ© utilisÃ©
  if (conversationSummary) {
    log.info(`[RAG] Conversation ${options.conversationId}: rÃ©sumÃ© injectÃ© (${totalMessageCount} messages total)`)
  }

  // Construire le systÃ¨me prompt avec rÃ©sumÃ© pour Anthropic
  const systemPromptWithSummary = conversationSummary
    ? `${baseSystemPrompt}\n\n[RÃ©sumÃ© de la conversation prÃ©cÃ©dente]\n${conversationSummary}`
    : baseSystemPrompt

  log.info(`[RAG] Utilisation du prompt structurÃ©: contextType=${contextType}, langue=${supportedLang}`)

  let answer: string
  let tokensUsed: { input: number; output: number; total: number }
  let modelUsed: string
  let llmError: string | undefined
  let fallbackUsed = false

  // 5. Appeler le LLM avec fallback automatique sur erreur 429
  // Ollama est traitÃ© sÃ©parÃ©ment (local, pas de fallback cloud)
  // Pour les autres: Groq â†’ DeepSeek â†’ Anthropic â†’ OpenAI
  try {
    if (provider === 'ollama') {
      // Ollama (local, gratuit, illimitÃ©) - pas de fallback
      const client = getOllamaClient()

      // Adapter tempÃ©rature selon le contexte (consultation = plus prÃ©cis)
      const promptConfig = PROMPT_CONFIG[contextType]
      const temperature = options.temperature ?? promptConfig.temperature

      const response = await client.chat.completions.create({
        model: aiConfig.ollama.chatModelDefault,
        max_tokens: promptConfig.maxTokens,
        messages: [
          { role: 'system', content: baseSystemPrompt },
          ...messagesOpenAI,
        ],
        temperature,
      })

      answer = response.choices[0]?.message?.content || ''
      tokensUsed = {
        input: response.usage?.prompt_tokens || 0,
        output: response.usage?.completion_tokens || 0,
        total: response.usage?.total_tokens || 0,
      }
      modelUsed = `ollama/${aiConfig.ollama.chatModelDefault}`
    } else {
      // Utiliser le service de fallback pour les providers cloud
      // Convertir les messages au format LLMMessage
      const llmMessages: LLMMessage[] = messagesOpenAI.map((m) => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content,
      }))

      // Adapter tempÃ©rature et maxTokens selon le contexte
      const promptConfig = PROMPT_CONFIG[contextType]
      const temperature = options.temperature ?? promptConfig.temperature

      const llmResponse = await callLLMWithFallback(
        llmMessages,
        {
          temperature,
          maxTokens: promptConfig.maxTokens,
          systemPrompt: systemPromptWithSummary,
          context: 'rag-chat', // StratÃ©gie optimisÃ©e : Gemini â†’ DeepSeek â†’ Ollama
          operationName: options.operationName, // Configuration par opÃ©ration
        },
        options.usePremiumModel ?? false // Mode premium si demandÃ©
      )

      answer = llmResponse.answer
      tokensUsed = llmResponse.tokensUsed
      modelUsed = llmResponse.modelUsed
      fallbackUsed = llmResponse.fallbackUsed

      // âœ¨ PHASE 5: Citation-First Enforcement
      // Valider que la rÃ©ponse commence par une citation (seulement pour consultation, pas chat)
      if (contextType !== 'chat' && sources.length > 0) {
        const citationValidation = validateCitationFirst(answer)

        if (!citationValidation.valid) {
          log.warn(
            `[RAG] Citation-first violation detected: ${citationValidation.issue} ` +
            `(words before citation: ${citationValidation.metrics.wordsBeforeFirstCitation})`
          )

          // Conversion des sources au format attendu par l'enforcer
          const enforcerSources = sources.map((src, idx) => ({
            label: `[Source-${idx + 1}]`,
            content: src.chunkContent,
            title: src.documentName,
            category: src.metadata?.category as string | undefined,
          }))

          // Auto-correction
          const correctedAnswer = enforceCitationFirst(answer, enforcerSources)

          // VÃ©rifier si correction rÃ©ussie
          const correctedValidation = validateCitationFirst(correctedAnswer)

          if (correctedValidation.valid) {
            log.info(
              `[RAG] Citation-first enforced successfully ` +
              `(${citationValidation.issue} â†’ valid)`
            )
            answer = correctedAnswer
          } else {
            log.warn(
              `[RAG] Citation-first enforcement partial ` +
              `(issue: ${correctedValidation.issue})`
            )
            // Utiliser rÃ©ponse corrigÃ©e mÃªme si pas parfaite (mieux que rien)
            answer = correctedAnswer
          }
        } else {
          log.info(
            `[RAG] Citation-first validation passed ` +
            `(${citationValidation.metrics.totalCitations} citations, ` +
            `${citationValidation.metrics.wordsBeforeFirstCitation} words before first)`
          )
        }
      }

      // Log si fallback utilisÃ©
      if (fallbackUsed && llmResponse.originalProvider) {
        log.info(
          `[RAG] Fallback LLM activÃ©: ${llmResponse.originalProvider} â†’ ${llmResponse.provider}`
        )
      }
    }
  } catch (error) {
    // Enregistrer l'erreur LLM dans les mÃ©triques
    const totalTimeMs = Date.now() - startTotal
    const llmTimeMs = totalTimeMs - searchTimeMs
    llmError = `LLM error: ${error instanceof Error ? error.message : String(error)}`

    recordRAGMetric({
      searchTimeMs,
      llmTimeMs,
      totalTimeMs,
      inputTokens: 0,
      outputTokens: 0,
      resultsCount: sources.length,
      cacheHit,
      degradedMode: isDegradedMode,
      provider: provider || 'unknown',
      error: llmError,
    })

    logger.error('llm', 'Erreur LLM - Tous providers Ã©puisÃ©s', error)
    throw error // Re-throw pour que l'appelant puisse gÃ©rer
  }

  // DÃ©clencher gÃ©nÃ©ration de rÃ©sumÃ© en async si seuil atteint
  if (options.conversationId && totalMessageCount >= SUMMARY_CONFIG.triggerMessageCount) {
    triggerSummaryGenerationIfNeeded(options.conversationId).catch((err) =>
      logger.error('llm', 'Erreur trigger rÃ©sumÃ© conversation', err)
    )
  }

  // Logging mÃ©triques RAG structurÃ©
  const totalTimeMs = Date.now() - startTotal
  const llmTimeMs = totalTimeMs - searchTimeMs

  // Enregistrer dans le service de mÃ©triques
  recordRAGMetric({
    searchTimeMs,
    llmTimeMs,
    totalTimeMs,
    inputTokens: tokensUsed.input,
    outputTokens: tokensUsed.output,
    resultsCount: sources.length,
    cacheHit: cacheHit,
    degradedMode: isDegradedMode,
    provider: modelUsed,
  })

  log.info('RAG_METRICS', JSON.stringify({
    searchTimeMs,
    llmTimeMs,
    totalTimeMs,
    contextTokens: tokensUsed.input,
    outputTokens: tokensUsed.output,
    resultsCount: sources.length,
    degradedMode: isDegradedMode,
    provider: modelUsed,
    fallbackUsed,
    conversationId: options.conversationId || null,
    dossierId: options.dossierId || null,
  }))

  // Sanitizer: supprimer les citations inventÃ©es par le LLM
  answer = sanitizeCitations(answer, sources.length)

  // Phase 2.2+2.3 : Validation citations + dÃ©tection abrogations (parallÃ¨le)
  let citationWarnings: string[] = []
  let abrogationWarnings: AbrogationWarning[] = []

  const [citationResult, abrogationResult] = await Promise.all([
    (process.env.ENABLE_CITATION_VALIDATION !== 'false')
      ? Promise.resolve(validateArticleCitations(answer, sources)).catch((error) => {
          logger.error('filter', 'Erreur validation citations', error)
          return null
        })
      : Promise.resolve(null),
    (process.env.ENABLE_ABROGATION_DETECTION !== 'false')
      ? detectAbrogatedReferences(answer, sources).catch((error) => {
          logger.error('abrogation', 'Erreur dÃ©tection abrogations', error)
          return [] as AbrogationWarning[]
        })
      : Promise.resolve([] as AbrogationWarning[]),
  ])

  if (citationResult?.warnings?.length) {
    logger.warn('filter', 'Citations non vÃ©rifiÃ©es dÃ©tectÃ©es', {
      count: citationResult.warnings.length,
      warnings: formatValidationWarnings(citationResult),
    })
    citationWarnings = citationResult.warnings.map(w => w.citation)
  }

  // Fix Feb 24, 2026 : disclaimer si citation_accuracy < 0.5 (citations invalides > 50%)
  // Jusque-lÃ  la validation Ã©tait informative seulement â€” maintenant visible cÃ´tÃ© utilisateur
  if (citationResult && citationResult.totalCitations > 0) {
    const invalidCount = citationResult.invalidCitations.length
    const citationAccuracy = (citationResult.totalCitations - invalidCount) / citationResult.totalCitations
    if (citationAccuracy < 0.5) {
      const langForDisclaimer = detectLanguage(answer)
      const disclaimer = langForDisclaimer === 'ar'
        ? '\n\nâš ï¸ ØªÙ†Ø¨ÙŠÙ‡: ØªØ¹Ø°Ù‘Ø± Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. ÙŠÙØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚ØªÙ‡Ø§.'
        : '\n\nâš ï¸ Avertissement : certaines rÃ©fÃ©rences juridiques citÃ©es dans cette rÃ©ponse n\'ont pas pu Ãªtre vÃ©rifiÃ©es. Veuillez consulter les textes originaux pour confirmation.'
      answer += disclaimer
      logger.warn('filter', `Citation accuracy faible (${(citationAccuracy * 100).toFixed(0)}%) â€” disclaimer ajoutÃ©`, {
        totalCitations: citationResult.totalCitations,
        invalidCitations: invalidCount,
      })
    }
  }

  // Sprint 4: logger les sources sans citation_locator (non auditables)
  if (citationResult?.locatorsMissing && citationResult.locatorsMissing.length > 0) {
    logger.warn('filter', 'Sources sans citation_locator (non auditables)', {
      count: citationResult.locatorsMissing.length,
      sources: citationResult.locatorsMissing,
    })
  }

  abrogationWarnings = abrogationResult || []
  if (abrogationWarnings.length > 0) {
    logger.warn('abrogation', 'Lois abrogÃ©es dÃ©tectÃ©es dans la rÃ©ponse', {
      count: abrogationWarnings.length,
      warnings: formatAbrogationWarnings(abrogationWarnings),
    })
  }

  // Phase 4: Claim verification â€” vÃ©rifier alignement claimsâ†”sources
  if (process.env.ENABLE_CLAIM_VERIFICATION !== 'false') {
    try {
      const claimResult = verifyClaimSourceAlignment(answer, sources)
      if (claimResult.unsupportedClaims.length > 0) {
        const ratio = claimResult.totalClaims > 0
          ? claimResult.supportedClaims / claimResult.totalClaims
          : 1
        if (ratio < 0.7) {
          answer += '\n\nâš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ Ø¨Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªÙˆÙØ±Ø©. ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚.'
        }
        log.info(`[Claim Verify] ${claimResult.supportedClaims}/${claimResult.totalClaims} claims supportÃ©es`)
      }
    } catch (error) {
      log.error('[Claim Verify] Erreur:', error instanceof Error ? error.message : error)
    }
  }

  // Sprint 3 RAG Audit-Proof : dÃ©tection cross-domaine + rÃ©gÃ©nÃ©ration automatique (1 tentative)
  let wasRegenerated = false
  let validationStatus: 'passed' | 'regenerated' | 'insufficient_sources' = 'passed'

  if (process.env.ENABLE_BRANCH_REGENERATION !== 'false') {
    try {
      // Le router est dÃ©jÃ  appelÃ© dans searchRelevantContext â€” cache Redis hit ici (~0ms)
      const { routeQuery } = await import('./legal-router-service')
      const routerForValidation = await routeQuery(question, { maxTracks: 1 })
      const allowedBranches = routerForValidation.allowedBranches

      if (allowedBranches && allowedBranches.length > 0) {
        const branchCheck = verifyBranchAlignment(sources, allowedBranches)

        if (branchCheck.violatingCount > 0) {
          log.warn(
            `[RAG Sprint3] ${branchCheck.violatingCount}/${branchCheck.totalSources} sources hors-domaine dÃ©tectÃ©es:`,
            branchCheck.violatingSources.map(v => `${v.documentName} (branch=${v.branch})`).join(', ')
          )

          // Filtrer pour ne garder que sources dans le domaine autorisÃ©
          const alignedSources = sources.filter(s => {
            const branch = s.metadata?.branch as string | undefined
            if (!branch || branch === 'autre') return true // pas de branch = on garde
            return allowedBranches.includes(branch)
          })

          if (alignedSources.length >= 2) {
            // RÃ©gÃ©nÃ©rer avec sources filtrÃ©es (appel LLM synchrone, 1 tentative max)
            const filteredContext = await buildContextFromSources(alignedSources, questionLang)
            const regenMessages: LLMMessage[] = [
              {
                role: 'user',
                content: `${msgTemplate.prefix}\n\n[ØªÙ†Ø¨ÙŠÙ‡: ØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø³Ø¤Ø§Ù„]\n\n${filteredContext}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
              },
            ]
            const regenResponse = await callLLMWithFallback(regenMessages, {
              systemPrompt: systemPromptWithSummary,
              operationName: options.operationName,
            })
            answer = sanitizeCitations(regenResponse.answer, alignedSources.length)
            sources = alignedSources
            wasRegenerated = true
            validationStatus = 'regenerated'
            log.info('[RAG Sprint3] âœ… RÃ©ponse rÃ©gÃ©nÃ©rÃ©e avec sources filtrÃ©es par domaine')
          } else {
            validationStatus = 'insufficient_sources'
            log.warn('[RAG Sprint3] Sources filtrÃ©es insuffisantes (<2) â€” pas de rÃ©gÃ©nÃ©ration')
          }
        }
      }
    } catch (error) {
      // Non-bloquant : si la rÃ©gÃ©nÃ©ration Ã©choue, on garde la rÃ©ponse originale
      log.error('[RAG Sprint3] Erreur validation branche:', error instanceof Error ? error.message : error)
    }
  }

  // Log mÃ©triques finales du pipeline complet
  logger.metrics({
    totalTimeMs: Date.now() - startTotal,
    searchTimeMs,
    sourcesCount: sources.length,
    tokensInput: tokensUsed.input,
    tokensOutput: tokensUsed.output,
    tokensTotal: tokensUsed.total,
    model: modelUsed,
    cacheHit,
    degradedMode: isDegradedMode,
    citationWarnings: citationWarnings.length,
    abrogationWarnings: abrogationWarnings.length,
    requestId: logger.getRequestId(),
  })

  return {
    answer,
    sources,
    tokensUsed,
    model: modelUsed,
    conversationId: options.conversationId,
    citationWarnings: citationWarnings.length > 0 ? citationWarnings : undefined,
    abrogationWarnings: abrogationWarnings.length > 0 ? abrogationWarnings : undefined,
    qualityIndicator: qualityMetrics.qualityLevel,
    averageSimilarity: qualityMetrics.averageSimilarity,
    wasRegenerated: wasRegenerated || undefined,
    validationStatus: validationStatus !== 'passed' ? validationStatus : undefined,
  }
}

/**
 * RÃ©pond Ã  une question en streaming natif Gemini.
 *
 * Phase 1 : Pipeline RAG complet (non-streaming) â†’ sources + contexte
 * Phase 2 : callLLMStream() â†’ yield chunks texte en temps rÃ©el (Groq ou Gemini)
 * Phase 3 : Post-processing (sanitize citations, mÃ©triques)
 *
 * Format des Ã©vÃ©nements :
 * - 'metadata' â†’ sources trouvÃ©es, Ã  envoyer en premier au client
 * - 'chunk'    â†’ fragment de texte gÃ©nÃ©rÃ© par Gemini
 * - 'done'     â†’ fin du stream avec tokensUsed estimÃ©s
 * - 'error'    â†’ erreur fatale (rate limit, timeoutâ€¦)
 */
export async function* answerQuestionStream(
  question: string,
  userId: string,
  options: ChatOptions = {}
): AsyncGenerator<StreamChunk> {
  if (!isChatEnabled()) {
    yield { type: 'error', message: 'Chat IA dÃ©sactivÃ© (activer OLLAMA_ENABLED ou configurer GROQ_API_KEY)' }
    return
  }

  // 1. Phase RAG (non-streaming)
  yield { type: 'progress', step: 'searching' }
  let sources: ChatSource[] = []
  let streamSearchResult: SearchResult | null = null
  try {
    streamSearchResult = ENABLE_QUERY_EXPANSION
      ? await searchRelevantContextBilingual(question, userId, options)
      : await searchRelevantContext(question, userId, options)
    sources = streamSearchResult.sources
  } catch (error) {
    const errMsg = error instanceof Error ? error.message : 'Erreur recherche contexte'
    log.error('[RAG Stream] Erreur recherche:', errMsg)
    yield { type: 'error', message: errMsg }
    return
  }

  const questionLang = detectLanguage(question)

  // Aucune source â†’ rÃ©ponse rapide sans appel LLM
  if (sources.length === 0) {
    log.warn(`[RAG Diagnostic Stream] ğŸ” Aucune source trouvÃ©e pour requÃªte:`, {
      queryLength: question.length,
      queryWords: question.trim().split(/\s+/).length,
      language: questionLang,
      condensationOccurred: streamSearchResult?.embeddingQuestion !== undefined && streamSearchResult.embeddingQuestion !== question,
      condensedQuery: streamSearchResult?.embeddingQuestion?.substring(0, 100),
      failureReason: streamSearchResult?.reason || 'unknown',
      queryPreview: question.substring(0, 100) + (question.length > 100 ? '...' : ''),
    })
    const noSourcesMsg = questionLang === 'fr'
      ? 'Ma base de connaissances ne contient pas de rÃ©fÃ©rences directement applicables Ã  cette question. Je vous oriente vers les textes officiels publiÃ©s au JORT ou vers un confrÃ¨re spÃ©cialisÃ© dans ce domaine.'
      : 'Ù„Ø§ ØªØªÙˆÙØ± Ù„Ø¯ÙŠÙ‘ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù†ØµÙˆØµ Ø£Ùˆ Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©. Ø£Ù†ØµØ­Ùƒ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØ´Ø±ÙŠØ¹Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„ØµØ§Ø¯Ø±Ø© ÙÙŠ Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠØŒ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.'
    yield { type: 'metadata', sources: [], model: 'groq/llama-3.3-70b-versatile', qualityIndicator: 'low', averageSimilarity: 0 }
    yield { type: 'chunk', text: noSourcesMsg }
    yield { type: 'done', tokensUsed: { input: 0, output: 0, total: 0 } }
    return
  }

  // 2. Construire le contexte RAG
  const qualityMetrics = computeSourceQualityMetrics(sources)
  yield {
    type: 'progress',
    step: 'sources_found',
    count: sources.length,
    avgSimilarity: Math.round((qualityMetrics.averageSimilarity ?? 0) * 100),
    quality: qualityMetrics.qualityLevel,
  }
  const context = await buildContextFromSources(sources, questionLang)

  // B1: Abstention progressive en streaming â€” quality gate Ã  3 niveaux (zone grise 0.30-0.40)
  const streamAvg = qualityMetrics.averageSimilarity
  const streamIsHardAbstention = streamAvg < 0.30
  const streamIsGreyZone = streamAvg >= 0.30 && streamAvg < 0.40 && qualityMetrics.qualityLevel === 'low'
  const streamIsGreyZoneAbstention = streamIsGreyZone && sources.length < 2

  if (streamIsHardAbstention || streamIsGreyZoneAbstention) {
    const abstentionReason = streamIsHardAbstention
      ? `SimilaritÃ© ${Math.round(streamAvg * 100)}% < 30%`
      : `Zone grise: similaritÃ© ${Math.round(streamAvg * 100)}% avec ${sources.length} source(s)`
    log.info(`[RAG Stream] Abstention: ${abstentionReason}`)
    const abstentionMsg = questionLang === 'fr'
      ? 'Les documents disponibles ne traitent pas cette problÃ©matique avec suffisamment de prÃ©cision pour formuler un avis juridique fiable. Je vous recommande de consulter directement les textes lÃ©gislatifs applicables ou un confrÃ¨re spÃ©cialisÃ© dans ce domaine.'
      : 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ø§ ØªØ¹Ø§Ù„Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ Ù„Ø¥Ø¨Ø¯Ø§Ø¡ Ø±Ø£ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ÙˆØ«ÙˆÙ‚. Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØ´Ø±ÙŠØ¹ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©ØŒ Ø£Ùˆ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ Ù„Ù„ØªØ¹Ù…Ù‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©.'
    yield { type: 'metadata', sources: [], model: 'abstained', qualityIndicator: 'low', averageSimilarity: streamAvg }
    yield { type: 'chunk', text: abstentionMsg }
    yield { type: 'done', tokensUsed: { input: 0, output: 0, total: 0 } }
    return
  }

  if (streamIsGreyZone) {
    log.info(`[RAG Stream] Zone grise acceptÃ©e: similaritÃ© ${Math.round(streamAvg * 100)}%, ${sources.length} sources`)
  }

  const contextWithWarning = qualityMetrics.warningMessage
    ? `${qualityMetrics.warningMessage}\n\n---\n\n${context}`
    : context

  // 3. Historique conversation
  let conversationHistory: ConversationMessage[] = []
  let conversationSummary: string | null = null
  if (options.conversationId) {
    const historyContext = await getConversationHistoryWithSummary(
      options.conversationId,
      SUMMARY_CONFIG.recentMessagesLimit
    )
    conversationHistory = historyContext.messages
    conversationSummary = historyContext.summary
  }

  // 4. Construire messages
  const contextType: PromptContextType = options.contextType || (options.conversationId ? 'chat' : 'consultation')
  const supportedLang: SupportedLanguage = questionLang === 'fr' ? 'fr' : 'ar'
  const stance = options.stance ?? 'defense'
  const baseSystemPrompt = getSystemPromptForContext(contextType, supportedLang, stance)
  const systemPrompt = conversationSummary
    ? `${baseSystemPrompt}\n\n[RÃ©sumÃ© de la conversation prÃ©cÃ©dente]\n${conversationSummary}`
    : baseSystemPrompt

  const messagesForLLM: Array<{ role: string; content: string }> = []
  for (const msg of conversationHistory) {
    messagesForLLM.push({ role: msg.role, content: msg.content })
  }
  const msgTemplate = USER_MESSAGE_TEMPLATES[supportedLang]
  // DÃ©tecter si c'est un follow-up (au moins 1 Ã©change Q+R prÃ©cÃ©dent)
  const isFollowUp = contextType === 'chat' && conversationHistory.length >= 2
  const questionNumber = isFollowUp ? Math.floor(conversationHistory.length / 2) + 1 : null
  // Pour les follow-ups : instruction anti-rÃ©pÃ©tition. Pour Q1 : instruction d'analyse.
  let analysisLine = ''
  if (contextType === 'chat') {
    if (isFollowUp) {
      analysisLine = `\n${msgTemplate.followUpHint}\n`
    } else {
      analysisLine = `\n${msgTemplate.analysisHint}\n`
    }
  }
  const questionPrefix = questionNumber ? `[Question ${questionNumber}]\n` : ''
  messagesForLLM.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${questionPrefix}${msgTemplate.questionLabel} ${question}`,
  })

  // 5. Yield metadata (sources disponibles avant le stream LLM)
  const opName = options.operationName ?? 'assistant-ia'
  const streamProvider = getOperationProvider(opName)
  const streamModel = getOperationModel(opName)
  const modelName = `${streamProvider}/${streamModel}`
  yield {
    type: 'metadata',
    sources,
    model: modelName,
    qualityIndicator: qualityMetrics.qualityLevel,
    averageSimilarity: qualityMetrics.averageSimilarity,
  }

  // 6. Stream LLM â†’ yield chunks (Groq ou Gemini selon operations-config)
  yield { type: 'progress', step: 'generating' }
  const promptConfig = PROMPT_CONFIG[contextType]
  let fullText = ''
  const streamUsage: StreamTokenUsage = { input: 0, output: 0, total: 0 }
  try {
    const streamGen = callLLMStream(messagesForLLM, {
      temperature: options.temperature ?? promptConfig.temperature,
      maxTokens: promptConfig.maxTokens,
      operationName: options.operationName ?? 'assistant-ia',
      systemInstruction: systemPrompt,
    }, streamUsage)

    for await (const chunk of streamGen) {
      fullText += chunk
      yield { type: 'chunk', text: chunk }
    }
  } catch (error) {
    const errMsg = error instanceof Error ? error.message : 'Erreur streaming LLM'
    log.error('[RAG Stream] Erreur streaming:', errMsg)
    yield { type: 'error', message: errMsg }
    return
  }

  // 7. Post-processing : sanitize citations
  fullText = sanitizeCitations(fullText, sources.length)

  // Tokens : utiliser les stats rÃ©elles Groq si disponibles, sinon estimation
  const tokensUsed = streamUsage.total > 0
    ? streamUsage
    : { input: 0, output: Math.ceil(fullText.length / 4), total: Math.ceil(fullText.length / 4) }

  yield { type: 'done', tokensUsed }
}

/**
 * Service RAG Chat - Pipeline complet pour l'assistant juridique Qadhya
 *
 * Ce service orchestre:
 * 1. R√©cup√©ration du contexte (documents pertinents via recherche s√©mantique)
 * 2. Construction du prompt avec le contexte
 * 3. Appel √† Claude pour g√©n√©rer la r√©ponse
 * 4. Extraction et formatage des sources
 */

import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'
import { db } from '@/lib/db/postgres'
import {
  generateEmbedding,
  formatEmbeddingForPostgres,
} from './embeddings-service'
import {
  aiConfig,
  SYSTEM_PROMPTS,
  isChatEnabled,
  getChatProvider,
  RAG_THRESHOLDS,
  SOURCE_BOOST,
  RAG_DIVERSITY,
} from './config'
import {
  batchEnrichSourcesWithMetadata,
  type ChatSource as EnhancedChatSource,
} from './enhanced-rag-search-service'
import {
  getSystemPromptForContext,
  PROMPT_CONFIG,
  type PromptContextType,
  type SupportedLanguage,
  type LegalStance,
} from './legal-reasoning-prompts'
import { searchKnowledgeBase, searchKnowledgeBaseHybrid } from './knowledge-base-service'
import {
  getCachedSearchResults,
  setCachedSearchResults,
  SearchScope,
} from '@/lib/cache/search-cache'
import {
  detectLanguage,
  getOppositeLanguage,
  DetectedLanguage,
} from './language-utils'
import { translateQuery, isTranslationAvailable } from './translation-service'
import { filterAbrogatedSources } from './rag-abrogation-filter'
import {
  validateCitationFirst,
  enforceCitationFirst,
  CITATION_FIRST_SYSTEM_PROMPT,
} from './citation-first-enforcer'
import {
  getConversationContext,
  triggerSummaryGenerationIfNeeded,
  SUMMARY_CONFIG,
} from './conversation-summary-service'
import { RAGLogger } from '@/lib/logging/rag-logger'
import { countTokens } from './token-utils'
import { getDynamicBoostFactors } from './feedback-service'
import {
  rerankDocuments,
  combineScores,
  isRerankerEnabled,
  DocumentToRerank,
} from './reranker-service'
import { recordRAGMetric } from '@/lib/metrics/rag-metrics'
import {
  callLLMWithFallback,
  callLLMStream,
  LLMMessage,
  LLMResponse,
  type StreamTokenUsage,
} from './llm-fallback-service'
import { type OperationName, getOperationProvider, getOperationModel } from './operations-config'
import {
  validateArticleCitations,
  formatValidationWarnings,
  verifyClaimSourceAlignment,
  verifyBranchAlignment,
} from './citation-validator-service'
import {
  detectAbrogatedReferences,
  formatAbrogationWarnings,
  type AbrogationWarning,
} from './abrogation-detector-service'

// Configuration Query Expansion
const ENABLE_QUERY_EXPANSION = process.env.ENABLE_QUERY_EXPANSION !== 'false'

// Timeout global pour la recherche bilingue (60 secondes par d√©faut)
// R√©duit de 90s √† 60s gr√¢ce √† parall√©lisation recherche primaire + traduction (Phase 2.1)
const BILINGUAL_SEARCH_TIMEOUT_MS = parseInt(process.env.BILINGUAL_SEARCH_TIMEOUT_MS || '60000', 10)

// =============================================================================
// CLIENTS LLM (Ollama prioritaire, puis Groq, puis Anthropic)
// =============================================================================

let anthropicClient: Anthropic | null = null
let groqClient: OpenAI | null = null
let ollamaClient: OpenAI | null = null
let deepseekClient: OpenAI | null = null

function getOllamaClient(): OpenAI {
  if (!ollamaClient) {
    ollamaClient = new OpenAI({
      apiKey: 'ollama', // Ollama n'a pas besoin de cl√©
      baseURL: `${aiConfig.ollama.baseUrl}/v1`,
      timeout: 120000,
    })
  }
  return ollamaClient
}

function getAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    if (!aiConfig.anthropic.apiKey) {
      throw new Error('ANTHROPIC_API_KEY non configur√©')
    }
    anthropicClient = new Anthropic({ apiKey: aiConfig.anthropic.apiKey })
  }
  return anthropicClient
}

function getGroqClient(): OpenAI {
  if (!groqClient) {
    if (!aiConfig.groq.apiKey) {
      throw new Error('GROQ_API_KEY non configur√©')
    }
    groqClient = new OpenAI({
      apiKey: aiConfig.groq.apiKey,
      baseURL: aiConfig.groq.baseUrl,
    })
  }
  return groqClient
}

function getDeepSeekClient(): OpenAI {
  if (!deepseekClient) {
    if (!aiConfig.deepseek.apiKey) {
      throw new Error('DEEPSEEK_API_KEY non configur√©')
    }
    deepseekClient = new OpenAI({
      apiKey: aiConfig.deepseek.apiKey,
      baseURL: aiConfig.deepseek.baseUrl,
    })
  }
  return deepseekClient
}

// =============================================================================
// TYPES
// =============================================================================

export interface ChatSource {
  documentId: string
  documentName: string
  chunkContent: string
  similarity: number
  metadata?: Record<string, unknown>
}

export interface ChatResponse {
  answer: string
  sources: ChatSource[]
  tokensUsed: {
    input: number
    output: number
    total: number
  }
  model: string
  conversationId?: string
  citationWarnings?: string[] // Phase 2.2 - Citations non v√©rifi√©es
  abrogationWarnings?: import('./abrogation-detector-service').AbrogationWarning[] // Phase 2.3 - Lois abrog√©es
  qualityIndicator?: 'high' | 'medium' | 'low'
  averageSimilarity?: number
  abstentionReason?: string // Sprint 1 B1 - Raison de l'abstention si sources insuffisantes
  /** Sprint 3 RAG Audit-Proof : true si la r√©ponse a √©t√© r√©g√©n√©r√©e apr√®s d√©tection cross-domaine */
  wasRegenerated?: boolean
  /** Sprint 3 : statut de validation des sources apr√®s g√©n√©ration */
  validationStatus?: 'passed' | 'regenerated' | 'insufficient_sources'
}

import type { DocumentType } from '@/lib/categories/doc-types'

export interface ChatOptions {
  dossierId?: string
  conversationId?: string
  maxContextChunks?: number
  includeJurisprudence?: boolean
  includeKnowledgeBase?: boolean
  temperature?: number
  /** Type de contexte pour s√©lectionner le prompt appropri√© */
  contextType?: PromptContextType
  /** Mode Premium: utiliser cloud providers (Groq/DeepSeek/Anthropic) au lieu d'Ollama */
  usePremiumModel?: boolean
  /** Type d'op√©ration pour configuration sp√©cifique */
  operationName?: OperationName
  /** Logger structur√© pour tra√ßabilit√© (auto-cr√©√© si non fourni) */
  logger?: RAGLogger
  /** Filtrer la recherche KB par type de document */
  docType?: DocumentType
  /** Posture strat√©gique : d√©fense / attaque / neutre */
  stance?: LegalStance
  /** Exclure des cat√©gories de sources (filtre post-retrieval). Ex: ['google_drive'] */
  excludeCategories?: string[]
}

export interface ConversationMessage {
  role: 'user' | 'assistant'
  content: string
}

// Interface √©tendue pour le re-ranking
interface RankedSource extends ChatSource {
  boostedScore: number
  sourceType: string
  sourceId: string
}

// Interface pour les m√©triques de recherche
interface SearchMetrics {
  totalFound: number
  aboveThreshold: number
  scoreRange: {
    min: number
    max: number
    avg: number
  }
  sourceDistribution: Record<string, number>
  searchTimeMs: number
}

// =============================================================================
// RE-RANKING ET DIVERSIT√â DES SOURCES
// =============================================================================

/**
 * D√©termine le type de source √† partir des m√©tadonn√©es
 */
function getSourceType(metadata: Record<string, unknown> | undefined): string {
  if (!metadata) return 'document'
  const type = metadata.type as string | undefined
  const category = metadata.category as string | undefined
  return category || type || 'document'
}

/**
 * G√©n√®re un identifiant unique pour une source
 */
function getSourceId(source: ChatSource): string {
  const meta = source.metadata as Record<string, unknown> | undefined
  const type = getSourceType(meta)
  // Pour les documents, utiliser documentId; pour KB, utiliser le titre
  if (type === 'knowledge_base') {
    return `kb:${source.documentName}`
  }
  return `doc:${source.documentId}`
}

// =============================================================================
// HI√âRARCHIE DES NORMES (BOOST D√âTERMINISTE)
// =============================================================================

type HierarchyClass = 'norme' | 'jurisprudence' | 'doctrine' | 'modeles' | 'autre'

const HIERARCHY_CLASS_BOOST: Record<HierarchyClass, number> = {
  norme: 1.08,
  jurisprudence: 1.04,
  doctrine: 0.97,
  modeles: 0.95,
  autre: 1.0,
}

const NORMATIVE_CATEGORIES = new Set([
  'constitution',
  'conventions',
  'legislation',
  'codes',
  'jort',
])

const JURIS_CATEGORIES = new Set(['jurisprudence'])
const MODEL_CATEGORIES = new Set(['modeles', 'templates', 'procedures', 'formulaires', 'google_drive'])
const DOCTRINE_CATEGORIES = new Set(['doctrine', 'guides', 'lexique', 'actualites', 'autre'])

const NORMATIVE_LEVEL_BOOSTS: Array<{ patterns: RegExp[]; boost: number }> = [
  { patterns: [/constitution|ÿØÿ≥ÿ™Ÿàÿ±/i], boost: 1.08 },
  { patterns: [/convention|conventions|traite|trait√©|ÿßÿ™ŸÅÿßŸÇŸäÿ©|ÿßÿ™ŸÅÿßŸÇŸäÿßÿ™|ŸÖÿπÿßŸáÿØÿ©/i], boost: 1.06 },
  { patterns: [/loi[\s_-]?organique|ŸÇÿßŸÜŸàŸÜ\s*ÿ£ÿ≥ÿßÿ≥Ÿä/i], boost: 1.04 },
  { patterns: [/^loi$|law|legislation|code|code_article|ŸÖÿ¨ŸÑÿ©|ŸÇÿßŸÜŸàŸÜ(?!\s*ÿ£ÿ≥ÿßÿ≥Ÿä)/i], boost: 1.02 },
  { patterns: [/decret|d√©cret|decree|ŸÖÿ±ÿ≥ŸàŸÖ/i], boost: 1.01 },
  { patterns: [/ordre|order|ÿ£ŸÖÿ±/i], boost: 1.0 },
  { patterns: [/arrete|arr√™t√©|ŸÇÿ±ÿßÿ±|ministerial|Ÿàÿ≤ÿßÿ±Ÿä/i], boost: 0.99 },
]

function normalizeHierarchyValue(value: unknown): string | null {
  if (value === null || value === undefined) return null
  const str = String(value).trim()
  return str.length > 0 ? str.toLowerCase() : null
}

function classifyHierarchyType(
  category: string | null,
  docTypeMeta: string | null,
  docTypeRaw: string | null
): HierarchyClass {
  const meta = docTypeMeta || ''
  if (meta === 'textes') return 'norme'
  if (meta === 'juris') return 'jurisprudence'
  if (meta === 'doctrine') return 'doctrine'
  if (meta === 'templates' || meta === 'proc') return 'modeles'

  if (category && NORMATIVE_CATEGORIES.has(category)) return 'norme'
  if (category && JURIS_CATEGORIES.has(category)) return 'jurisprudence'
  if (category && MODEL_CATEGORIES.has(category)) return 'modeles'
  if (category && DOCTRINE_CATEGORIES.has(category)) return 'doctrine'

  if (docTypeRaw) {
    if (/juris|arret|arr√™t|jugement|decision|cassation|appel/i.test(docTypeRaw)) return 'jurisprudence'
    if (/modele|formulaire|template|proc/i.test(docTypeRaw)) return 'modeles'
    if (/doctrine|commentaire|guide|lexique/i.test(docTypeRaw)) return 'doctrine'
    if (NORMATIVE_LEVEL_BOOSTS.some(level => level.patterns.some(p => p.test(docTypeRaw)))) return 'norme'
  }

  return 'autre'
}

function getNormativeLevelBoost(value: string | null): number {
  if (!value) return 1.0
  for (const level of NORMATIVE_LEVEL_BOOSTS) {
    if (level.patterns.some(p => p.test(value))) {
      return level.boost
    }
  }
  return 1.01
}

// --- Boost temporel (r√©cence) ---
const CURRENT_YEAR = new Date().getFullYear()
const TEMPORAL_BOOST_CONFIG = {
  baseYear: 1956,
  maxYearRange: 70,
  maxBoost: 0.03,
  enabledClasses: new Set<HierarchyClass>(['norme', 'jurisprudence']),
} as const

const TEMPORAL_BOOST_ENABLED = process.env.ENABLE_TEMPORAL_BOOST !== 'false'

function extractDocumentYear(metadata: Record<string, unknown>): number | null {
  const dateFields = [
    'effective_date', 'jort_date', 'version_date',
    'document_date', 'decision_date', 'publication_date',
  ]
  for (const field of dateFields) {
    const val = metadata[field]
    if (typeof val === 'string' && val.length >= 4) {
      const year = parseInt(val.substring(0, 4), 10)
      if (year >= TEMPORAL_BOOST_CONFIG.baseYear && year <= CURRENT_YEAR) {
        return year
      }
    }
  }
  return null
}

function getTemporalRecencyBoost(
  metadata: Record<string, unknown> | undefined,
  hierarchyClass: HierarchyClass
): number {
  if (!TEMPORAL_BOOST_ENABLED) return 1.0
  if (!metadata || !TEMPORAL_BOOST_CONFIG.enabledClasses.has(hierarchyClass)) return 1.0

  const year = extractDocumentYear(metadata)
  if (year === null) return 1.0

  const { baseYear, maxYearRange, maxBoost } = TEMPORAL_BOOST_CONFIG
  const ratio = Math.min(Math.max((year - baseYear) / maxYearRange, 0), 1)
  return 1.0 + ratio * maxBoost
}

function getHierarchyBoost(metadata: Record<string, unknown> | undefined): number {
  if (!metadata) return 1.0

  const category = normalizeHierarchyValue(metadata.category)
  const docTypeMeta = normalizeHierarchyValue(metadata.doc_type)
  const docTypeRaw = normalizeHierarchyValue(
    (metadata.document_type as string | undefined) ||
    (metadata.documentType as string | undefined) ||
    (metadata.documentNature as string | undefined) ||
    (metadata.document_nature as string | undefined) ||
    (metadata.doc_type as string | undefined)
  )

  const classType = classifyHierarchyType(category, docTypeMeta, docTypeRaw)
  let boost = HIERARCHY_CLASS_BOOST[classType] || 1.0

  if (classType === 'norme') {
    boost *= getNormativeLevelBoost(docTypeRaw || category)
  }

  return boost
}

/**
 * D√©tecte si la query mentionne un domaine juridique sp√©cifique
 * et retourne les patterns de titre √† booster
 */
function detectDomainBoost(query: string): { pattern: string; factor: number }[] | null {
  // ‚ú® Fix (Feb 2026): factor 1.25‚Üí2.5 pour compenser l'√©cart s√©mantique entre queries naturelles et textes l√©gaux.
  // Seul le code EXACT attendu re√ßoit 2.5√ó. Les mauvais codes re√ßoivent seulement le CODE_BOOST g√©n√©rique.
  const DOMAIN_KEYWORDS: { keywords: string[]; titlePatterns: string[]; factor: number }[] = [
    // P√©nal
    {
      keywords: ['ÿ¨ÿ≤ÿßÿ¶Ÿä', 'ÿ¨ÿ≤ÿßÿ¶Ÿäÿ©', 'ÿ¨ŸÜÿßÿ¶Ÿä', 'ÿπŸÇŸàÿ®ÿ©', 'ÿπŸÇŸàÿ®ÿßÿ™', 'ÿ¨ÿ±ŸäŸÖÿ©', 'ÿßŸÑŸÇÿ™ŸÑ', 'ÿßŸÑÿ≥ÿ±ŸÇÿ©', 'ÿßŸÑÿØŸÅÿßÿπ ÿßŸÑÿ¥ÿ±ÿπŸä', 'ÿßŸÑÿ±ÿ¥Ÿàÿ©', 'p√©nal', 'criminel', 'l√©gitime d√©fense'],
      titlePatterns: ['ÿßŸÑŸÖÿ¨ŸÑÿ© ÿßŸÑÿ¨ÿ≤ÿßÿ¶Ÿäÿ©'],
      factor: 2.5,
    },
    // Civil
    {
      keywords: ['ŸÖÿØŸÜŸä', 'ÿßŸÑÿ™ÿ≤ÿßŸÖÿßÿ™', 'ÿπŸÇŸàÿØ', 'ÿ™ÿπŸàŸäÿ∂', 'ŸÖÿ≥ÿ§ŸàŸÑŸäÿ© ŸÖÿØŸÜŸäÿ©', 'ÿ™ŸÇÿßÿØŸÖ', 'civil', 'responsabilit√©', 'd√©lictuel'],
      titlePatterns: ['ŸÖÿ¨ŸÑÿ© ÿßŸÑÿßŸÑÿ™ÿ≤ÿßŸÖÿßÿ™ ŸàÿßŸÑÿπŸÇŸàÿØ'],
      factor: 2.5,
    },
    // Famille
    {
      keywords: ['ÿ£ÿ≠ŸàÿßŸÑ ÿ¥ÿÆÿµŸäÿ©', 'ÿ∑ŸÑÿßŸÇ', 'ÿ≤Ÿàÿßÿ¨', 'ŸÜŸÅŸÇÿ©', 'ÿ≠ÿ∂ÿßŸÜÿ©', 'ŸÖŸäÿ±ÿßÿ´', 'divorce', 'mariage', 'garde', 'famille'],
      titlePatterns: ['ŸÖÿ¨ŸÑÿ© ÿßŸÑÿ£ÿ≠ŸàÿßŸÑ ÿßŸÑÿ¥ÿÆÿµŸäÿ©'],
      factor: 2.5,
    },
    // Travail
    {
      keywords: ['ÿ¥ÿ∫ŸÑ', 'ÿπŸÖŸÑ', 'ÿ∑ÿ±ÿØ ÿ™ÿπÿ≥ŸÅŸä', 'ÿ•ÿ∂ÿ±ÿßÿ®', 'ÿ£ÿ¨ÿ±', 'ÿπÿßŸÖŸÑ', 'ŸÖÿ§ÿ¨ÿ±', 'travail', 'licenciement', 'gr√®ve'],
      titlePatterns: ['ŸÖÿ¨ŸÑÿ© ÿßŸÑÿ¥ÿ∫ŸÑ'],
      factor: 2.5,
    },
    // Commercial: "ŸÖÿ¨ŸÑÿ© ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑÿ™ÿ¨ÿßÿ±Ÿäÿ©" retir√© (causait des faux positifs car raw score > ÿßŸÑŸÖÿ¨ŸÑÿ© ÿßŸÑÿ™ÿ¨ÿßÿ±Ÿäÿ©)
    {
      keywords: ['ÿ™ÿ¨ÿßÿ±Ÿä', 'ÿ™ÿ¨ÿßÿ±Ÿäÿ©', 'ÿ¥ŸäŸÉ', 'ÿ•ŸÅŸÑÿßÿ≥', 'ÿ™ŸÅŸÑŸäÿ≥', 'ŸÉŸÖÿ®ŸäÿßŸÑÿ©', 'commercial', 'ch√®que', 'faillite'],
      titlePatterns: ['ÿßŸÑŸÖÿ¨ŸÑÿ© ÿßŸÑÿ™ÿ¨ÿßÿ±Ÿäÿ©'],
      factor: 2.5,
    },
    // Proc√©dure civile
    {
      keywords: ['ŸÖÿ±ÿßŸÅÿπÿßÿ™', 'ÿßÿ≥ÿ™ÿ¶ŸÜÿßŸÅ', 'ÿ™ÿπŸÇŸäÿ®', 'ÿØÿπŸàŸâ', 'ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ŸÖÿØŸÜŸäÿ©', 'proc√©dure'],
      titlePatterns: ['ŸÖÿ¨ŸÑÿ© ÿßŸÑŸÖÿ±ÿßŸÅÿπÿßÿ™ ÿßŸÑŸÖÿØŸÜŸäÿ© ŸàÿßŸÑÿ™ÿ¨ÿßÿ±Ÿäÿ©'],
      factor: 2.0,
    },
    // Saisies conservatoires / ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞
    {
      keywords: ['ÿπŸÇŸÑÿ©', 'ÿ™ÿ≠ŸÅÿ∏Ÿä', 'ÿ™ÿ≠ŸÅÿ∏Ÿäÿ©', 'ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞', 'ÿßÿπÿ™ÿ±ÿßÿ∂ ÿ™ÿ≠ŸÅÿ∏Ÿä', 'ÿπÿ±Ÿäÿ∂ÿ©', 'ÿ∂ÿ±ÿ® ÿπŸÇŸÑÿ©', 'saisie', 'conservatoire'],
      titlePatterns: ['ÿπŸÇŸÑÿ©', 'ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞', 'ÿ™ÿ≠ŸÅÿ∏'],
      factor: 2.5,
    },
  ]

  const matches: { pattern: string; factor: number }[] = []

  for (const domain of DOMAIN_KEYWORDS) {
    const queryLower = query.toLowerCase()
    const hasKeyword = domain.keywords.some(kw => query.includes(kw) || queryLower.includes(kw))
    if (hasKeyword) {
      for (const pattern of domain.titlePatterns) {
        matches.push({ pattern, factor: domain.factor })
      }
    }
  }

  return matches.length > 0 ? matches : null
}

/**
 * Re-rank les sources avec boost par type, cross-encoder et diversit√©
 * Utilise:
 * 1. Boost factors dynamiques bas√©s sur le feedback utilisateur
 * 2. Cross-encoder pour re-scorer les paires (query, document)
 * 3. Diversit√© pour limiter les chunks par source
 */
async function rerankSources(
  sources: ChatSource[],
  query?: string,
  boostFactors?: Record<string, number>,
  branchOptions?: { forbiddenBranches?: string[]; allowedBranches?: string[] },
  stance?: LegalStance
): Promise<ChatSource[]> {
  if (sources.length === 0) return sources

  // R√©cup√©rer les boosts dynamiques si non fournis (avec fallback sur valeurs statiques)
  let boosts: Record<string, number>
  if (boostFactors) {
    boosts = boostFactors
  } else {
    try {
      boosts = (await getDynamicBoostFactors()).factors
    } catch (err) {
      console.warn('[RAG] Erreur getDynamicBoostFactors, utilisation valeurs statiques:', err)
      boosts = SOURCE_BOOST
    }
  }

  // 1. Appliquer boost par type (dynamique ou statique) + boost s√©mantique par domaine
  const domainBoost = query ? detectDomainBoost(query) : null
  let rankedSources: RankedSource[] = sources.map((s) => {
    const sourceType = getSourceType(s.metadata as Record<string, unknown>)
    let boost = boosts[sourceType] || boosts.autre || SOURCE_BOOST.autre || 1.0

    // Boost s√©mantique: si la query mentionne un domaine, booster les r√©sultats correspondants
    if (domainBoost && s.documentName) {
      for (const { pattern, factor } of domainBoost) {
        if (s.documentName.includes(pattern)) {
          boost *= factor
          break
        }
      }
    }

    // Sprint 1 RAG Audit-Proof: p√©nalit√© forte pour branches hors-scope (√ó0.05 ‚âà √©limination)
    if (branchOptions?.forbiddenBranches && branchOptions.forbiddenBranches.length > 0) {
      const branch = s.metadata?.branch as string | undefined
      if (branch && branch !== 'autre' && branchOptions.forbiddenBranches.includes(branch)) {
        boost *= 0.05
        console.log(`[RAG Branch] P√©nalit√© 0.05√ó sur "${s.documentName}" (branch=${branch}, forbidden=[${branchOptions.forbiddenBranches.join(',')}])`)
      }
    }

    // Boost hi√©rarchique (normes > jurisprudence > doctrine > mod√®les)
    const _meta = s.metadata as Record<string, unknown> | undefined
    boost *= getHierarchyBoost(_meta)

    // Boost temporel (r√©cence) : +0% √† +3% selon l'ann√©e du document (norme/jurisprudence uniquement)
    const _hierarchyClass = classifyHierarchyType(
      (_meta?.category as string | null) ?? null,
      normalizeHierarchyValue(_meta?.doc_type),
      normalizeHierarchyValue(
        (_meta?.document_type as string | undefined) ||
        (_meta?.documentType as string | undefined) ||
        (_meta?.doc_type as string | undefined)
      )
    )
    boost *= getTemporalRecencyBoost(_meta, _hierarchyClass)

    // Boost stance-aware : favoriser les types de documents pertinents √† la posture
    // Fix 4 : fallback sur category si doc_type absent (√©vite boost nul silencieux)
    if (stance === 'defense' || stance === 'attack') {
      const CATEGORY_TO_DOCTYPE: Record<string, string> = {
        codes: 'TEXTES', legislation: 'TEXTES', lois: 'TEXTES',
        jurisprudence: 'JURIS', cassation: 'JURIS',
        procedure: 'PROC', modeles: 'TEMPLATES', doctrine: 'DOCTRINE',
      }
      const rawDocType = s.metadata?.doc_type as string | undefined
      const rawCategory = s.metadata?.category as string | undefined
      const docType = rawDocType ?? (rawCategory ? CATEGORY_TO_DOCTYPE[rawCategory] : undefined)

      if (stance === 'defense') {
        if (docType === 'JURIS') boost *= 1.3
        if (docType === 'PROC') boost *= 1.2
      } else {
        if (docType === 'TEXTES') boost *= 1.3
        if (docType === 'JURIS') boost *= 1.2
      }
    }

    return {
      ...s,
      boostedScore: s.similarity * boost,
      sourceType,
      sourceId: getSourceId(s),
    }
  })

  // 2. Appliquer cross-encoder re-ranking si activ√© et query fournie
  if (isRerankerEnabled() && query && rankedSources.length > 1) {
    try {
      const docsToRerank: DocumentToRerank[] = rankedSources.map((s) => ({
        content: s.chunkContent,
        originalScore: s.boostedScore,
        metadata: s.metadata as Record<string, unknown>,
      }))

      const rerankedResults = await rerankDocuments(query, docsToRerank, undefined, { useCrossEncoder: true })

      // Combiner scores cross-encoder avec boosts existants
      rankedSources = rerankedResults.map((result) => {
        const original = rankedSources[result.index]
        const finalScore = combineScores(result.score, original.boostedScore)
        return {
          ...original,
          boostedScore: finalScore,
        }
      })

      // Re-trier par score combin√©
      rankedSources.sort((a, b) => b.boostedScore - a.boostedScore)
    } catch (error) {
      console.error('[RAG] Erreur cross-encoder, fallback boost simple:', error)
      // Continuer avec le tri par boost simple
      rankedSources.sort((a, b) => b.boostedScore - a.boostedScore)
    }
  } else {
    // Trier par score boost√© d√©croissant
    rankedSources.sort((a, b) => b.boostedScore - a.boostedScore)
  }

  // 3. Appliquer diversit√© : limiter chunks par source
  const sourceCount = new Map<string, number>()
  const diversifiedSources: ChatSource[] = []

  for (const source of rankedSources) {
    const count = sourceCount.get(source.sourceId) || 0
    if (count < RAG_DIVERSITY.maxChunksPerSource) {
      sourceCount.set(source.sourceId, count + 1)
      // Retourner ChatSource sans les champs ajout√©s
      const { boostedScore, sourceType, sourceId, ...originalSource } = source
      diversifiedSources.push(originalSource)
    }
  }

  return diversifiedSources
}

/**
 * Compte les sources par type
 */
function countSourcesByType(sources: ChatSource[]): Record<string, number> {
  const counts: Record<string, number> = {}
  for (const source of sources) {
    const type = getSourceType(source.metadata as Record<string, unknown>)
    counts[type] = (counts[type] || 0) + 1
  }
  return counts
}

/**
 * Log les m√©triques de recherche RAG
 */
function logSearchMetrics(metrics: SearchMetrics): void {
  console.log('[RAG Search]', JSON.stringify({
    totalFound: metrics.totalFound,
    aboveThreshold: metrics.aboveThreshold,
    scores: {
      min: metrics.scoreRange.min.toFixed(3),
      max: metrics.scoreRange.max.toFixed(3),
      avg: metrics.scoreRange.avg.toFixed(3),
    },
    sources: metrics.sourceDistribution,
    timeMs: metrics.searchTimeMs,
  }))
}

// =============================================================================
// RECHERCHE CONTEXTUELLE
// =============================================================================

// Type de retour pour les recherches avec info de cache
interface SearchResult {
  sources: ChatSource[]
  cacheHit: boolean
}

/**
 * Recherche les documents pertinents pour une question
 * Avec cache Redis pour les recherches r√©p√©t√©es.
 *
 * @exported Pour tests unitaires
 */
export async function searchRelevantContext(
  question: string,
  userId: string,
  options: ChatOptions = {}
): Promise<SearchResult> {
  const startTime = Date.now()
  const {
    dossierId,
    maxContextChunks = aiConfig.rag.maxResults,
    includeJurisprudence = false,
    includeKnowledgeBase = true, // Activ√© par d√©faut
  } = options

  // Phase 1: Legal Router ‚Äî remplace classifyQuery par routeQuery (classification + tracks en 1 appel)
  // Lanc√© en parall√®le avec l'expansion/embedding pour recouvrir la latence
  const _earlyRouterPromise = includeKnowledgeBase
    ? import('./legal-router-service').then(m => m.routeQuery(question, { maxTracks: 4 }))
    : null

  // ‚ú® OPTIMISATION RAG - Sprint 2 (Feb 2026) + Fix requ√™tes longues (Feb 16, 2026)
  // 1. Query Expansion pour requ√™tes courtes / Condensation pour requ√™tes longues
  let embeddingQuestion = question // Question utilis√©e pour l'embedding
  if (ENABLE_QUERY_EXPANSION) {
    if (question.length < 50) {
      // Requ√™tes courtes : expansion LLM (ajouter termes juridiques)
      const { expandQuery } = await import('./query-expansion-service')
      try {
        embeddingQuestion = await expandQuery(question)
        if (embeddingQuestion !== question) {
          console.log(`[RAG Search] Query expand√©e: ${question} ‚Üí ${embeddingQuestion.substring(0, 80)}...`)
        }
      } catch (error) {
        console.error('[RAG Search] Erreur expansion query:', error)
        embeddingQuestion = question
      }
    } else if (question.length > 200) {
      // Requ√™tes longues : condensation (extraire concepts cl√©s pour embedding cibl√©)
      const { condenseQuery } = await import('./query-expansion-service')
      try {
        // Timeout 5s : condenseQuery appelle un LLM ‚Üí peut bloquer si lent
        embeddingQuestion = await withTimeout(condenseQuery(question), 5000, 'condenseQuery')
          .catch(() => question) // Fallback silencieux : question originale
        if (embeddingQuestion !== question) {
          console.log(`[RAG Search] Query condens√©e: ${question.length} chars ‚Üí "${embeddingQuestion}" (${embeddingQuestion.length} chars)`)
        }
      } catch (error) {
        console.error('[RAG Search] Erreur condensation query:', error)
        embeddingQuestion = question
      }
    }
  }

  // 2. Enrichissement synonymes juridiques arabes (applicable √† toutes les queries)
  // Lookup instantan√© O(n) - pas de LLM, pas de latence ajout√©e
  try {
    const { enrichQueryWithLegalSynonyms } = await import('./query-expansion-service')
    const enriched = enrichQueryWithLegalSynonyms(embeddingQuestion)
    if (enriched !== embeddingQuestion) {
      console.log(`[RAG Search] Synonymes juridiques: ${embeddingQuestion.substring(0, 50)}... ‚Üí +synonymes`)
      embeddingQuestion = enriched
    }
  } catch (error) {
    // Non-bloquant : si enrichissement √©choue, on continue avec la query existante
  }

  // Enrichissement stance-aware : ajout de termes sp√©cifiques √† la posture juridique
  // Fix 2 : uniquement pour les requ√™tes arabes (√©vite pollution embedding FR)
  const stanceTerms: Record<string, string> = {
    defense: 'ÿ®ÿ∑ŸÑÿßŸÜ ÿØŸÅÿπ ÿ±ŸÅÿ∂ ÿπÿØŸÖ ŸÇÿ®ŸàŸÑ ÿ™ŸÇÿßÿØŸÖ ÿπÿØŸÖ ÿßŸÑÿßÿÆÿ™ÿµÿßÿµ',
    attack: 'ŸÖÿ≥ÿ§ŸàŸÑŸäÿ© ÿ™ÿπŸàŸäÿ∂ ÿ•ÿÆŸÑÿßŸÑ ÿßŸÑÿ™ÿ≤ÿßŸÖ ÿ∂ÿ±ÿ± ŸÖÿ∑ÿßŸÑÿ®ÿ©',
  }
  if (options.stance && options.stance !== 'neutral' && stanceTerms[options.stance]) {
    const embeddingLang = detectLanguage(embeddingQuestion)
    if (embeddingLang === 'ar') {
      embeddingQuestion = `${embeddingQuestion} ${stanceTerms[options.stance]}`
    }
  }

  // G√©n√©rer l'embedding de la question transform√©e (expand√©e ou condens√©e)
  const queryEmbedding = await generateEmbedding(embeddingQuestion, {
    operationName: options.operationName,
  })
  const embeddingStr = formatEmbeddingForPostgres(queryEmbedding.embedding)

  // V√©rifier le cache de recherche
  const searchScope: SearchScope = { userId, dossierId }
  const cachedResults = await getCachedSearchResults(queryEmbedding.embedding, searchScope)
  if (cachedResults) {
    console.log(`[RAG Search] Cache HIT - ${cachedResults.length} sources (${Date.now() - startTime}ms)`)
    return { sources: cachedResults as ChatSource[], cacheHit: true }
  }

  const allSources: ChatSource[] = []

  // Recherche dans les documents du dossier ou de l'utilisateur
  // Skip si userId n'est pas un UUID valide (ex: 'eval-system' dans les benchmarks)
  const isValidUUID = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(userId)
  let docSql: string
  let docParams: (string | number)[]

  if (!isValidUUID) {
    // Skip document_embeddings search for non-UUID userIds (eval, system, etc.)
    docSql = 'SELECT 1 WHERE false'
    docParams = []
  } else if (dossierId) {
    docSql = `
      SELECT
        de.document_id,
        d.nom as document_name,
        de.content_chunk,
        (1 - (de.embedding <=> $1::vector)) as similarity,
        de.metadata
      FROM document_embeddings de
      JOIN documents d ON de.document_id = d.id
      WHERE de.user_id = $2
        AND d.dossier_id = $3
        AND (1 - (de.embedding <=> $1::vector)) >= $4
      ORDER BY de.embedding <=> $1::vector
      LIMIT $5
    `
    docParams = [
      embeddingStr,
      userId,
      dossierId,
      RAG_THRESHOLDS.documents,
      maxContextChunks * 2, // R√©cup√©rer plus pour le re-ranking
    ]
  } else {
    docSql = `
      SELECT
        de.document_id,
        d.nom as document_name,
        de.content_chunk,
        (1 - (de.embedding <=> $1::vector)) as similarity,
        de.metadata
      FROM document_embeddings de
      JOIN documents d ON de.document_id = d.id
      WHERE de.user_id = $2
        AND (1 - (de.embedding <=> $1::vector)) >= $3
      ORDER BY de.embedding <=> $1::vector
      LIMIT $4
    `
    docParams = [
      embeddingStr,
      userId,
      RAG_THRESHOLDS.documents,
      maxContextChunks * 2, // R√©cup√©rer plus pour le re-ranking
    ]
  }

  const docResult = await db.query(docSql, docParams)

  for (const row of docResult.rows) {
    allSources.push({
      documentId: row.document_id,
      documentName: row.document_name,
      chunkContent: row.content_chunk,
      similarity: parseFloat(row.similarity),
      metadata: row.metadata,
    })
  }

  // Optionnel: Recherche dans la jurisprudence
  if (includeJurisprudence) {
    const juriSql = `
      SELECT
        j.id as document_id,
        j.decision_number || ' - ' || j.court as document_name,
        COALESCE(j.summary, LEFT(j.full_text, 800)) as content_chunk,
        (1 - (j.embedding <=> $1::vector)) as similarity,
        jsonb_build_object(
          'type', 'jurisprudence',
          'court', j.court,
          'chamber', j.chamber,
          'domain', j.domain,
          'date', j.decision_date,
          'articles', j.articles_cited
        ) as metadata
      FROM jurisprudence j
      WHERE j.embedding IS NOT NULL
        AND (1 - (j.embedding <=> $1::vector)) >= $2
      ORDER BY j.embedding <=> $1::vector
      LIMIT $3
    `

    const juriResult = await db.query(juriSql, [
      embeddingStr,
      RAG_THRESHOLDS.jurisprudence,
      Math.ceil(maxContextChunks / 2), // Plus de jurisprudence pour le re-ranking
    ])

    for (const row of juriResult.rows) {
      allSources.push({
        documentId: row.document_id,
        documentName: row.document_name,
        chunkContent: row.content_chunk,
        similarity: parseFloat(row.similarity),
        metadata: row.metadata,
      })
    }
  }

  // D√©tection langue pour seuils adaptatifs (arabe ‚Üí scores plus bas)
  const queryLangForSearch = detectLanguage(question)

  // R√©soudre le router une seule fois (hiss√© pour r√©utilisation par relevance gating + multi-track)
  // Si le routeur √©choue (rate limit, timeout), on continue sans classification ‚Üí recherche KB globale
  let routerResult: Awaited<ReturnType<typeof import('./legal-router-service').routeQuery>> | null = null
  if (includeKnowledgeBase) {
    try {
      routerResult = _earlyRouterPromise
        ? await _earlyRouterPromise
        : await (await import('./legal-router-service')).routeQuery(question, { maxTracks: 4 })
    } catch (routerError) {
      console.warn('[RAG Search] Router √©chou√©, fallback recherche KB sans classification:', routerError instanceof Error ? routerError.message : routerError)
    }
  }
  const classification = routerResult?.classification || null

  // Recherche dans la base de connaissances partag√©e
  // Note: si classification est null (routeQuery √©chou√©/rate limited), on fait quand m√™me une recherche globale
  if (includeKnowledgeBase) {
    try {

      let kbResults: Array<{
        knowledgeBaseId: string
        title: string
        chunkContent: string
        similarity: number
        category: string
        metadata: Record<string, unknown>
      }> = []

      // Recherche globale hybride + multi-track si tracks disponibles
      const globalThreshold = queryLangForSearch === 'ar'
        ? Math.min(RAG_THRESHOLDS.knowledgeBase, 0.30)
        : RAG_THRESHOLDS.knowledgeBase

      // Phase 1: Multi-track retrieval si le router a g√©n√©r√© plusieurs tracks ET classification disponible
      const tracks = routerResult?.tracks || []
      if (tracks.length > 1 && classification) {
        console.log(
          `[RAG Search] Multi-track: ${tracks.length} tracks, ${tracks.reduce((a, t) => a + t.searchQueries.length, 0)} queries (source: ${routerResult?.source}, confiance: ${(classification.confidence * 100).toFixed(1)}%)`
        )
        const { searchMultiTrack } = await import('./knowledge-base-service')
        kbResults = await searchMultiTrack(tracks, {
          topKPerQuery: 5,
          threshold: globalThreshold,
          operationName: options.operationName,
          limit: maxContextChunks,
        })
      } else {
        // Fallback: recherche globale hybride classique (aussi utilis√© si classification null)
        if (!classification) {
          console.warn('[RAG Search] Classification null (routeQuery √©chou√©?), fallback recherche KB globale')
        } else {
          console.log(
            `[RAG Search] Recherche KB globale hybride (classifieur: ${classification.categories.join(', ')}, domaines: ${classification.domains.join(',') || 'aucun'}, confiance: ${(classification.confidence * 100).toFixed(1)}%, seuil: ${globalThreshold})`
          )
        }
        kbResults = await searchKnowledgeBaseHybrid(embeddingQuestion, {
          limit: maxContextChunks,
          threshold: globalThreshold,
          operationName: options.operationName,
          docType: options.docType,
        })
      }

      // Filtrer les cat√©gories exclues (post-retrieval)
      if (options.excludeCategories && options.excludeCategories.length > 0) {
        const before = kbResults.length
        kbResults = kbResults.filter(
          r => !options.excludeCategories!.includes(r.category as string)
        )
        const excluded = before - kbResults.length
        if (excluded > 0) {
          console.log(`[RAG Search] excludeCategories: ${excluded} chunks exclus (${options.excludeCategories.join(', ')})`)
        }
      }

      // Ajouter r√©sultats KB aux sources
      for (const result of kbResults) {
        allSources.push({
          documentId: result.knowledgeBaseId,
          documentName: `[ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ©] ${result.title}`,
          chunkContent: result.chunkContent,
          similarity: result.similarity,
          metadata: {
            type: 'knowledge_base',
            category: result.category,
            ...result.metadata,
          },
        })
      }
    } catch (error) {
      // CRITIQUE: Ne PAS avaler silencieusement les erreurs KB
      // Si la KB search √©choue, le chat retournera "pas de documents" ‚Üí mauvaise UX
      const errMsg = error instanceof Error ? error.message : String(error)
      console.error('[RAG Search] ‚ùå ERREUR CRITIQUE recherche knowledge base:', errMsg)
      console.error('[RAG Search] Stack:', error instanceof Error ? error.stack : 'N/A')
      // Propager l'erreur pour d√©clencher le mode d√©grad√© au lieu de retourner 0 sources
      throw error
    }
  }

  // Filtrer par seuil minimum absolu (plus bas pour l'arabe: embeddings produisent des scores plus faibles)
  const queryLangForThreshold = detectLanguage(question)
  const effectiveMinimum = queryLangForThreshold === 'ar'
    ? Math.min(RAG_THRESHOLDS.minimum, 0.30)
    : RAG_THRESHOLDS.minimum
  const aboveThreshold = allSources.filter(
    (s) => s.similarity >= effectiveMinimum
  )

  // Sprint 1 RAG Audit-Proof: branches issues du routeur pour p√©naliser sources hors-domaine
  const branchOptions = routerResult
    ? { forbiddenBranches: routerResult.forbiddenBranches, allowedBranches: routerResult.allowedBranches }
    : undefined

  // Appliquer re-ranking avec boost dynamique, cross-encoder et diversit√©
  let rerankedSources = await rerankSources(aboveThreshold, question, undefined, branchOptions, options.stance)

  // Seuils adaptatifs: si moins de 3 r√©sultats, baisser le seuil de 20% (une seule fois)
  // Plancher plus bas pour l'arabe (embeddings arabes produisent des scores plus faibles)
  const queryLang = detectLanguage(question)
  const ADAPTIVE_FLOOR = queryLang === 'ar' ? 0.35 : 0.45
  if (rerankedSources.length < 3 && allSources.length > rerankedSources.length) {
    const adaptiveThreshold = Math.max(RAG_THRESHOLDS.minimum * 0.8, ADAPTIVE_FLOOR)
    if (adaptiveThreshold < RAG_THRESHOLDS.minimum) {
      const adaptiveResults = allSources.filter(
        (s) => s.similarity >= adaptiveThreshold
      )
      if (adaptiveResults.length > rerankedSources.length) {
        console.log(`[RAG Search] Seuil adaptatif: ${rerankedSources.length} ‚Üí ${adaptiveResults.length} r√©sultats (seuil ${adaptiveThreshold.toFixed(2)}, plancher ${ADAPTIVE_FLOOR})`)
        rerankedSources = await rerankSources(adaptiveResults, question, undefined, branchOptions, options.stance)
      }
    }
  }

  // Phase 2: Relevance Gating ‚Äî bloquer sources hors-domaine
  if (classification && classification.confidence >= 0.7 && classification.domains.length > 0) {
    try {
      const { gateSourceRelevance } = await import('./relevance-gate-service')
      const gating = await gateSourceRelevance(question, rerankedSources, classification)
      if (gating.blocked.length > 0) {
        console.log(`[RAG Gate] Bloqu√© ${gating.blocked.length} sources hors-domaine`)
        rerankedSources = gating.passed
      }
    } catch (error) {
      console.error('[RAG Gate] Erreur gating, skip:', error instanceof Error ? error.message : error)
    }
  }

  // FILTRAGE ABROGATIONS : Exclure documents abrog√©s/suspendus
  const filteredResult = await filterAbrogatedSources(rerankedSources, {
    enableFilter: true,
    warnOnModified: true,
    logExclusions: true,
  })

  // Si trop de sources filtr√©es, logger pour monitoring
  if (filteredResult.filteredCount > 0) {
    console.log(`[RAG Filter] ‚ö†Ô∏è  ${filteredResult.filteredCount} source(s) filtr√©e(s) (abrog√©es/suspendues)`)
  }

  // Limiter au nombre demand√© (sur sources valides filtr√©es)
  let finalSources = filteredResult.validSources.slice(0, maxContextChunks)

  // Hard quality gate: si TOUTES les sources sont en dessous du seuil, ne pas les envoyer au LLM
  // Seuil diff√©renci√© : r√©sultats vectoriels (similarity = vecSim r√©el) sont plus fiables que BM25-only
  // Seuils plus bas pour l'arabe : embeddings OpenAI donnent des scores 0.35-0.65 pour contenu pertinent
  // BM25-only: synthetic similarity = max(0.35, bm25Rank*10) ‚Üí plancher 0.35, donc gate < 0.35 = jamais filtr√©
  const HARD_QUALITY_GATE = queryLang === 'ar' ? 0.30 : 0.50
  const HARD_QUALITY_GATE_VECTOR = queryLang === 'ar' ? 0.20 : 0.35
  const hasVectorResults = finalSources.some(s => s.metadata?.searchType === 'vector' || s.metadata?.searchType === 'hybrid')
  const effectiveGate = hasVectorResults ? HARD_QUALITY_GATE_VECTOR : HARD_QUALITY_GATE
  if (finalSources.length > 0 && finalSources.every(s => s.similarity < effectiveGate)) {
    const bestScore = Math.max(...finalSources.map(s => s.similarity))
    console.warn(`[RAG Search] ‚ö†Ô∏è Hard quality gate (${effectiveGate}, lang=${queryLang}): toutes ${finalSources.length} sources < seuil, meilleur score=${bestScore.toFixed(3)}, retour 0 sources`)
    return { sources: [], cacheHit: false }
  }

  // Calculer et logger les m√©triques
  const scores = allSources.map((s) => s.similarity)
  const searchTimeMs = Date.now() - startTime

  if (scores.length > 0) {
    const metrics: SearchMetrics = {
      totalFound: allSources.length,
      aboveThreshold: aboveThreshold.length,
      scoreRange: {
        min: Math.min(...scores),
        max: Math.max(...scores),
        avg: scores.reduce((a, b) => a + b, 0) / scores.length,
      },
      sourceDistribution: countSourcesByType(finalSources),
      searchTimeMs,
    }
    logSearchMetrics(metrics)
  } else {
    console.log('[RAG Search]', JSON.stringify({
      totalFound: 0,
      aboveThreshold: 0,
      timeMs: searchTimeMs,
    }))
  }

  // Mettre en cache les r√©sultats
  if (finalSources.length > 0) {
    await setCachedSearchResults(queryEmbedding.embedding, finalSources, searchScope)
  }

  return { sources: finalSources, cacheHit: false }
}

/**
 * Helper pour cr√©er une promesse avec timeout
 */
function withTimeout<T>(promise: Promise<T>, ms: number, context: string): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) =>
      setTimeout(() => reject(new Error(`Timeout ${context} (${ms}ms)`)), ms)
    ),
  ])
}

/**
 * Recherche bilingue avec query expansion AR ‚Üî FR
 * Traduit la question et fusionne les r√©sultats des deux langues.
 * Applique un timeout global pour √©viter les latences excessives.
 */
async function searchRelevantContextBilingual(
  question: string,
  userId: string,
  options: ChatOptions = {}
): Promise<SearchResult> {
  const startTime = Date.now()

  // D√©tecter la langue de la question
  const detectedLang = detectLanguage(question)
  console.log(`[RAG Bilingual] Langue d√©tect√©e: ${detectedLang}`)

  // ========================================
  // PARALL√âLISATION Phase 2.1 : Recherche primaire + Traduction en parall√®le
  // ========================================
  const targetLang = getOppositeLanguage(detectedLang)
  const canTranslate = ENABLE_QUERY_EXPANSION && isTranslationAvailable()

  // Lancer recherche primaire ET traduction en PARALL√àLE
  const [primaryResult, translationResult] = await Promise.allSettled([
    // Recherche primaire avec timeout global
    withTimeout(
      searchRelevantContext(question, userId, options),
      BILINGUAL_SEARCH_TIMEOUT_MS,
      'recherche primaire'
    ),

    // Traduction parall√®le (ou reject si d√©sactiv√©)
    canTranslate
      ? withTimeout(
          translateQuery(question, detectedLang === 'mixed' ? 'fr' : detectedLang, targetLang),
          5000, // 5s max pour traduction (augment√© de 3s pour √©viter timeouts)
          'traduction'
        )
      : Promise.reject(new Error('Translation disabled')),
  ])

  // V√©rifier r√©sultat recherche primaire ‚Äî fallback KB search simple si timeout
  if (primaryResult.status === 'rejected') {
    const errMsg = primaryResult.reason instanceof Error ? primaryResult.reason.message : String(primaryResult.reason)
    console.error('[RAG Bilingual] Erreur recherche primaire:', errMsg)

    // Fallback : recherche KB simple (sans router/expansion) pour √©viter 0 r√©sultats
    try {
      console.log('[RAG Bilingual] Fallback recherche KB simple...')
      const fallbackResults = await withTimeout(
        searchKnowledgeBaseHybrid(question, { limit: 10 }),
        15000,
        'fallback KB search'
      )
      if (fallbackResults.length > 0) {
        const fallbackSources: ChatSource[] = fallbackResults.slice(0, 5).map(r => ({
          documentId: r.knowledgeBaseId,
          documentName: r.title || 'Document',
          chunkContent: r.chunkContent,
          similarity: r.similarity || 0,
          metadata: r.metadata,
        }))
        console.log(`[RAG Bilingual] Fallback: ${fallbackSources.length} sources r√©cup√©r√©es`)
        return { sources: fallbackSources, cacheHit: false }
      }
    } catch (fallbackErr) {
      console.error('[RAG Bilingual] Fallback KB search √©chou√©:', fallbackErr instanceof Error ? fallbackErr.message : fallbackErr)
    }

    return { sources: [], cacheHit: false }
  }

  // Si traduction non disponible ou √©chou√©e, retourner r√©sultats primaires seuls
  if (!canTranslate || translationResult.status === 'rejected') {
    console.log(
      `[RAG Bilingual] Traduction ${!canTranslate ? 'd√©sactiv√©e' : '√©chou√©e'}, retour r√©sultats primaires seuls`
    )
    return primaryResult.value
  }

  // V√©rifier temps restant pour recherche secondaire
  const elapsed = Date.now() - startTime
  const remaining = BILINGUAL_SEARCH_TIMEOUT_MS - elapsed

  // Si moins de 15s restantes, ne pas lancer la recherche secondaire
  if (remaining < 15000) {
    console.log(
      `[RAG Bilingual] Temps restant insuffisant (${remaining}ms < 15s), skip recherche secondaire`
    )
    return primaryResult.value
  }

  // V√©rifier validit√© traduction
  const translation = translationResult.value
  if (!translation.success || translation.translatedText === question) {
    console.log('[RAG Bilingual] Traduction identique ou invalide, retour r√©sultats primaires')
    return primaryResult.value
  }

  console.log(`[RAG Bilingual] Question traduite: "${translation.translatedText.substring(0, 50)}..."`)

  // ========================================
  // Recherche secondaire avec timeout adaptatif
  // ========================================
  let secondaryResult: SearchResult = { sources: [], cacheHit: false }

  try {
    secondaryResult = await withTimeout(
      searchRelevantContext(translation.translatedText, userId, options),
      Math.max(15000, remaining), // Au moins 15s pour recherche secondaire
      'recherche secondaire'
    )
  } catch (error) {
    console.warn(
      '[RAG Bilingual] Timeout recherche secondaire, retour r√©sultats primaires seuls:',
      error instanceof Error ? error.message : error
    )
    return primaryResult.value
  }

  // ========================================
  // Fusion r√©sultats primaires + secondaires
  // ========================================
  const primarySources = primaryResult.value.sources
  const secondarySources = secondaryResult.sources

  // Poids: primaire 0.7, secondaire 0.3
  const PRIMARY_WEIGHT = 0.7
  const SECONDARY_WEIGHT = 0.3

  const mergedSources: ChatSource[] = []
  const seenChunks = new Set<string>()

  // Ajouter les sources primaires avec poids ajust√©
  for (const source of primarySources) {
    const key = `${source.documentId}:${source.chunkContent.substring(0, 100)}`
    if (!seenChunks.has(key)) {
      seenChunks.add(key)
      mergedSources.push({
        ...source,
        similarity: source.similarity * PRIMARY_WEIGHT + (1 - source.similarity) * 0.1,
      })
    }
  }

  // Ajouter les sources secondaires avec poids ajust√©
  for (const source of secondarySources) {
    const key = `${source.documentId}:${source.chunkContent.substring(0, 100)}`
    if (!seenChunks.has(key)) {
      seenChunks.add(key)
      mergedSources.push({
        ...source,
        similarity: source.similarity * SECONDARY_WEIGHT,
      })
    }
  }

  // Re-trier par similarit√© ajust√©e
  mergedSources.sort((a, b) => b.similarity - a.similarity)

  // Limiter au nombre demand√©
  const maxResults = options.maxContextChunks || aiConfig.rag.maxResults
  const finalSources = mergedSources.slice(0, maxResults)

  const totalTimeMs = Date.now() - startTime
  console.log(
    `[RAG Bilingual PARALLEL] Fusion: ${primarySources.length} primaires + ${secondarySources.length} secondaires ‚Üí ${finalSources.length} finaux (${totalTimeMs}ms, -${Math.round((1 - totalTimeMs / BILINGUAL_SEARCH_TIMEOUT_MS) * 100)}% vs timeout)`
  )

  // Cache hit si au moins une des deux recherches √©tait en cache
  return {
    sources: finalSources,
    cacheHit: primaryResult.value.cacheHit || secondaryResult.cacheHit,
  }
}

// =============================================================================
// CONSTRUCTION DU PROMPT
// =============================================================================

// Limite de tokens pour le contexte RAG (6000 par d√©faut pour les LLM modernes 8k+)
const RAG_MAX_CONTEXT_TOKENS = parseInt(process.env.RAG_MAX_CONTEXT_TOKENS || '6000', 10)

// Templates bilingues pour le message utilisateur
const USER_MESSAGE_TEMPLATES = {
  ar: {
    prefix: 'Ÿàÿ´ÿßÿ¶ŸÇ ŸÖÿ±ÿ¨ÿπŸäÿ©:',
    questionLabel: 'ÿßŸÑÿ≥ÿ§ÿßŸÑ:',
    analysisHint: 'ÿ™ÿπŸÑŸäŸÖÿßÿ™: ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑÿ¥ÿ±Ÿàÿ∑ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖŸÜ ŸÉŸÑ ŸÅÿµŸÑÿå ÿ≠ÿØŸëÿØ ÿßŸÑÿ¢ÿ¨ÿßŸÑ ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿπŸÖŸÑŸäÿ©ÿå Ÿàÿßÿ±ÿ®ÿ∑ ÿ®ŸäŸÜ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©.',
  },
  fr: {
    prefix: 'Documents du dossier:',
    questionLabel: 'Question:',
    analysisHint: 'Instructions: extraire les conditions l√©gales de chaque article, identifier les d√©lais et proc√©dures, relier les textes entre eux.',
  },
}

/**
 * Calcule les m√©triques de qualit√© des sources pour avertir le LLM
 */
function computeSourceQualityMetrics(sources: ChatSource[]): {
  averageSimilarity: number
  qualityLevel: 'high' | 'medium' | 'low'
  warningMessage: string | null
} {
  if (sources.length === 0) {
    return { averageSimilarity: 0, qualityLevel: 'low', warningMessage: null }
  }
  const avg = sources.reduce((a, s) => a + s.similarity, 0) / sources.length

  if (avg >= 0.70) {
    return { averageSimilarity: avg, qualityLevel: 'high', warningMessage: null }
  }
  if (avg >= 0.55) {
    return {
      averageSimilarity: avg,
      qualityLevel: 'medium',
      warningMessage: `‚ö†Ô∏è AVERTISSEMENT: Les documents ci-dessous ont une pertinence MOYENNE (similarit√© ~${Math.round(avg * 100)}%). V√©rifie leur pertinence th√©matique avant de les citer. Si aucun ne correspond au domaine de la question, dis-le explicitement.`,
    }
  }
  return {
    averageSimilarity: avg,
    qualityLevel: 'low',
    warningMessage: `üö® ATTENTION: Les documents ci-dessous ont une FAIBLE pertinence (similarit√© ~${Math.round(avg * 100)}%).
Ils proviennent probablement d'un domaine juridique DIFF√âRENT de la question pos√©e.

INSTRUCTIONS STRICTES:
1. NE CITE PAS ces sources comme si elles r√©pondaient √† la question
2. NE CONSTRUIS PAS de raisonnement juridique bas√© sur ces sources
3. Indique clairement que la base de connaissances ne contient pas de documents pertinents
4. Fournis des orientations G√âN√âRALES bas√©es sur tes connaissances du droit tunisien
5. Recommande de consulter les textes officiels pour une r√©ponse pr√©cise`,
  }
}

// Labels bilingues pour le contexte RAG
const CONTEXT_LABELS = {
  ar: {
    jurisprudence: 'ÿßÿ¨ÿ™ŸáÿßÿØ ŸÇÿ∂ÿßÿ¶Ÿä',
    chamber: 'ÿßŸÑÿ∫ÿ±ŸÅÿ©',
    date: 'ÿßŸÑÿ™ÿßÿ±ŸäÿÆ',
    articles: 'ÿßŸÑŸÅÿµŸàŸÑ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ©',
    na: 'ÿ∫/ŸÖ',
    knowledgeBase: 'ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ©',
    document: 'Ÿàÿ´ŸäŸÇÿ©',
    noDocuments: 'ŸÑÿß ÿ™Ÿàÿ¨ÿØ Ÿàÿ´ÿßÿ¶ŸÇ ÿ∞ÿßÿ™ ÿµŸÑÿ©.',
    categoryLabels: {
      jurisprudence: 'ÿßÿ¨ÿ™ŸáÿßÿØ ŸÇÿ∂ÿßÿ¶Ÿä',
      code: 'ŸÇÿßŸÜŸàŸÜ',
      doctrine: 'ŸÅŸÇŸá',
      modele: 'ŸÜŸÖŸàÿ∞ÿ¨',
      autre: 'ÿ£ÿÆÿ±Ÿâ',
    } as Record<string, string>,
    defaultCategory: 'ŸÖÿ±ÿ¨ÿπ',
  },
  fr: {
    jurisprudence: 'Jurisprudence',
    chamber: 'Chambre',
    date: 'Date',
    articles: 'Articles cit√©s',
    na: 'N/D',
    knowledgeBase: 'Base de connaissances',
    document: 'Document',
    noDocuments: 'Aucun document pertinent trouv√©.',
    categoryLabels: {
      jurisprudence: 'Jurisprudence',
      code: 'Code',
      doctrine: 'Doctrine',
      modele: 'Mod√®le',
      autre: 'Autre',
    } as Record<string, string>,
    defaultCategory: 'R√©f√©rence',
  },
}

/**
 * Construit le contexte √† partir des sources avec limite de tokens
 * Les labels sont adapt√©s √† la langue d√©tect√©e de la question
 */
/**
 * Enrichit les m√©tadonn√©es d'une source avec les donn√©es structur√©es de la DB
 */
async function enrichSourceWithStructuredMetadata(source: ChatSource): Promise<any> {
  if (!source.documentId) return source.metadata

  try {
    const result = await db.query(
      `SELECT
        meta.tribunal_code,
        trib_tax.label_ar AS tribunal_label_ar,
        trib_tax.label_fr AS tribunal_label_fr,
        meta.chambre_code,
        chambre_tax.label_ar AS chambre_label_ar,
        chambre_tax.label_fr AS chambre_label_fr,
        meta.decision_date,
        meta.decision_number,
        meta.legal_basis,
        meta.solution,
        meta.extraction_confidence,
        -- Compteurs relations
        (SELECT COUNT(*) FROM kb_legal_relations WHERE source_kb_id = $1 AND validated = true) AS cites_count,
        (SELECT COUNT(*) FROM kb_legal_relations WHERE target_kb_id = $1 AND validated = true) AS cited_by_count
      FROM kb_structured_metadata meta
      LEFT JOIN legal_taxonomy trib_tax ON meta.tribunal_code = trib_tax.code
      LEFT JOIN legal_taxonomy chambre_tax ON meta.chambre_code = chambre_tax.code
      WHERE meta.knowledge_base_id = $1`,
      [source.documentId]
    )

    if (result.rows.length > 0) {
      const row = result.rows[0]
      return {
        ...source.metadata,
        structuredMetadata: {
          tribunalCode: row.tribunal_code,
          tribunalLabelAr: row.tribunal_label_ar,
          tribunalLabelFr: row.tribunal_label_fr,
          chambreCode: row.chambre_code,
          chambreLabelAr: row.chambre_label_ar,
          chambreLabelFr: row.chambre_label_fr,
          decisionDate: row.decision_date,
          decisionNumber: row.decision_number,
          legalBasis: row.legal_basis,
          solution: row.solution,
          extractionConfidence: row.extraction_confidence,
          citesCount: parseInt(row.cites_count || '0', 10),
          citedByCount: parseInt(row.cited_by_count || '0', 10),
        },
      }
    }
  } catch (error) {
    console.error('[RAG Context] Erreur enrichissement m√©tadonn√©es:', error)
  }

  return source.metadata
}

/**
 * Construit le contexte √† partir des sources avec m√©tadonn√©es enrichies
 *
 * @exported Pour tests unitaires
 */
export async function buildContextFromSources(sources: ChatSource[], questionLang?: DetectedLanguage): Promise<string> {
  // Choisir les labels selon la langue
  const lang = questionLang === 'ar' ? 'ar' : 'fr'
  const labels = CONTEXT_LABELS[lang]

  if (sources.length === 0) {
    return labels.noDocuments
  }

  const contextParts: string[] = []
  let totalTokens = 0
  let sourcesUsed = 0

  // Enrichir sources avec m√©tadonn√©es structur√©es (batch - une seule requ√™te SQL)
  const metadataMap = await batchEnrichSourcesWithMetadata(sources)

  const enrichedSources = sources.map((source) => {
    if (!source.documentId) return source

    const batchMetadata = metadataMap.get(source.documentId)
    if (batchMetadata) {
      return {
        ...source,
        metadata: {
          ...source.metadata,
          ...batchMetadata,
        },
      }
    }
    return source
  })

  for (let i = 0; i < enrichedSources.length; i++) {
    const source = enrichedSources[i]
    const meta = source.metadata as any
    const sourceType = meta?.type
    const structuredMeta = meta?.structuredMetadata

    // Indicateur de pertinence visible par le LLM
    const relevanceLabel = source.similarity >= 0.75 ? '‚úÖ Tr√®s pertinent'
      : source.similarity >= 0.60 ? '‚ö†Ô∏è Pertinence moyenne'
      : '‚ùå Pertinence faible'
    const relevancePct = `${Math.round(source.similarity * 100)}%`

    // Labels fixes [Source-N], [Juris-N], [KB-N] ‚Äî compatibles avec le regex frontend
    let part: string
    if (sourceType === 'jurisprudence') {
      // Format enrichi pour jurisprudence
      let enrichedHeader = `[Juris-${i + 1}] ${source.documentName} (${relevanceLabel} - ${relevancePct})\n`

      // Ajouter m√©tadonn√©es structur√©es si disponibles
      if (structuredMeta) {
        const tribunalLabel = lang === 'ar' ? structuredMeta.tribunalLabelAr : structuredMeta.tribunalLabelFr
        const chambreLabel = lang === 'ar' ? structuredMeta.chambreLabelAr : structuredMeta.chambreLabelFr

        enrichedHeader += lang === 'ar' ? 'üèõÔ∏è ' : 'üèõÔ∏è '
        enrichedHeader += `${lang === 'ar' ? 'ÿßŸÑŸÖÿ≠ŸÉŸÖÿ©' : 'Tribunal'}: ${tribunalLabel || labels.na}\n`

        if (chambreLabel) {
          enrichedHeader += lang === 'ar' ? '‚öñÔ∏è ' : '‚öñÔ∏è '
          enrichedHeader += `${labels.chamber}: ${chambreLabel}\n`
        }

        if (structuredMeta.decisionDate) {
          enrichedHeader += 'üìÖ '
          enrichedHeader += `${labels.date}: ${new Date(structuredMeta.decisionDate).toLocaleDateString(lang === 'ar' ? 'ar-TN' : 'fr-TN')}\n`
        }

        if (structuredMeta.decisionNumber) {
          enrichedHeader += lang === 'ar' ? 'üìã ÿπÿØÿØ ÿßŸÑŸÇÿ±ÿßÿ±: ' : 'üìã N¬∞ d√©cision: '
          enrichedHeader += `${structuredMeta.decisionNumber}\n`
        }

        if (structuredMeta.legalBasis && structuredMeta.legalBasis.length > 0) {
          enrichedHeader += 'üìö '
          enrichedHeader += `${labels.articles}: ${structuredMeta.legalBasis.join(', ')}\n`
        }

        if (structuredMeta.solution) {
          enrichedHeader += lang === 'ar' ? '‚úÖ ÿßŸÑŸÖŸÜÿ∑ŸàŸÇ: ' : '‚úÖ Solution: '
          enrichedHeader += `${structuredMeta.solution}\n`
        }

        // Relations juridiques
        if (structuredMeta.citesCount > 0 || structuredMeta.citedByCount > 0) {
          enrichedHeader += 'üîó '
          enrichedHeader += lang === 'ar' ? 'ÿπŸÑÿßŸÇÿßÿ™: ' : 'Relations: '
          if (structuredMeta.citesCount > 0) {
            enrichedHeader += lang === 'ar' ? `Ÿäÿ¥Ÿäÿ± ÿ•ŸÑŸâ ${structuredMeta.citesCount}` : `Cite ${structuredMeta.citesCount}`
          }
          if (structuredMeta.citedByCount > 0) {
            if (structuredMeta.citesCount > 0) enrichedHeader += ', '
            enrichedHeader += lang === 'ar' ? `ŸÖÿ¥ÿßÿ± ÿ•ŸÑŸäŸá ŸÖŸÜ ${structuredMeta.citedByCount}` : `Cit√© par ${structuredMeta.citedByCount}`
          }
          enrichedHeader += '\n'
        }
      } else {
        // Fallback sur m√©tadonn√©es legacy
        enrichedHeader += `${labels.chamber}: ${meta?.chamber || labels.na}, ${labels.date}: ${meta?.date || labels.na}\n`
        enrichedHeader += `${labels.articles}: ${meta?.articles?.join(', ') || labels.na}\n`
      }

      part = enrichedHeader + '\n' + source.chunkContent
    } else if (meta?.sourceType === 'legal_document' || meta?.citationKey) {
      // Format enrichi pour documents juridiques consolid√©s
      let enrichedHeader = `[KB-${i + 1}] ${source.documentName} (${relevanceLabel} - ${relevancePct})\n`
      enrichedHeader += `üìå ${lang === 'ar' ? 'ÿßŸÑŸÖÿµÿØÿ±' : 'Source'}: ${meta.codeName || meta.citationKey || source.documentName}\n`

      if (meta.articleNumber) {
        enrichedHeader += `‚öñÔ∏è ${lang === 'ar' ? 'ÿßŸÑŸÅÿµŸÑ' : 'Article'} ${meta.articleNumber}\n`
      }

      if (meta.sourceUrl) {
        enrichedHeader += `üîó ${lang === 'ar' ? 'ÿßŸÑÿ±ÿßÿ®ÿ∑' : 'Lien'}: ${meta.sourceUrl}\n`
      }

      if (meta.lastVerifiedAt) {
        enrichedHeader += `üìÖ ${lang === 'ar' ? 'ÿ¢ÿÆÿ± ÿ™ÿ≠ŸÇŸÇ' : 'Derni√®re v√©rification'}: ${new Date(meta.lastVerifiedAt).toLocaleDateString(lang === 'ar' ? 'ar-TN' : 'fr-TN')}\n`
      }

      if (meta.isAbrogated) {
        enrichedHeader += `‚ö†Ô∏è ${lang === 'ar' ? 'ŸÖŸÑÿ∫Ÿâ' : 'Abrog√©'}\n`
      }

      if (meta.amendments && Array.isArray(meta.amendments)) {
        for (const amendment of meta.amendments.slice(0, 3)) {
          enrichedHeader += `üîÑ ${lang === 'ar' ? 'ÿ™ŸÜŸÇŸäÿ≠' : 'Modifi√© par'}: ${amendment}\n`
        }
      }

      part = enrichedHeader + '\n' + source.chunkContent
    } else if (sourceType === 'knowledge_base') {
      let enrichedHeader = `[KB-${i + 1}] ${source.documentName} (${relevanceLabel} - ${relevancePct})\n`

      // Ajouter m√©tadonn√©es structur√©es KB si disponibles
      if (structuredMeta) {
        if (structuredMeta.author) {
          enrichedHeader += lang === 'ar' ? '‚úçÔ∏è ÿßŸÑŸÖÿ§ŸÑŸÅ: ' : '‚úçÔ∏è Auteur: '
          enrichedHeader += `${structuredMeta.author}\n`
        }

        if (structuredMeta.publicationDate) {
          enrichedHeader += 'üìÖ '
          enrichedHeader += `${labels.date}: ${new Date(structuredMeta.publicationDate).toLocaleDateString(lang === 'ar' ? 'ar-TN' : 'fr-TN')}\n`
        }

        if (structuredMeta.keywords && structuredMeta.keywords.length > 0) {
          enrichedHeader += lang === 'ar' ? 'üîë ŸÉŸÑŸÖÿßÿ™ ŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©: ' : 'üîë Mots-cl√©s: '
          enrichedHeader += `${structuredMeta.keywords.join(', ')}\n`
        }
      }

      part = enrichedHeader + '\n' + source.chunkContent
    } else {
      part = `[Source-${i + 1}] ${source.documentName} (${relevanceLabel} - ${relevancePct})\n\n` + source.chunkContent
    }

    // ‚îÄ‚îÄ SPRINT 2 : Avertissement OCR faible confiance ‚îÄ‚îÄ
    if (meta?.ocr_low_confidence === true) {
      const ocrConf = meta.ocr_page_confidence as number | undefined
      const ocrWarning = lang === 'ar'
        ? `‚ö†Ô∏è ŸÖÿµÿØÿ± OCR (ŸÖŸàÿ´ŸàŸÇŸäÿ© ŸÖŸÜÿÆŸÅÿ∂ÿ©${ocrConf !== undefined ? ` - ${ocrConf.toFixed(0)}%` : ''} - ŸäŸèÿ±ÿ¨Ÿâ ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿßŸÑÿ£ÿµŸÑ)\n`
        : `‚ö†Ô∏è Source OCR (fiabilit√© faible${ocrConf !== undefined ? ` - ${ocrConf.toFixed(0)}%` : ''} - √† v√©rifier sur original)\n`
      part = ocrWarning + part
    }

    // ‚îÄ‚îÄ SPRINT 3 : Pr√©fixe pour chunks TABLE ‚îÄ‚îÄ
    if (meta?.chunk_type === 'table') {
      part = `[TABLE]\n${part}`
    }

    const partTokens = countTokens(part)
    const separatorTokens = contextParts.length > 0 ? countTokens('\n\n---\n\n') : 0

    // V√©rifier si on d√©passe la limite
    if (totalTokens + partTokens + separatorTokens > RAG_MAX_CONTEXT_TOKENS) {
      console.log(`[RAG Context] Limite atteinte: ${sourcesUsed}/${sources.length} sources, ~${totalTokens} tokens`)
      break
    }

    contextParts.push(part)
    totalTokens += partTokens + separatorTokens
    sourcesUsed++
  }

  console.log(`[RAG Context] ${sourcesUsed}/${sources.length} sources, ~${totalTokens} tokens, m√©tadonn√©es enrichies`)

  // Grouper les sources par type pour faciliter le croisement par le LLM
  // On garde les index originaux pour pr√©server le num√©rotage [KB-N]
  const grouped: { codes: string[]; jurisprudence: string[]; doctrine: string[]; other: string[] } = {
    codes: [], jurisprudence: [], doctrine: [], other: [],
  }

  for (let i = 0; i < contextParts.length; i++) {
    const source = enrichedSources[i]
    const meta = source?.metadata as any
    const sourceType = meta?.type
    const category = meta?.category

    if (sourceType === 'jurisprudence') {
      grouped.jurisprudence.push(contextParts[i])
    } else if (category === 'codes' || category === 'codes_juridiques') {
      grouped.codes.push(contextParts[i])
    } else if (category === 'doctrine' || category === 'articles_juridiques') {
      grouped.doctrine.push(contextParts[i])
    } else {
      grouped.other.push(contextParts[i])
    }
  }

  // Si tout est dans "other" (pas de m√©tadonn√©es type/category), retourner en ordre original
  if (grouped.codes.length === 0 && grouped.jurisprudence.length === 0 && grouped.doctrine.length === 0) {
    return contextParts.join('\n\n---\n\n')
  }

  // Construire le contexte group√© avec headers (r√©utilise `lang` d√©j√† d√©clar√© plus haut)
  const sections: string[] = []

  if (grouped.codes.length > 0) {
    const header = lang === 'ar' ? 'üìö ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©' : 'üìö Textes juridiques'
    sections.push(`${header}\n\n${grouped.codes.join('\n\n---\n\n')}`)
  }
  if (grouped.jurisprudence.length > 0) {
    const header = lang === 'ar' ? '‚öñÔ∏è ÿßŸÑÿßÿ¨ÿ™ŸáÿßÿØ ÿßŸÑŸÇÿ∂ÿßÿ¶Ÿä' : '‚öñÔ∏è Jurisprudence'
    sections.push(`${header}\n\n${grouped.jurisprudence.join('\n\n---\n\n')}`)
  }
  if (grouped.doctrine.length > 0) {
    const header = lang === 'ar' ? 'üìñ ÿßŸÑŸÅŸÇŸá ŸàÿßŸÑŸÖŸÇÿßŸÑÿßÿ™' : 'üìñ Doctrine'
    sections.push(`${header}\n\n${grouped.doctrine.join('\n\n---\n\n')}`)
  }
  if (grouped.other.length > 0) {
    const header = lang === 'ar' ? 'üìÑ ŸÖÿµÿßÿØÿ± ÿ£ÿÆÿ±Ÿâ' : 'üìÑ Autres sources'
    sections.push(`${header}\n\n${grouped.other.join('\n\n---\n\n')}`)
  }

  return sections.join('\n\n===\n\n')
}

/**
 * R√©cup√®re l'historique de conversation pour le contexte (version simple)
 */
async function getConversationHistory(
  conversationId: string,
  limit: number = 10
): Promise<ConversationMessage[]> {
  const result = await db.query(
    `SELECT role, content
     FROM chat_messages
     WHERE conversation_id = $1
       AND role IN ('user', 'assistant')
     ORDER BY created_at DESC
     LIMIT $2`,
    [conversationId, limit]
  )

  // Inverser pour avoir l'ordre chronologique
  return result.rows
    .reverse()
    .map((row) => ({
      role: row.role as 'user' | 'assistant',
      content: row.content,
    }))
}

/**
 * R√©cup√®re l'historique de conversation avec r√©sum√© si disponible
 * Retourne le r√©sum√© + les messages r√©cents pour un contexte optimal
 */
async function getConversationHistoryWithSummary(
  conversationId: string,
  recentLimit: number = SUMMARY_CONFIG.recentMessagesLimit
): Promise<{
  summary: string | null
  messages: ConversationMessage[]
  totalCount: number
}> {
  const context = await getConversationContext(conversationId, recentLimit)

  return {
    summary: context.summary,
    messages: context.messages.map((m) => ({
      role: m.role,
      content: m.content,
    })),
    totalCount: context.totalCount,
  }
}

// =============================================================================
// SANITIZER CITATIONS ‚Äî Supprime les citations invent√©es par le LLM
// =============================================================================

/**
 * Supprime les citations dont le num√©ro d√©passe le nombre de sources r√©elles.
 * Emp√™che le LLM d'halluciner des [Source-5] quand il n'y a que 3 sources.
 *
 * @exported Pour tests unitaires
 */
export function sanitizeCitations(answer: string, sourceCount: number): string {
  return answer.replace(
    /\[(Source|KB|Juris)-?(\d+)\]/g,
    (fullMatch, _type: string, numStr: string) => {
      const num = parseInt(numStr, 10)
      return (num >= 1 && num <= sourceCount) ? fullMatch : ''
    }
  )
}

// =============================================================================
// FONCTION PRINCIPALE: R√âPONDRE √Ä UNE QUESTION
// =============================================================================

/**
 * R√©pond √† une question en utilisant le pipeline RAG complet
 */
export async function answerQuestion(
  question: string,
  userId: string,
  options: ChatOptions = {}
): Promise<ChatResponse> {
  const startTotal = Date.now()

  // Initialiser logger structur√© (ou utiliser celui fourni)
  const logger = options.logger || new RAGLogger(undefined, { userId, operation: options.operationName })
  logger.addContext('question', question.substring(0, 100)) // Truncate pour √©viter logs massifs
  logger.info('search', 'Pipeline RAG d√©marr√©', {
    enableExpansion: ENABLE_QUERY_EXPANSION,
    operationName: options.operationName,
  })

  if (!isChatEnabled()) {
    logger.error('search', 'Chat IA d√©sactiv√©')
    throw new Error('Chat IA d√©sactiv√© (activer OLLAMA_ENABLED ou configurer GROQ_API_KEY)')
  }

  const provider = getChatProvider()
  logger.addContext('provider', provider)

  // M√©triques RAG
  let searchTimeMs = 0
  let cacheHit = false

  // 1. Rechercher le contexte pertinent (bilingue si activ√©) avec fallback d√©grad√©
  let sources: ChatSource[] = []
  let isDegradedMode = false

  const startSearch = Date.now()
  try {
    const searchResult = ENABLE_QUERY_EXPANSION
      ? await searchRelevantContextBilingual(question, userId, options)
      : await searchRelevantContext(question, userId, options)
    sources = searchResult.sources
    cacheHit = searchResult.cacheHit
    searchTimeMs = Date.now() - startSearch
  } catch (error) {
    // Mode d√©grad√©: retourner une erreur claire au lieu de continuer sans contexte
    // √âvite les hallucinations juridiques en mode sans source
    logger.error('search', 'Erreur recherche contexte - Sources indisponibles', error)
    isDegradedMode = true
    sources = []
    searchTimeMs = Date.now() - startSearch
  }

  // 2. Si la recherche a r√©ussi mais n'a trouv√© aucune source pertinente,
  // retourner un message clair au lieu d'appeler le LLM (√©vite les hallucinations)
  if (!isDegradedMode && sources.length === 0) {
    const noSourcesLang = detectLanguage(question)
    console.warn(`[RAG Diagnostic] üîç Aucune source trouv√©e pour requ√™te:`, {
      queryLength: question.length,
      language: noSourcesLang,
      queryPreview: question.substring(0, 100) + (question.length > 100 ? '...' : ''),
      searchTimeMs,
      enableExpansion: ENABLE_QUERY_EXPANSION,
    })
    const noSourcesMessage = noSourcesLang === 'fr'
      ? 'Je n\'ai trouv√© aucun document pertinent pour votre question. Veuillez reformuler ou v√©rifier que les documents n√©cessaires ont √©t√© t√©l√©vers√©s.'
      : 'ŸÑŸÖ ÿ£ÿ¨ÿØ Ÿàÿ´ÿßÿ¶ŸÇ ÿ∞ÿßÿ™ ÿµŸÑÿ© ÿ®ÿ≥ÿ§ÿßŸÑŸÉ ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ£Ÿà ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ±ŸÅÿπ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿßŸÑŸÖŸàÿ∂Ÿàÿπ.'

    return {
      answer: noSourcesMessage,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'none',
      conversationId: options.conversationId,
    }
  }

  // 3. Construire le contexte (bloquer si mode d√©grad√© pour √©viter les hallucinations)
  if (isDegradedMode) {
    // Enregistrer la m√©trique d'erreur
    const totalTimeMs = Date.now() - startTotal
    recordRAGMetric({
      searchTimeMs,
      llmTimeMs: 0,
      totalTimeMs,
      inputTokens: 0,
      outputTokens: 0,
      resultsCount: 0,
      cacheHit: false,
      degradedMode: true,
      provider: provider || 'unknown',
      error: 'Sources indisponibles - mode d√©grad√© bloqu√©',
    })

    // Retourner un message explicite au lieu de throw (√©vite 500 error)
    const degradedLang = detectLanguage(question)
    const degradedMessage = degradedLang === 'fr'
      ? 'Les sources juridiques sont temporairement indisponibles. Veuillez r√©essayer dans quelques instants.'
      : 'ÿßŸÑŸÖÿµÿßÿØÿ± ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±ÿ© ŸÖÿ§ŸÇÿ™Ÿãÿß. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ÿ®ÿπÿØ ŸÇŸÑŸäŸÑ.'

    return {
      answer: degradedMessage,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'degraded',
      conversationId: options.conversationId,
    }
  }

  // D√©tecter la langue de la question pour adapter les labels du contexte
  const questionLang = detectLanguage(question)
  const context = await buildContextFromSources(sources, questionLang)

  // Calculer m√©triques qualit√© et injecter avertissement si n√©cessaire
  const qualityMetrics = computeSourceQualityMetrics(sources)

  // B1: Abstention progressive ‚Äî quality gate √† 3 niveaux (zone grise 0.30-0.40)
  // < 0.30 ‚Üí abstention directe (sources non pertinentes)
  // 0.30-0.40 ‚Üí zone grise : accept√© si ‚â•2 sources, sinon abstention
  // ‚â• 0.40 ‚Üí accept√© (comportement standard)
  const avg = qualityMetrics.averageSimilarity
  const isHardAbstention = avg < 0.30
  const isGreyZone = avg >= 0.30 && avg < 0.40 && qualityMetrics.qualityLevel === 'low'
  const isGreyZoneAbstention = isGreyZone && sources.length < 2

  if (isHardAbstention || isGreyZoneAbstention) {
    const abstentionReason = isHardAbstention
      ? `Similarit√© ${Math.round(avg * 100)}% < 30% (sources non pertinentes)`
      : `Zone grise: similarit√© ${Math.round(avg * 100)}% (30-40%) avec seulement ${sources.length} source(s)`
    console.log(`[RAG] Abstention: ${abstentionReason}`)
    const abstentionMsg = questionLang === 'fr'
      ? 'Je n\'ai pas trouv√© de sources suffisamment pertinentes dans la base de connaissances pour r√©pondre √† cette question de mani√®re fiable. Je vous recommande de consulter directement les textes juridiques officiels ou un professionnel du droit.'
      : 'ŸÑŸÖ ÿ£ÿ¨ÿØ ŸÖÿµÿßÿØÿ± ŸÉÿßŸÅŸäÿ© ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿ¥ŸÉŸÑ ŸÖŸàÿ´ŸàŸÇ. ÿ£ŸÜÿµÿ≠ŸÉ ÿ®ÿßŸÑÿ±ÿ¨Ÿàÿπ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ•ŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ÿ£Ÿà ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ© ŸÖÿÆÿ™ÿµ ŸÅŸä ÿßŸÑŸÇÿßŸÜŸàŸÜ.'
    return {
      answer: abstentionMsg,
      sources: [],
      tokensUsed: { input: 0, output: 0, total: 0 },
      model: 'abstained',
      conversationId: options.conversationId,
      qualityIndicator: 'low',
      averageSimilarity: avg,
      abstentionReason,
    }
  }

  // Zone grise accept√©e (0.30-0.40 avec ‚â•2 sources) ‚Üí log avertissement
  if (isGreyZone) {
    console.log(`[RAG] Zone grise accept√©e: similarit√© ${Math.round(avg * 100)}%, ${sources.length} sources ‚Äî r√©ponse avec avertissement`)
  }

  let contextWithWarning = context
  if (qualityMetrics.warningMessage) {
    contextWithWarning = `${qualityMetrics.warningMessage}\n\n---\n\n${context}`
    logger.warn('search', 'Low quality sources', {
      averageSimilarity: qualityMetrics.averageSimilarity,
      qualityLevel: qualityMetrics.qualityLevel,
    })
  }

  // Phase 3b: Avertissement conditionnel si domaine principal non couvert par les sources
  // D√©tection rapide bas√©e sur les cat√©gories des sources (pas d'appel LLM suppl√©mentaire)
  const sourceCategories = new Set(
    sources.map(s => (s.metadata as Record<string, unknown>)?.category).filter(Boolean) as string[]
  )
  // Si toutes les sources proviennent d'un seul domaine et que la qualit√© est moyenne/basse,
  // injecter un avertissement pour d√©clencher le raisonnement conditionnel
  if (sources.length > 0 && qualityMetrics.qualityLevel !== 'high' && sourceCategories.size <= 1) {
    contextWithWarning = `‚ö†Ô∏è ÿ™ŸÜÿ®ŸäŸá: ÿßŸÑŸÖÿµÿßÿØÿ± ÿßŸÑŸÖÿ™ŸàŸÅÿ±ÿ© ŸÖÿ≠ÿØŸàÿØÿ© ÿßŸÑŸÜÿ∑ÿßŸÇ. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ±ÿ£Ÿä ÿßŸÑŸÖÿ¥ÿ±Ÿàÿ∑ ŸàŸÇÿØŸëŸÖ ÿßŸÅÿ™ÿ±ÿßÿ∂ÿßÿ™ ÿ®ÿØŸäŸÑÿ© ÿ•ŸÜ ŸÑÿ≤ŸÖ ÿßŸÑÿ£ŸÖÿ±.\n\n${contextWithWarning}`
  }

  // 3b. Multi-Chain Reasoning (optionnel ‚Äî activ√© via ENABLE_MULTI_CHAIN_CONSULTATION=true)
  // D√©clench√© uniquement pour les consultations formelles avec suffisamment de sources
  if (
    process.env.ENABLE_MULTI_CHAIN_CONSULTATION === 'true' &&
    options.operationName === 'dossiers-consultation' &&
    sources.length >= 3
  ) {
    try {
      const { multiChainReasoning } = await import('./multi-chain-legal-reasoning')
      const multiChainSources = sources.map((s) => ({
        id: s.documentId,
        content: s.chunkContent,
        category: (s.metadata?.category as string) || 'autre',
        metadata: s.metadata as Parameters<typeof multiChainReasoning>[0]['sources'][0]['metadata'],
      }))
      const mcResult = await multiChainReasoning({
        question,
        sources: multiChainSources,
        language: questionLang === 'fr' ? 'fr' : 'ar',
        usePremiumModel: options.usePremiumModel ?? false,
      })
      // Pr√©fixer le contexte RAG avec l'analyse multi-chain
      contextWithWarning = `## Analyse Multi-Chain (Raisonnement juridique structur√©)\n\n${mcResult.finalResponse}\n\n---\n\n${contextWithWarning}`
      logger.info('search', '[MultiChain] Raisonnement multi-chain int√©gr√©', {
        confidence: mcResult.overallConfidence,
        durationMs: mcResult.totalDurationMs,
        chains: mcResult.metadata.chainsExecuted,
      })
    } catch (mcError) {
      // Non-bloquant : si le multi-chain √©choue, on continue sans lui
      console.error('[MultiChain] Erreur (non-bloquant):', mcError instanceof Error ? mcError.message : mcError)
    }
  }

  // 3. R√©cup√©rer l'historique avec r√©sum√© si conversation existante
  let conversationHistory: ConversationMessage[] = []
  let conversationSummary: string | null = null
  let totalMessageCount = 0

  if (options.conversationId) {
    const historyContext = await getConversationHistoryWithSummary(
      options.conversationId,
      SUMMARY_CONFIG.recentMessagesLimit
    )
    conversationHistory = historyContext.messages
    conversationSummary = historyContext.summary
    totalMessageCount = historyContext.totalCount
  }

  // S√©lectionner le prompt syst√®me appropri√© selon le contexte
  // Par d√©faut: 'chat' si conversation, 'consultation' sinon
  const contextType: PromptContextType = options.contextType || (options.conversationId ? 'chat' : 'consultation')
  const supportedLang: SupportedLanguage = questionLang === 'fr' ? 'fr' : 'ar'
  const stance = options.stance ?? 'defense'
  const baseSystemPrompt = getSystemPromptForContext(contextType, supportedLang, stance)

  // 4. Construire les messages (format OpenAI-compatible pour Ollama/Groq)
  const messagesOpenAI: Array<{ role: 'user' | 'assistant' | 'system'; content: string }> = []

  // Injecter le r√©sum√© de la conversation si disponible (pour Ollama/Groq)
  if (conversationSummary) {
    messagesOpenAI.push({
      role: 'system',
      content: `[R√©sum√© de la conversation pr√©c√©dente]\n${conversationSummary}`,
    })
  }

  // Ajouter l'historique de conversation r√©cent
  for (const msg of conversationHistory) {
    messagesOpenAI.push({ role: msg.role, content: msg.content })
  }

  // Ajouter la nouvelle question avec le contexte (template bilingue)
  const msgTemplate = USER_MESSAGE_TEMPLATES[supportedLang]
  // Ajouter l'instruction d'analyse seulement pour le chat (pas consultation/structuration)
  const analysisLine = contextType === 'chat' ? `\n${msgTemplate.analysisHint}\n` : ''
  messagesOpenAI.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
  })

  // Messages format Anthropic (sans 'system' dans les messages)
  const messagesAnthropic: Array<{ role: 'user' | 'assistant'; content: string }> = []
  for (const msg of conversationHistory) {
    messagesAnthropic.push({ role: msg.role, content: msg.content })
  }
  messagesAnthropic.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
  })

  // Log si r√©sum√© utilis√©
  if (conversationSummary) {
    console.log(`[RAG] Conversation ${options.conversationId}: r√©sum√© inject√© (${totalMessageCount} messages total)`)
  }

  // Construire le syst√®me prompt avec r√©sum√© pour Anthropic
  const systemPromptWithSummary = conversationSummary
    ? `${baseSystemPrompt}\n\n[R√©sum√© de la conversation pr√©c√©dente]\n${conversationSummary}`
    : baseSystemPrompt

  console.log(`[RAG] Utilisation du prompt structur√©: contextType=${contextType}, langue=${supportedLang}`)

  let answer: string
  let tokensUsed: { input: number; output: number; total: number }
  let modelUsed: string
  let llmError: string | undefined
  let fallbackUsed = false

  // 5. Appeler le LLM avec fallback automatique sur erreur 429
  // Ollama est trait√© s√©par√©ment (local, pas de fallback cloud)
  // Pour les autres: Groq ‚Üí DeepSeek ‚Üí Anthropic ‚Üí OpenAI
  try {
    if (provider === 'ollama') {
      // Ollama (local, gratuit, illimit√©) - pas de fallback
      const client = getOllamaClient()

      // Adapter temp√©rature selon le contexte (consultation = plus pr√©cis)
      const promptConfig = PROMPT_CONFIG[contextType]
      const temperature = options.temperature ?? promptConfig.temperature

      const response = await client.chat.completions.create({
        model: aiConfig.ollama.chatModelDefault,
        max_tokens: promptConfig.maxTokens,
        messages: [
          { role: 'system', content: baseSystemPrompt },
          ...messagesOpenAI,
        ],
        temperature,
      })

      answer = response.choices[0]?.message?.content || ''
      tokensUsed = {
        input: response.usage?.prompt_tokens || 0,
        output: response.usage?.completion_tokens || 0,
        total: response.usage?.total_tokens || 0,
      }
      modelUsed = `ollama/${aiConfig.ollama.chatModelDefault}`
    } else {
      // Utiliser le service de fallback pour les providers cloud
      // Convertir les messages au format LLMMessage
      const llmMessages: LLMMessage[] = messagesOpenAI.map((m) => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content,
      }))

      // Adapter temp√©rature et maxTokens selon le contexte
      const promptConfig = PROMPT_CONFIG[contextType]
      const temperature = options.temperature ?? promptConfig.temperature

      const llmResponse = await callLLMWithFallback(
        llmMessages,
        {
          temperature,
          maxTokens: promptConfig.maxTokens,
          systemPrompt: systemPromptWithSummary,
          context: 'rag-chat', // Strat√©gie optimis√©e : Gemini ‚Üí DeepSeek ‚Üí Ollama
          operationName: options.operationName, // Configuration par op√©ration
        },
        options.usePremiumModel ?? false // Mode premium si demand√©
      )

      answer = llmResponse.answer
      tokensUsed = llmResponse.tokensUsed
      modelUsed = llmResponse.modelUsed
      fallbackUsed = llmResponse.fallbackUsed

      // ‚ú® PHASE 5: Citation-First Enforcement
      // Valider que la r√©ponse commence par une citation (seulement pour consultation, pas chat)
      if (contextType !== 'chat' && sources.length > 0) {
        const citationValidation = validateCitationFirst(answer)

        if (!citationValidation.valid) {
          console.warn(
            `[RAG] Citation-first violation detected: ${citationValidation.issue} ` +
            `(words before citation: ${citationValidation.metrics.wordsBeforeFirstCitation})`
          )

          // Conversion des sources au format attendu par l'enforcer
          const enforcerSources = sources.map((src, idx) => ({
            label: `[Source-${idx + 1}]`,
            content: src.chunkContent,
            title: src.documentName,
            category: src.metadata?.category as string | undefined,
          }))

          // Auto-correction
          const correctedAnswer = enforceCitationFirst(answer, enforcerSources)

          // V√©rifier si correction r√©ussie
          const correctedValidation = validateCitationFirst(correctedAnswer)

          if (correctedValidation.valid) {
            console.log(
              `[RAG] Citation-first enforced successfully ` +
              `(${citationValidation.issue} ‚Üí valid)`
            )
            answer = correctedAnswer
          } else {
            console.warn(
              `[RAG] Citation-first enforcement partial ` +
              `(issue: ${correctedValidation.issue})`
            )
            // Utiliser r√©ponse corrig√©e m√™me si pas parfaite (mieux que rien)
            answer = correctedAnswer
          }
        } else {
          console.log(
            `[RAG] Citation-first validation passed ` +
            `(${citationValidation.metrics.totalCitations} citations, ` +
            `${citationValidation.metrics.wordsBeforeFirstCitation} words before first)`
          )
        }
      }

      // Log si fallback utilis√©
      if (fallbackUsed && llmResponse.originalProvider) {
        console.log(
          `[RAG] Fallback LLM activ√©: ${llmResponse.originalProvider} ‚Üí ${llmResponse.provider}`
        )
      }
    }
  } catch (error) {
    // Enregistrer l'erreur LLM dans les m√©triques
    const totalTimeMs = Date.now() - startTotal
    const llmTimeMs = totalTimeMs - searchTimeMs
    llmError = `LLM error: ${error instanceof Error ? error.message : String(error)}`

    recordRAGMetric({
      searchTimeMs,
      llmTimeMs,
      totalTimeMs,
      inputTokens: 0,
      outputTokens: 0,
      resultsCount: sources.length,
      cacheHit,
      degradedMode: isDegradedMode,
      provider: provider || 'unknown',
      error: llmError,
    })

    logger.error('llm', 'Erreur LLM - Tous providers √©puis√©s', error)
    throw error // Re-throw pour que l'appelant puisse g√©rer
  }

  // D√©clencher g√©n√©ration de r√©sum√© en async si seuil atteint
  if (options.conversationId && totalMessageCount >= SUMMARY_CONFIG.triggerMessageCount) {
    triggerSummaryGenerationIfNeeded(options.conversationId).catch((err) =>
      logger.error('llm', 'Erreur trigger r√©sum√© conversation', err)
    )
  }

  // Logging m√©triques RAG structur√©
  const totalTimeMs = Date.now() - startTotal
  const llmTimeMs = totalTimeMs - searchTimeMs

  // Enregistrer dans le service de m√©triques
  recordRAGMetric({
    searchTimeMs,
    llmTimeMs,
    totalTimeMs,
    inputTokens: tokensUsed.input,
    outputTokens: tokensUsed.output,
    resultsCount: sources.length,
    cacheHit: cacheHit,
    degradedMode: isDegradedMode,
    provider: modelUsed,
  })

  console.log('RAG_METRICS', JSON.stringify({
    searchTimeMs,
    llmTimeMs,
    totalTimeMs,
    contextTokens: tokensUsed.input,
    outputTokens: tokensUsed.output,
    resultsCount: sources.length,
    degradedMode: isDegradedMode,
    provider: modelUsed,
    fallbackUsed,
    conversationId: options.conversationId || null,
    dossierId: options.dossierId || null,
  }))

  // Sanitizer: supprimer les citations invent√©es par le LLM
  answer = sanitizeCitations(answer, sources.length)

  // Phase 2.2+2.3 : Validation citations + d√©tection abrogations (parall√®le)
  let citationWarnings: string[] = []
  let abrogationWarnings: AbrogationWarning[] = []

  const [citationResult, abrogationResult] = await Promise.all([
    (process.env.ENABLE_CITATION_VALIDATION !== 'false')
      ? Promise.resolve(validateArticleCitations(answer, sources)).catch((error) => {
          logger.error('filter', 'Erreur validation citations', error)
          return null
        })
      : Promise.resolve(null),
    (process.env.ENABLE_ABROGATION_DETECTION !== 'false')
      ? detectAbrogatedReferences(answer, sources).catch((error) => {
          logger.error('abrogation', 'Erreur d√©tection abrogations', error)
          return [] as AbrogationWarning[]
        })
      : Promise.resolve([] as AbrogationWarning[]),
  ])

  if (citationResult?.warnings?.length) {
    logger.warn('filter', 'Citations non v√©rifi√©es d√©tect√©es', {
      count: citationResult.warnings.length,
      warnings: formatValidationWarnings(citationResult),
    })
    citationWarnings = citationResult.warnings.map(w => w.citation)
  }

  // Sprint 4: logger les sources sans citation_locator (non auditables)
  if (citationResult?.locatorsMissing && citationResult.locatorsMissing.length > 0) {
    logger.warn('filter', 'Sources sans citation_locator (non auditables)', {
      count: citationResult.locatorsMissing.length,
      sources: citationResult.locatorsMissing,
    })
  }

  abrogationWarnings = abrogationResult || []
  if (abrogationWarnings.length > 0) {
    logger.warn('abrogation', 'Lois abrog√©es d√©tect√©es dans la r√©ponse', {
      count: abrogationWarnings.length,
      warnings: formatAbrogationWarnings(abrogationWarnings),
    })
  }

  // Phase 4: Claim verification ‚Äî v√©rifier alignement claims‚Üîsources
  if (process.env.ENABLE_CLAIM_VERIFICATION !== 'false') {
    try {
      const claimResult = verifyClaimSourceAlignment(answer, sources)
      if (claimResult.unsupportedClaims.length > 0) {
        const ratio = claimResult.totalClaims > 0
          ? claimResult.supportedClaims / claimResult.totalClaims
          : 1
        if (ratio < 0.7) {
          answer += '\n\n‚ö†Ô∏è ÿ™ŸÜÿ®ŸäŸá: ÿ®ÿπÿ∂ ÿßŸÑÿßÿ≥ÿ™ŸÜÿ™ÿßÿ¨ÿßÿ™ ŸÇÿØ ŸÑÿß ÿ™ŸÉŸàŸÜ ŸÖÿØÿπŸàŸÖÿ© ÿ®ÿ¥ŸÉŸÑ ŸÉÿßŸÅŸç ÿ®ÿßŸÑŸÖÿµÿßÿØÿ± ÿßŸÑŸÖÿ™ŸàŸÅÿ±ÿ©. ŸäŸèÿ±ÿ¨Ÿâ ÿßŸÑÿ™ÿ≠ŸÇŸÇ.'
        }
        console.log(`[Claim Verify] ${claimResult.supportedClaims}/${claimResult.totalClaims} claims support√©es`)
      }
    } catch (error) {
      console.error('[Claim Verify] Erreur:', error instanceof Error ? error.message : error)
    }
  }

  // Sprint 3 RAG Audit-Proof : d√©tection cross-domaine + r√©g√©n√©ration automatique (1 tentative)
  let wasRegenerated = false
  let validationStatus: 'passed' | 'regenerated' | 'insufficient_sources' = 'passed'

  if (process.env.ENABLE_BRANCH_REGENERATION !== 'false') {
    try {
      // Le router est d√©j√† appel√© dans searchRelevantContext ‚Äî cache Redis hit ici (~0ms)
      const { routeQuery } = await import('./legal-router-service')
      const routerForValidation = await routeQuery(question, { maxTracks: 1 })
      const allowedBranches = routerForValidation.allowedBranches

      if (allowedBranches && allowedBranches.length > 0) {
        const branchCheck = verifyBranchAlignment(sources, allowedBranches)

        if (branchCheck.violatingCount > 0) {
          console.warn(
            `[RAG Sprint3] ${branchCheck.violatingCount}/${branchCheck.totalSources} sources hors-domaine d√©tect√©es:`,
            branchCheck.violatingSources.map(v => `${v.documentName} (branch=${v.branch})`).join(', ')
          )

          // Filtrer pour ne garder que sources dans le domaine autoris√©
          const alignedSources = sources.filter(s => {
            const branch = s.metadata?.branch as string | undefined
            if (!branch || branch === 'autre') return true // pas de branch = on garde
            return allowedBranches.includes(branch)
          })

          if (alignedSources.length >= 2) {
            // R√©g√©n√©rer avec sources filtr√©es (appel LLM synchrone, 1 tentative max)
            const filteredContext = await buildContextFromSources(alignedSources, questionLang)
            const regenMessages: LLMMessage[] = [
              {
                role: 'user',
                content: `${msgTemplate.prefix}\n\n[ÿ™ŸÜÿ®ŸäŸá: ÿ™ŸÖ ÿßÿ≥ÿ™ÿ®ÿπÿßÿØ ÿßŸÑŸÖÿµÿßÿØÿ± ÿÆÿßÿ±ÿ¨ ÿßŸÑŸÜÿ∑ÿßŸÇ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ŸÑŸÑÿ≥ÿ§ÿßŸÑ]\n\n${filteredContext}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
              },
            ]
            const regenResponse = await callLLMWithFallback(regenMessages, {
              systemPrompt: systemPromptWithSummary,
              operationName: options.operationName,
            })
            answer = sanitizeCitations(regenResponse.answer, alignedSources.length)
            sources = alignedSources
            wasRegenerated = true
            validationStatus = 'regenerated'
            console.log('[RAG Sprint3] ‚úÖ R√©ponse r√©g√©n√©r√©e avec sources filtr√©es par domaine')
          } else {
            validationStatus = 'insufficient_sources'
            console.warn('[RAG Sprint3] Sources filtr√©es insuffisantes (<2) ‚Äî pas de r√©g√©n√©ration')
          }
        }
      }
    } catch (error) {
      // Non-bloquant : si la r√©g√©n√©ration √©choue, on garde la r√©ponse originale
      console.error('[RAG Sprint3] Erreur validation branche:', error instanceof Error ? error.message : error)
    }
  }

  // Log m√©triques finales du pipeline complet
  logger.metrics({
    totalTimeMs: Date.now() - startTotal,
    searchTimeMs,
    sourcesCount: sources.length,
    tokensInput: tokensUsed.input,
    tokensOutput: tokensUsed.output,
    tokensTotal: tokensUsed.total,
    model: modelUsed,
    cacheHit,
    degradedMode: isDegradedMode,
    citationWarnings: citationWarnings.length,
    abrogationWarnings: abrogationWarnings.length,
    requestId: logger.getRequestId(),
  })

  return {
    answer,
    sources,
    tokensUsed,
    model: modelUsed,
    conversationId: options.conversationId,
    citationWarnings: citationWarnings.length > 0 ? citationWarnings : undefined,
    abrogationWarnings: abrogationWarnings.length > 0 ? abrogationWarnings : undefined,
    qualityIndicator: qualityMetrics.qualityLevel,
    averageSimilarity: qualityMetrics.averageSimilarity,
    wasRegenerated: wasRegenerated || undefined,
    validationStatus: validationStatus !== 'passed' ? validationStatus : undefined,
  }
}

// =============================================================================
// STREAMING NATIF GEMINI
// =============================================================================

/**
 * √âv√©nements √©mis par answerQuestionStream()
 */
export type StreamChunk =
  | { type: 'metadata'; sources: ChatSource[]; model: string; qualityIndicator: 'high' | 'medium' | 'low'; averageSimilarity: number }
  | { type: 'chunk'; text: string }
  | { type: 'done'; tokensUsed: { input: number; output: number; total: number } }
  | { type: 'error'; message: string }

/**
 * R√©pond √† une question en streaming natif Gemini.
 *
 * Phase 1 : Pipeline RAG complet (non-streaming) ‚Üí sources + contexte
 * Phase 2 : callLLMStream() ‚Üí yield chunks texte en temps r√©el (Groq ou Gemini)
 * Phase 3 : Post-processing (sanitize citations, m√©triques)
 *
 * Format des √©v√©nements :
 * - 'metadata' ‚Üí sources trouv√©es, √† envoyer en premier au client
 * - 'chunk'    ‚Üí fragment de texte g√©n√©r√© par Gemini
 * - 'done'     ‚Üí fin du stream avec tokensUsed estim√©s
 * - 'error'    ‚Üí erreur fatale (rate limit, timeout‚Ä¶)
 */
export async function* answerQuestionStream(
  question: string,
  userId: string,
  options: ChatOptions = {}
): AsyncGenerator<StreamChunk> {
  if (!isChatEnabled()) {
    yield { type: 'error', message: 'Chat IA d√©sactiv√© (activer OLLAMA_ENABLED ou configurer GROQ_API_KEY)' }
    return
  }

  // 1. Phase RAG (non-streaming)
  let sources: ChatSource[] = []
  try {
    const searchResult = ENABLE_QUERY_EXPANSION
      ? await searchRelevantContextBilingual(question, userId, options)
      : await searchRelevantContext(question, userId, options)
    sources = searchResult.sources
  } catch (error) {
    const errMsg = error instanceof Error ? error.message : 'Erreur recherche contexte'
    console.error('[RAG Stream] Erreur recherche:', errMsg)
    yield { type: 'error', message: errMsg }
    return
  }

  const questionLang = detectLanguage(question)

  // Aucune source ‚Üí r√©ponse rapide sans appel LLM
  if (sources.length === 0) {
    const noSourcesMsg = questionLang === 'fr'
      ? 'Je n\'ai trouv√© aucun document pertinent pour votre question. Veuillez reformuler.'
      : 'ŸÑŸÖ ÿ£ÿ¨ÿØ Ÿàÿ´ÿßÿ¶ŸÇ ÿ∞ÿßÿ™ ÿµŸÑÿ© ÿ®ÿ≥ÿ§ÿßŸÑŸÉ. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿßŸÑÿ≥ÿ§ÿßŸÑ.'
    yield { type: 'metadata', sources: [], model: 'groq/llama-3.3-70b-versatile', qualityIndicator: 'low', averageSimilarity: 0 }
    yield { type: 'chunk', text: noSourcesMsg }
    yield { type: 'done', tokensUsed: { input: 0, output: 0, total: 0 } }
    return
  }

  // 2. Construire le contexte RAG
  const context = await buildContextFromSources(sources, questionLang)
  const qualityMetrics = computeSourceQualityMetrics(sources)

  // B1: Abstention progressive en streaming ‚Äî quality gate √† 3 niveaux (zone grise 0.30-0.40)
  const streamAvg = qualityMetrics.averageSimilarity
  const streamIsHardAbstention = streamAvg < 0.30
  const streamIsGreyZone = streamAvg >= 0.30 && streamAvg < 0.40 && qualityMetrics.qualityLevel === 'low'
  const streamIsGreyZoneAbstention = streamIsGreyZone && sources.length < 2

  if (streamIsHardAbstention || streamIsGreyZoneAbstention) {
    const abstentionReason = streamIsHardAbstention
      ? `Similarit√© ${Math.round(streamAvg * 100)}% < 30%`
      : `Zone grise: similarit√© ${Math.round(streamAvg * 100)}% avec ${sources.length} source(s)`
    console.log(`[RAG Stream] Abstention: ${abstentionReason}`)
    const abstentionMsg = questionLang === 'fr'
      ? 'Je n\'ai pas trouv√© de sources suffisamment pertinentes dans la base de connaissances pour r√©pondre √† cette question de mani√®re fiable. Je vous recommande de consulter directement les textes juridiques officiels ou un professionnel du droit.'
      : 'ŸÑŸÖ ÿ£ÿ¨ÿØ ŸÖÿµÿßÿØÿ± ŸÉÿßŸÅŸäÿ© ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿ¥ŸÉŸÑ ŸÖŸàÿ´ŸàŸÇ. ÿ£ŸÜÿµÿ≠ŸÉ ÿ®ÿßŸÑÿ±ÿ¨Ÿàÿπ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ•ŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ÿ£Ÿà ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ© ŸÖÿÆÿ™ÿµ ŸÅŸä ÿßŸÑŸÇÿßŸÜŸàŸÜ.'
    yield { type: 'metadata', sources: [], model: 'abstained', qualityIndicator: 'low', averageSimilarity: streamAvg }
    yield { type: 'chunk', text: abstentionMsg }
    yield { type: 'done', tokensUsed: { input: 0, output: 0, total: 0 } }
    return
  }

  if (streamIsGreyZone) {
    console.log(`[RAG Stream] Zone grise accept√©e: similarit√© ${Math.round(streamAvg * 100)}%, ${sources.length} sources`)
  }

  const contextWithWarning = qualityMetrics.warningMessage
    ? `${qualityMetrics.warningMessage}\n\n---\n\n${context}`
    : context

  // 3. Historique conversation
  let conversationHistory: ConversationMessage[] = []
  let conversationSummary: string | null = null
  if (options.conversationId) {
    const historyContext = await getConversationHistoryWithSummary(
      options.conversationId,
      SUMMARY_CONFIG.recentMessagesLimit
    )
    conversationHistory = historyContext.messages
    conversationSummary = historyContext.summary
  }

  // 4. Construire messages
  const contextType: PromptContextType = options.contextType || (options.conversationId ? 'chat' : 'consultation')
  const supportedLang: SupportedLanguage = questionLang === 'fr' ? 'fr' : 'ar'
  const stance = options.stance ?? 'defense'
  const baseSystemPrompt = getSystemPromptForContext(contextType, supportedLang, stance)
  const systemPrompt = conversationSummary
    ? `${baseSystemPrompt}\n\n[R√©sum√© de la conversation pr√©c√©dente]\n${conversationSummary}`
    : baseSystemPrompt

  const messagesForLLM: Array<{ role: string; content: string }> = []
  for (const msg of conversationHistory) {
    messagesForLLM.push({ role: msg.role, content: msg.content })
  }
  const msgTemplate = USER_MESSAGE_TEMPLATES[supportedLang]
  // Ajouter l'instruction d'analyse seulement pour le chat
  const analysisLine = contextType === 'chat' ? `\n${msgTemplate.analysisHint}\n` : ''
  messagesForLLM.push({
    role: 'user',
    content: `${msgTemplate.prefix}\n\n${contextWithWarning}\n${analysisLine}\n---\n\n${msgTemplate.questionLabel} ${question}`,
  })

  // 5. Yield metadata (sources disponibles avant le stream LLM)
  const opName = options.operationName ?? 'assistant-ia'
  const streamProvider = getOperationProvider(opName)
  const streamModel = getOperationModel(opName)
  const modelName = `${streamProvider}/${streamModel}`
  yield {
    type: 'metadata',
    sources,
    model: modelName,
    qualityIndicator: qualityMetrics.qualityLevel,
    averageSimilarity: qualityMetrics.averageSimilarity,
  }

  // 6. Stream LLM ‚Üí yield chunks (Groq ou Gemini selon operations-config)
  const promptConfig = PROMPT_CONFIG[contextType]
  let fullText = ''
  const streamUsage: StreamTokenUsage = { input: 0, output: 0, total: 0 }
  try {
    const streamGen = callLLMStream(messagesForLLM, {
      temperature: options.temperature ?? promptConfig.temperature,
      maxTokens: promptConfig.maxTokens,
      operationName: options.operationName ?? 'assistant-ia',
      systemInstruction: systemPrompt,
    }, streamUsage)

    for await (const chunk of streamGen) {
      fullText += chunk
      yield { type: 'chunk', text: chunk }
    }
  } catch (error) {
    const errMsg = error instanceof Error ? error.message : 'Erreur streaming LLM'
    console.error('[RAG Stream] Erreur streaming:', errMsg)
    yield { type: 'error', message: errMsg }
    return
  }

  // 7. Post-processing : sanitize citations
  fullText = sanitizeCitations(fullText, sources.length)

  // Tokens : utiliser les stats r√©elles Groq si disponibles, sinon estimation
  const tokensUsed = streamUsage.total > 0
    ? streamUsage
    : { input: 0, output: Math.ceil(fullText.length / 4), total: Math.ceil(fullText.length / 4) }

  yield { type: 'done', tokensUsed }
}

// =============================================================================
// GESTION DES CONVERSATIONS
// =============================================================================

/**
 * Cr√©e une nouvelle conversation
 */
export async function createConversation(
  userId: string,
  dossierId?: string,
  title?: string
): Promise<string> {
  const result = await db.query(
    `INSERT INTO chat_conversations (user_id, dossier_id, title)
     VALUES ($1, $2, $3)
     RETURNING id`,
    [userId, dossierId || null, title || null]
  )

  return result.rows[0].id
}

/**
 * Sauvegarde un message dans une conversation
 */
export async function saveMessage(
  conversationId: string,
  role: 'user' | 'assistant',
  content: string,
  sources?: ChatSource[],
  tokensUsed?: number,
  model?: string,
  metadata?: Record<string, any>
): Promise<string> {
  const result = await db.query(
    `INSERT INTO chat_messages (conversation_id, role, content, sources, tokens_used, model, metadata)
     VALUES ($1, $2, $3, $4, $5, $6, $7)
     RETURNING id`,
    [
      conversationId,
      role,
      content,
      sources ? JSON.stringify(sources) : null,
      tokensUsed != null ? tokensUsed : null,
      model || null,
      metadata ? JSON.stringify(metadata) : null,
    ]
  )

  // Mettre √† jour la conversation
  await db.query(
    `UPDATE chat_conversations SET updated_at = NOW() WHERE id = $1`,
    [conversationId]
  )

  return result.rows[0].id
}

/**
 * R√©cup√®re les conversations d'un utilisateur
 */
export async function getUserConversations(
  userId: string,
  dossierId?: string,
  limit: number = 20,
  actionType?: 'chat' | 'structure' | 'consult'
): Promise<
  Array<{
    id: string
    title: string | null
    dossierId: string | null
    dossierNumero: string | null
    messageCount: number
    lastMessageAt: Date
    createdAt: Date
  }>
> {
  let sql = `
    SELECT
      c.id,
      c.title,
      c.dossier_id,
      d.numero as dossier_numero,
      c.updated_at as last_message_at,
      c.created_at,
      (SELECT COUNT(*) FROM chat_messages WHERE conversation_id = c.id) as message_count
    FROM chat_conversations c
    LEFT JOIN dossiers d ON c.dossier_id = d.id
    WHERE c.user_id = $1
  `

  const params: (string | number)[] = [userId]
  let paramIndex = 2

  if (dossierId) {
    sql += ` AND c.dossier_id = $${paramIndex}`
    params.push(dossierId)
    paramIndex++
  }

  if (actionType) {
    if (actionType === 'chat') {
      // Mode 'chat' = par d√©faut : inclure aussi les conversations sans actionType (historiques)
      sql += ` AND (
        NOT EXISTS (
          SELECT 1 FROM chat_messages cm
          WHERE cm.conversation_id = c.id
            AND cm.metadata->>'actionType' IS NOT NULL
            AND cm.metadata->>'actionType' != 'chat'
          LIMIT 1
        )
      )`
    } else {
      // Modes 'structure'/'consult' : filtre strict
      sql += ` AND EXISTS (
        SELECT 1 FROM chat_messages cm
        WHERE cm.conversation_id = c.id
          AND cm.metadata->>'actionType' = $${paramIndex}
        LIMIT 1
      )`
      params.push(actionType)
      paramIndex++
    }
  }

  sql += ` ORDER BY c.updated_at DESC LIMIT $${paramIndex}`
  params.push(limit)

  const result = await db.query(sql, params)

  return result.rows.map((row) => ({
    id: row.id,
    title: row.title,
    dossierId: row.dossier_id,
    dossierNumero: row.dossier_numero,
    messageCount: parseInt(row.message_count),
    lastMessageAt: row.last_message_at,
    createdAt: row.created_at,
  }))
}

/**
 * Supprime une conversation
 */
export async function deleteConversation(
  conversationId: string,
  userId: string
): Promise<boolean> {
  const result = await db.query(
    `DELETE FROM chat_conversations
     WHERE id = $1 AND user_id = $2`,
    [conversationId, userId]
  )

  return (result.rowCount || 0) > 0
}

/**
 * G√©n√®re un titre automatique pour une conversation
 */
export async function generateConversationTitle(
  conversationId: string
): Promise<string> {
  // R√©cup√©rer le premier message utilisateur
  const result = await db.query(
    `SELECT content FROM chat_messages
     WHERE conversation_id = $1 AND role = 'user'
     ORDER BY created_at ASC
     LIMIT 1`,
    [conversationId]
  )

  if (result.rows.length === 0) {
    return 'Nouvelle conversation'
  }

  const firstMessage = result.rows[0].content
  // Tronquer et nettoyer pour faire un titre (supporte formats FR et AR)
  const title = firstMessage
    .replace(/(?:Documents du dossier|Ÿàÿ´ÿßÿ¶ŸÇ ŸÖÿ±ÿ¨ÿπŸäÿ©):[\s\S]*?---\s*(?:Question|ÿßŸÑÿ≥ÿ§ÿßŸÑ):\s*/i, '')
    .substring(0, 60)
    .trim()

  if (title.length === 60) {
    return title + '...'
  }

  return title || 'Nouvelle conversation'
}

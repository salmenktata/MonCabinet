/**
 * Service LLM - Mode No-Fallback
 *
 * Chaque op√©ration utilise UN SEUL provider fixe d√©fini dans operations-config.ts.
 * Si le provider √©choue ‚Üí throw + alerte email (pas de d√©gradation silencieuse).
 *
 * En cas d'urgence, LLM_FALLBACK_ENABLED=true r√©active le mode cascade legacy.
 *
 * Configuration d√©finitive RAG Haute Qualit√© (F√©vrier 2026)
 */

import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'
import { aiConfig, SYSTEM_PROMPTS } from './config'
import { callGemini, callGeminiStream, GeminiResponse } from './gemini-client'
import { getOperationConfig, getOperationProvider, getOperationModel, type OperationName } from './operations-config'
import { logger } from '@/lib/logger'
import { getRedisClient } from '@/lib/cache/redis'

// =============================================================================
// TYPES
// =============================================================================

export type LLMProvider = 'gemini' | 'groq' | 'deepseek' | 'anthropic' | 'ollama' | 'openai'

/**
 * Contextes d'utilisation IA (r√©trocompatibilit√©)
 */
export type AIContext =
  | 'rag-chat'
  | 'embeddings'
  | 'quality-analysis'
  | 'structuring'
  | 'translation'
  | 'web-scraping'
  | 'default'

export interface LLMMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

export interface LLMOptions {
  temperature?: number
  maxTokens?: number
  systemPrompt?: string
  /** @deprecated Utiliser operationName */
  context?: AIContext
  /** Type d'op√©ration pour configuration fixe (1 provider, pas de fallback) */
  operationName?: OperationName
}

export interface LLMResponse {
  answer: string
  tokensUsed: {
    input: number
    output: number
    total: number
  }
  modelUsed: string
  provider: LLMProvider
  fallbackUsed: boolean
  originalProvider?: LLMProvider
}

// =============================================================================
// CONFIGURATION
// =============================================================================

/** Kill switch d'urgence: r√©active le mode cascade legacy si true */
const LLM_FALLBACK_ENABLED = process.env.LLM_FALLBACK_ENABLED === 'true'

/** Ordre de fallback pour les r√©ponses chat : Gemini ‚Üí OpenAI ‚Üí Ollama */
const FALLBACK_ORDER: LLMProvider[] = ['gemini', 'openai', 'ollama']

// =============================================================================
// CIRCUIT BREAKER
// =============================================================================

interface CircuitBreakerState {
  failCount: number
  cooldownUntil: number // timestamp ms
}

/**
 * Circuit breaker persist√© dans Redis (P1 fix Feb 24, 2026).
 * Fallback in-memory si Redis indisponible (restart ne remet plus √† z√©ro).
 * Cl√© Redis : circuit:state:{provider} ‚Üí JSON CircuitBreakerState, TTL = cooldown
 */
const circuitBreakerMap = new Map<LLMProvider, CircuitBreakerState>() // fallback in-memory

const CIRCUIT_BREAKER_FAIL_THRESHOLD = 5
const CIRCUIT_BREAKER_INITIAL_COOLDOWN_MS = 60_000
const CIRCUIT_BREAKER_MAX_COOLDOWN_MS = 300_000

function _redisKey(provider: LLMProvider): string {
  return `circuit:state:${provider}`
}

/** Lit l'√©tat du circuit breaker depuis Redis (ou fallback in-memory) */
async function _getCircuitState(provider: LLMProvider): Promise<CircuitBreakerState> {
  try {
    const redis = await getRedisClient()
    if (redis) {
      const raw = await redis.get(_redisKey(provider))
      if (raw) return JSON.parse(raw) as CircuitBreakerState
    }
  } catch { /* Ignore Redis errors, use in-memory fallback */ }
  return circuitBreakerMap.get(provider) ?? { failCount: 0, cooldownUntil: 0 }
}

/** √âcrit l'√©tat du circuit breaker dans Redis ET in-memory */
async function _setCircuitState(provider: LLMProvider, state: CircuitBreakerState): Promise<void> {
  circuitBreakerMap.set(provider, state)
  try {
    const redis = await getRedisClient()
    if (redis) {
      const ttlSec = state.cooldownUntil > 0
        ? Math.ceil((state.cooldownUntil - Date.now()) / 1000) + 10
        : CIRCUIT_BREAKER_MAX_COOLDOWN_MS / 1000
      await redis.set(_redisKey(provider), JSON.stringify(state), { EX: Math.max(ttlSec, 60) })
    }
  } catch { /* Ignore Redis errors */ }
}

/**
 * Enregistre un √©chec pour un provider. Ouvre le circuit si seuil atteint.
 */
function recordProviderFailure(provider: LLMProvider): void {
  _getCircuitState(provider).then(state => {
    state.failCount += 1

    if (state.failCount >= CIRCUIT_BREAKER_FAIL_THRESHOLD) {
      // Backoff exponentiel : 60s ‚Üí 120s ‚Üí 240s ‚Üí 300s max
      const exponent = Math.min(state.failCount - CIRCUIT_BREAKER_FAIL_THRESHOLD, 3)
      const cooldownMs = Math.min(
        CIRCUIT_BREAKER_INITIAL_COOLDOWN_MS * Math.pow(2, exponent),
        CIRCUIT_BREAKER_MAX_COOLDOWN_MS
      )
      state.cooldownUntil = Date.now() + cooldownMs
      logger.warn(
        `[CircuitBreaker] üî¥ ${provider} ouvert apr√®s ${state.failCount} √©checs ‚Äî cooldown ${cooldownMs / 1000}s (persist√© Redis)`
      )
    }

    _setCircuitState(provider, state).catch(() => {})
  }).catch(() => {
    // Fallback synchrone in-memory si _getCircuitState √©choue
    const state = circuitBreakerMap.get(provider) ?? { failCount: 0, cooldownUntil: 0 }
    state.failCount += 1
    circuitBreakerMap.set(provider, state)
  })
}

/**
 * Enregistre un succ√®s pour un provider. R√©initialise le compteur d'√©checs.
 */
function recordProviderSuccess(provider: LLMProvider): void {
  _getCircuitState(provider).then(state => {
    if (state.failCount > 0) {
      _setCircuitState(provider, { failCount: 0, cooldownUntil: 0 }).catch(() => {})
    }
  }).catch(() => {
    const state = circuitBreakerMap.get(provider)
    if (state && state.failCount > 0) {
      circuitBreakerMap.set(provider, { failCount: 0, cooldownUntil: 0 })
    }
  })
}

/**
 * V√©rifie si un provider est disponible selon son √©tat de circuit breaker.
 * Retourne false si le circuit est ouvert (en cooldown).
 * NOTE: version synchrone (lit le cache in-memory) pour ne pas bloquer le hot path.
 * La persistance Redis est async en arri√®re-plan.
 */
function isProviderCircuitClosed(provider: LLMProvider): boolean {
  const state = circuitBreakerMap.get(provider)
  if (!state) return true

  if (state.cooldownUntil > 0 && Date.now() < state.cooldownUntil) {
    const remainingSec = Math.ceil((state.cooldownUntil - Date.now()) / 1000)
    logger.warn(`[CircuitBreaker] ‚õî ${provider} en cooldown (${remainingSec}s restants)`)
    return false
  }

  // Cooldown expir√© ‚Üí circuit en half-open (on laisse passer 1 tentative)
  if (state.cooldownUntil > 0 && Date.now() >= state.cooldownUntil) {
    logger.info(`[CircuitBreaker] üü° ${provider} half-open ‚Äî tentative de r√©tablissement`)
  }

  return true
}

/**
 * Initialise le circuit breaker depuis Redis au d√©marrage.
 * Appel√© une fois √† l'import (fire-and-forget).
 */
async function _restoreCircuitStateFromRedis(): Promise<void> {
  try {
    const redis = await getRedisClient()
    if (!redis) return
    const providers: LLMProvider[] = ['gemini', 'groq', 'deepseek', 'anthropic', 'openai', 'ollama']
    await Promise.all(providers.map(async (p) => {
      const raw = await redis.get(_redisKey(p))
      if (raw) {
        const state = JSON.parse(raw) as CircuitBreakerState
        // Ne restaurer que si le cooldown est encore actif
        if (state.cooldownUntil > Date.now()) {
          circuitBreakerMap.set(p, state)
          const remainingSec = Math.ceil((state.cooldownUntil - Date.now()) / 1000)
          logger.info(`[CircuitBreaker] üîÑ ${p} restaur√© depuis Redis ‚Äî cooldown ${remainingSec}s restants`)
        }
      }
    }))
  } catch { /* Redis indisponible au d√©marrage ‚Äî d√©gradation gracieuse */ }
}

// Restaurer l'√©tat au d√©marrage (fire-and-forget)
_restoreCircuitStateFromRedis().catch(() => {})

// =============================================================================
// CLIENTS LLM (singletons)
// =============================================================================

let anthropicClient: Anthropic | null = null
let groqClient: OpenAI | null = null
let deepseekClient: OpenAI | null = null
let openaiClient: OpenAI | null = null

function getAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    anthropicClient = new Anthropic({ apiKey: aiConfig.anthropic.apiKey })
  }
  return anthropicClient
}

function getGroqClient(): OpenAI {
  if (!groqClient) {
    groqClient = new OpenAI({
      apiKey: aiConfig.groq.apiKey,
      baseURL: aiConfig.groq.baseUrl,
    })
  }
  return groqClient
}

function getDeepSeekClient(): OpenAI {
  if (!deepseekClient) {
    const apiKey = process.env.DEEPSEEK_API_KEY || aiConfig.deepseek.apiKey
    if (!apiKey) {
      throw new Error('DEEPSEEK_API_KEY non configur√©')
    }
    deepseekClient = new OpenAI({
      apiKey,
      baseURL: aiConfig.deepseek.baseUrl,
    })
  }
  return deepseekClient
}

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    const apiKey = process.env.OPENAI_API_KEY || aiConfig.openai?.apiKey
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY non configur√©')
    }
    openaiClient = new OpenAI({
      apiKey,
    })
  }
  return openaiClient
}

/**
 * Appelle Ollama directement via fetch
 */
async function callOllamaAPI(
  messages: Array<{ role: string; content: string }>,
  temperature: number,
  maxTokens: number
): Promise<{ content: string; tokens?: number }> {
  const model = aiConfig.ollama.chatModelDefault
  const timeout = aiConfig.ollama.chatTimeoutDefault

  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), timeout)

  try {
    const response = await fetch(`${aiConfig.ollama.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages,
        stream: false,
        think: false,
        options: {
          temperature,
          num_predict: maxTokens,
        },
      }),
      signal: controller.signal,
    })

    if (!response.ok) {
      throw new Error(`Erreur Ollama: ${response.status}`)
    }

    const data = await response.json()
    // Supprimer les balises <think>...</think> (qwen3 thinking mode r√©siduel)
    const rawContent: string = data.message?.content || ''
    const content = rawContent.replace(/<think>[\s\S]*?<\/think>\n?/g, '').trim()
    return {
      content,
      tokens: data.eval_count || 0,
    }
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`Timeout apr√®s ${timeout / 1000}s (mod√®le: ${model})`)
    }
    throw error
  } finally {
    clearTimeout(timeoutId)
  }
}

// =============================================================================
// HELPERS
// =============================================================================

/**
 * V√©rifie si l'erreur est une erreur de rate limit (429)
 */
export function isRateLimitError(error: unknown): boolean {
  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    return (
      message.includes('429') ||
      message.includes('rate limit') ||
      message.includes('rate_limit') ||
      message.includes('too many requests') ||
      message.includes('quota exceeded')
    )
  }

  if (typeof error === 'object' && error !== null) {
    const err = error as { status?: number; statusCode?: number }
    return err.status === 429 || err.statusCode === 429
  }

  return false
}

/**
 * V√©rifie si l'erreur est une erreur serveur ou timeout (r√©cup√©rable par fallback)
 */
export function isServerOrTimeoutError(error: unknown): boolean {
  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    return (
      message.includes('timeout') ||
      message.includes('timed out') ||
      message.includes('abort') ||
      message.includes('503') ||
      message.includes('502') ||
      message.includes('500') ||
      message.includes('service unavailable') ||
      message.includes('internal server error') ||
      message.includes('overloaded')
    )
  }
  if (typeof error === 'object' && error !== null) {
    const err = error as { status?: number; statusCode?: number }
    return [500, 502, 503, 504].includes(err.status ?? err.statusCode ?? 0)
  }
  return false
}

/**
 * Retourne la liste des providers disponibles (avec cl√© API configur√©e)
 * Note: v√©rifie uniquement la pr√©sence de cl√© ‚Äî utiliser checkProviderHealth() pour un ping r√©el
 */
export function getAvailableProviders(): LLMProvider[] {
  return FALLBACK_ORDER.filter((provider) => {
    switch (provider) {
      case 'gemini':
        return !!(process.env.GOOGLE_API_KEY || aiConfig.gemini.apiKey)
      case 'groq':
        return !!(process.env.GROQ_API_KEY || aiConfig.groq.apiKey)
      case 'deepseek':
        return !!(process.env.DEEPSEEK_API_KEY || aiConfig.deepseek.apiKey)
      case 'anthropic':
        return !!(process.env.ANTHROPIC_API_KEY || aiConfig.anthropic.apiKey)
      case 'openai':
        return !!(process.env.OPENAI_API_KEY || aiConfig.openai?.apiKey)
      case 'ollama':
        return process.env.OLLAMA_ENABLED === 'true' || aiConfig.ollama.enabled
      default:
        return false
    }
  })
}

// =============================================================================
// HEALTH CHECK PROVIDERS (Fix P2 Feb 24, 2026)
// =============================================================================

/** Cache en m√©moire des r√©sultats health check (TTL 5 min) */
const _healthCheckCache = new Map<LLMProvider, { ok: boolean; ts: number }>()
const HEALTH_CHECK_TTL_MS = 5 * 60 * 1000 // 5 min

/**
 * V√©rifie si un provider est r√©ellement op√©rationnel via un ping l√©ger.
 * R√©sultat mis en cache 5 min. Utilis√© pour d√©tecter cl√©s expir√©es / quotas.
 * Appel√© de mani√®re proactive (pas dans le hot path).
 */
export async function checkProviderHealth(provider: LLMProvider): Promise<boolean> {
  const cached = _healthCheckCache.get(provider)
  if (cached && Date.now() - cached.ts < HEALTH_CHECK_TTL_MS) {
    return cached.ok
  }

  let ok = false
  try {
    switch (provider) {
      case 'groq': {
        const client = getGroqClient()
        await client.chat.completions.create({
          model: aiConfig.groq.model,
          messages: [{ role: 'user', content: 'ping' }],
          max_tokens: 1,
          temperature: 0,
        })
        ok = true
        break
      }
      case 'gemini': {
        const resp = await callGemini(
          [{ role: 'user', content: 'ping' }],
          { temperature: 0, maxTokens: 1 }
        )
        ok = resp.answer !== undefined
        break
      }
      case 'openai': {
        const client = getOpenAIClient()
        await client.chat.completions.create({
          model: aiConfig.openai?.chatModel || 'gpt-4o-mini',
          messages: [{ role: 'user', content: 'ping' }],
          max_tokens: 1,
          temperature: 0,
        })
        ok = true
        break
      }
      case 'ollama': {
        const resp = await fetch(`${aiConfig.ollama.baseUrl}/api/tags`, { signal: AbortSignal.timeout(3000) })
        ok = resp.ok
        break
      }
      default:
        ok = getAvailableProviders().includes(provider)
    }
  } catch (error) {
    const msg = error instanceof Error ? error.message : String(error)
    const isQuotaOrAuth = msg.includes('401') || msg.includes('403') || msg.includes('quota') || msg.includes('billing')
    if (isQuotaOrAuth) {
      logger.error(`[HealthCheck] üîë ${provider} cl√© invalide/quota d√©pass√©: ${msg.substring(0, 80)}`)
      // Ouvrir le circuit breaker imm√©diatement
      _setCircuitState(provider, {
        failCount: CIRCUIT_BREAKER_FAIL_THRESHOLD,
        cooldownUntil: Date.now() + CIRCUIT_BREAKER_MAX_COOLDOWN_MS,
      }).catch(() => {})
    } else {
      logger.warn(`[HealthCheck] ${provider} ping √©chou√©: ${msg.substring(0, 80)}`)
    }
    ok = false
  }

  _healthCheckCache.set(provider, { ok, ts: Date.now() })
  logger.info(`[HealthCheck] ${provider}: ${ok ? '‚úÖ OK' : '‚ùå KO'}`)
  return ok
}

/**
 * V√©rifie tous les providers configur√©s et retourne un r√©sum√©.
 * √Ä appeler depuis un endpoint de monitoring ou un cron l√©ger.
 */
export async function checkAllProvidersHealth(): Promise<Record<LLMProvider, boolean>> {
  const providers = getAvailableProviders()
  const results = await Promise.allSettled(providers.map(p => checkProviderHealth(p)))
  const summary: Partial<Record<LLMProvider, boolean>> = {}
  providers.forEach((p, i) => {
    summary[p] = results[i].status === 'fulfilled' ? results[i].value : false
  })
  return summary as Record<LLMProvider, boolean>
}

// =============================================================================
// APPELS LLM PAR PROVIDER
// =============================================================================

/**
 * Appelle un provider sp√©cifique
 */
async function callProvider(
  provider: LLMProvider,
  messages: LLMMessage[],
  options: LLMOptions
): Promise<LLMResponse> {
  const systemPrompt = options.systemPrompt || SYSTEM_PROMPTS.qadhya
  const temperature = options.temperature ?? 0.3
  const maxTokens = options.maxTokens || aiConfig.anthropic.maxTokens

  const userMessages = messages.filter((m) => m.role !== 'system')

  switch (provider) {
    case 'gemini': {
      const geminiResponse: GeminiResponse = await callGemini(
        [{ role: 'system', content: systemPrompt }, ...userMessages],
        { temperature, maxTokens, systemInstruction: systemPrompt }
      )

      return {
        answer: geminiResponse.answer,
        tokensUsed: geminiResponse.tokensUsed,
        modelUsed: geminiResponse.modelUsed,
        provider: 'gemini',
        fallbackUsed: false,
      }
    }

    case 'groq': {
      const client = getGroqClient()
      const response = await client.chat.completions.create({
        model: aiConfig.groq.model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: aiConfig.groq.model,
        provider: 'groq',
        fallbackUsed: false,
      }
    }

    case 'deepseek': {
      const client = getDeepSeekClient()
      const response = await client.chat.completions.create({
        model: aiConfig.deepseek.model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: `deepseek/${aiConfig.deepseek.model}`,
        provider: 'deepseek',
        fallbackUsed: false,
      }
    }

    case 'openai': {
      const client = getOpenAIClient()
      const model = aiConfig.openai?.chatModel || 'gpt-4o-mini'
      const response = await client.chat.completions.create({
        model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: `openai/${model}`,
        provider: 'openai',
        fallbackUsed: false,
      }
    }

    case 'anthropic': {
      const client = getAnthropicClient()
      const anthropicMessages = userMessages.map((m) => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      }))

      const response = await client.messages.create({
        model: aiConfig.anthropic.model,
        max_tokens: maxTokens,
        system: systemPrompt,
        messages: anthropicMessages,
        temperature,
      })

      return {
        answer: response.content[0].type === 'text' ? response.content[0].text : '',
        tokensUsed: {
          input: response.usage.input_tokens,
          output: response.usage.output_tokens,
          total: response.usage.input_tokens + response.usage.output_tokens,
        },
        modelUsed: aiConfig.anthropic.model,
        provider: 'anthropic',
        fallbackUsed: false,
      }
    }

    case 'ollama': {
      const ollamaResponse = await callOllamaAPI(
        [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
        maxTokens
      )

      return {
        answer: ollamaResponse.content,
        tokensUsed: {
          input: 0,
          output: ollamaResponse.tokens || 0,
          total: ollamaResponse.tokens || 0,
        },
        modelUsed: `ollama/${aiConfig.ollama.chatModelDefault}`,
        provider: 'ollama',
        fallbackUsed: false,
      }
    }

    default:
      throw new Error(`Provider non support√©: ${provider}`)
  }
}

// =============================================================================
// ALERTES EN CAS D'√âCHEC
// =============================================================================

/**
 * Envoie une alerte quand un provider √©choue (mode no-fallback)
 * Utilise le syst√®me d'alertes existant (checkAndSendAlerts via cron)
 * + log structur√© pour d√©tection par monitoring
 */
async function sendProviderFailureAlert(
  provider: LLMProvider,
  operationName: string | undefined,
  error: Error
): Promise<void> {
  // Log structur√© d√©tectable par le monitoring cron
  logger.error('LLM_PROVIDER_FAILURE', JSON.stringify({
    provider,
    operation: operationName || 'default',
    error: error.message,
    timestamp: new Date().toISOString(),
    severity: 'critical',
  }))
}

// =============================================================================
// FONCTION PRINCIPALE - MODE NO-FALLBACK
// =============================================================================

/**
 * Appelle le LLM configur√© pour l'op√©ration donn√©e.
 *
 * Mode No-Fallback (d√©faut):
 * - 1 appel = 1 provider fixe (d√©fini dans operations-config.ts)
 * - Si √©chec ‚Üí throw + alerte email
 *
 * Mode Legacy (LLM_FALLBACK_ENABLED=true):
 * - Cascade de providers (kill switch d'urgence)
 *
 * @param messages - Messages de la conversation
 * @param options - Options LLM (operationName recommand√©)
 * @param usePremiumModel - Ignor√© en mode no-fallback
 */
export async function callLLMWithFallback(
  messages: LLMMessage[],
  options: LLMOptions = {},
  usePremiumModel: boolean = false
): Promise<LLMResponse> {
  // Extraire le system message si pr√©sent dans les messages
  const systemMessage = messages.find(m => m.role === 'system')
  if (systemMessage && !options.systemPrompt) {
    options = { ...options, systemPrompt: systemMessage.content }
  }

  // R√©soudre le provider depuis la config op√©ration
  let operationConfig
  if (options.operationName) {
    operationConfig = getOperationConfig(options.operationName)
    options.temperature = options.temperature ?? operationConfig.llmConfig?.temperature
    options.maxTokens = options.maxTokens || operationConfig.llmConfig?.maxTokens
  }

  // =========================================================================
  // MODE NO-FALLBACK (d√©faut) : 1 provider fixe par op√©ration
  // =========================================================================
  if (!LLM_FALLBACK_ENABLED) {
    // D√©terminer le provider unique
    let provider: LLMProvider

    if (operationConfig) {
      provider = operationConfig.model.provider
    } else {
      // Pas d'operationName ‚Üí utiliser le provider par d√©faut selon l'env
      const isDev = process.env.NODE_ENV === 'development'
      if (isDev && aiConfig.ollama.enabled) {
        provider = 'ollama'
      } else if (aiConfig.groq.apiKey) {
        provider = 'groq'
      } else if (aiConfig.gemini.apiKey) {
        provider = 'gemini'
      } else if (aiConfig.openai?.apiKey) {
        provider = 'openai'
      } else if (aiConfig.ollama.enabled) {
        provider = 'ollama'
      } else {
        throw new Error(
          'Aucun provider LLM configur√©. D√©finissez GROQ_API_KEY, GOOGLE_API_KEY, OPENAI_API_KEY ou OLLAMA_ENABLED=true'
        )
      }
    }

    // V√©rifier le circuit breaker avant d'appeler le provider
    if (!isProviderCircuitClosed(provider)) {
      // Circuit ouvert ‚Üí chercher un fallback disponible imm√©diatement
      const available = getAvailableProviders().filter(p => p !== provider && isProviderCircuitClosed(p))
      if (available.length > 0) {
        const fallbackProvider = available[0]
        logger.warn(
          `[LLM] ‚ö° Circuit ouvert pour ${provider}, redirection directe ‚Üí ${fallbackProvider} (${options.operationName || 'default'})`
        )
        try {
          const response = await callProvider(fallbackProvider, messages, options)
          recordProviderSuccess(fallbackProvider)
          return { ...response, fallbackUsed: true, originalProvider: provider }
        } catch (fallbackErr) {
          recordProviderFailure(fallbackProvider)
          throw new Error(
            `Circuit ouvert pour ${provider} et fallback ${fallbackProvider} √©chou√©: ${fallbackErr instanceof Error ? fallbackErr.message : String(fallbackErr)}`
          )
        }
      }
      throw new Error(
        `Circuit ouvert pour ${provider} (${options.operationName || 'default'}) et aucun provider alternatif disponible`
      )
    }

    logger.info(
      `[LLM] ${options.operationName || 'default'} ‚Üí ${provider} (no-fallback)`
    )

    try {
      const response = await callProvider(provider, messages, options)
      recordProviderSuccess(provider)
      return response
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error))

      // Sur erreur r√©cup√©rable (429 rate-limit, 5xx serveur, timeout) ‚Üí enregistrer + cascade fallback
      const isRecoverable = isRateLimitError(error) || isServerOrTimeoutError(error)
      if (isRecoverable) {
        recordProviderFailure(provider)
        const reason = isRateLimitError(error) ? 'rate-limit√©' : 'erreur serveur/timeout'
        logger.warn(
          `[LLM] ‚ö†Ô∏è ${provider} ${reason} pour ${options.operationName || 'default'}, cascade fallback...`
        )
        // Trouver les providers alternatifs disponibles avec circuit ferm√©
        const available = getAvailableProviders().filter(p => p !== provider && isProviderCircuitClosed(p))
        for (const fallbackProvider of available) {
          try {
            const response = await callProvider(fallbackProvider, messages, options)
            recordProviderSuccess(fallbackProvider)
            logger.info(`[LLM] ‚úì Fallback r√©ussi: ${provider} ‚Üí ${fallbackProvider} (${reason})`)
            return {
              ...response,
              fallbackUsed: true,
              originalProvider: provider,
            }
          } catch (fallbackErr) {
            recordProviderFailure(fallbackProvider)
            logger.warn(`[LLM] ‚ö† Fallback ${fallbackProvider} √©chou√©:`, fallbackErr instanceof Error ? fallbackErr.message : String(fallbackErr))
          }
        }
        // Si tous les fallbacks √©chouent aussi
        throw new Error(
          `Provider ${provider} indisponible (${reason}) et aucun fallback pour ${options.operationName || 'default'}: ${err.message}`
        )
      }

      // Enregistrer l'√©chec non-r√©cup√©rable dans le circuit breaker
      recordProviderFailure(provider)

      // Envoyer alerte si op√©ration critique
      if (operationConfig?.alerts?.onFailure === 'email') {
        sendProviderFailureAlert(provider, options.operationName, err).catch(() => {})
      }

      logger.error(
        `[LLM] ‚ùå ${provider} √©chou√© pour ${options.operationName || 'default'}:`,
        err.message
      )

      throw new Error(
        `Provider ${provider} indisponible pour ${options.operationName || 'default'}: ${err.message}`
      )
    }
  }

  // =========================================================================
  // MODE CASCADE LEGACY (kill switch: LLM_FALLBACK_ENABLED=true)
  // =========================================================================
  logger.warn('[LLM] ‚ö†Ô∏è Mode cascade legacy activ√© (LLM_FALLBACK_ENABLED=true)')

  // D√©terminer l'ordre des providers
  let providers: LLMProvider[]
  if (operationConfig) {
    providers = [operationConfig.model.provider, ...FALLBACK_ORDER.filter(p => p !== operationConfig!.model.provider)]
  } else {
    providers = [...FALLBACK_ORDER]
  }

  // Filtrer par disponibilit√©
  const available = getAvailableProviders()
  const activeProviders = providers.filter(p => available.includes(p))

  if (activeProviders.length === 0) {
    throw new Error('Aucun provider LLM disponible')
  }

  const errors: { provider: LLMProvider; error: string }[] = []

  for (let i = 0; i < activeProviders.length; i++) {
    const provider = activeProviders[i]

    try {
      const response = await callProvider(provider, messages, options)

      if (i > 0) {
        logger.info(`[LLM] ‚úì Fallback r√©ussi: ${activeProviders[0]} ‚Üí ${provider}`)
        return {
          ...response,
          fallbackUsed: true,
          originalProvider: activeProviders[0],
        }
      }

      return response
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error)
      errors.push({ provider, error: errorMessage })
      logger.warn(`[LLM] ‚ö† ${provider} erreur: ${errorMessage}, tentative suivante...`)
    }
  }

  const errorSummary = errors.map((e) => `${e.provider}: ${e.error}`).join('; ')
  throw new Error(`Tous les providers LLM sont indisponibles. Erreurs: ${errorSummary}`)
}

/**
 * Appelle un provider sp√©cifique sans fallback
 */
export async function callSpecificProvider(
  provider: LLMProvider,
  messages: LLMMessage[],
  options: LLMOptions = {}
): Promise<LLMResponse> {
  const available = getAvailableProviders()
  if (!available.includes(provider)) {
    throw new Error(`Provider ${provider} non configur√©. Cl√© API manquante.`)
  }

  return callProvider(provider, messages, options)
}

/**
 * Alias pour callLLMWithFallback (r√©trocompatibilit√©)
 */
export const callLLM = callLLMWithFallback

// =============================================================================
// STREAMING
// =============================================================================

/**
 * R√©sultat d'usage tokens pour le streaming (captur√© depuis le dernier chunk)
 */
export interface StreamTokenUsage {
  input: number
  output: number
  total: number
}

/**
 * Stream Ollama via fetch NDJSON.
 * Ollama retourne des lignes JSON newline-delimited avec message.content + flag done.
 * Le timeout utilise le timeout de l'op√©ration (ex: 60s pour assistant-ia).
 */
async function* callOllamaStream(
  messages: Array<{ role: string; content: string }>,
  options: {
    maxTokens?: number
    temperature?: number
    systemInstruction?: string
    timeout?: number
  },
  usageOut?: StreamTokenUsage
): AsyncGenerator<string> {
  const model = aiConfig.ollama.chatModelDefault
  const timeout = options.timeout ?? 60000 // 60s par d√©faut pour streaming (pas le chatTimeoutDefault=15s)

  const allMessages: Array<{ role: string; content: string }> = []
  if (options.systemInstruction) {
    allMessages.push({ role: 'system', content: options.systemInstruction })
  }
  for (const m of messages) {
    allMessages.push(m)
  }

  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), timeout)

  try {
    const response = await fetch(`${aiConfig.ollama.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages: allMessages,
        stream: true,
        think: false,
        options: {
          temperature: options.temperature ?? 0.1,
          num_predict: options.maxTokens ?? 2048,
        },
      }),
      signal: controller.signal,
    })

    if (!response.ok) {
      throw new Error(`Erreur Ollama streaming: ${response.status}`)
    }

    const reader = response.body?.getReader()
    if (!reader) throw new Error('Ollama: pas de body dans la r√©ponse streaming')

    const decoder = new TextDecoder()
    let buffer = ''
    // Filtre <think>...</think> en streaming : accumuler les tokens pendant le bloc thinking
    let inThinkBlock = false
    let thinkBuffer = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() ?? ''

      for (const line of lines) {
        if (!line.trim()) continue
        try {
          const data = JSON.parse(line)
          let text: string = data.message?.content || ''

          if (text) {
            // Gestion du bloc <think> potentiellement fragment√© sur plusieurs tokens
            thinkBuffer += text
            if (inThinkBlock) {
              const endIdx = thinkBuffer.indexOf('</think>')
              if (endIdx !== -1) {
                inThinkBlock = false
                text = thinkBuffer.slice(endIdx + 8) // apr√®s </think>
                thinkBuffer = ''
              } else {
                text = '' // encore dans le bloc thinking
              }
            } else {
              const startIdx = thinkBuffer.indexOf('<think>')
              if (startIdx !== -1) {
                const before = thinkBuffer.slice(0, startIdx)
                const endIdx = thinkBuffer.indexOf('</think>', startIdx)
                if (endIdx !== -1) {
                  // Bloc complet dans ce chunk
                  text = before + thinkBuffer.slice(endIdx + 8)
                  thinkBuffer = ''
                } else {
                  // D√©but de bloc, pas encore la fin
                  inThinkBlock = true
                  text = before
                  thinkBuffer = thinkBuffer.slice(startIdx)
                }
              } else {
                thinkBuffer = ''
              }
            }
            if (text) yield text
          }

          if (data.done && usageOut) {
            usageOut.input = data.prompt_eval_count || 0
            usageOut.output = data.eval_count || 0
            usageOut.total = (data.prompt_eval_count || 0) + (data.eval_count || 0)
          }
        } catch {
          // Ignorer les lignes NDJSON mal form√©es
        }
      }
    }
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`Ollama streaming timeout apr√®s ${timeout / 1000}s (mod√®le: ${model})`)
    }
    throw error
  } finally {
    clearTimeout(timeoutId)
  }
}

/**
 * Stream DeepSeek via OpenAI SDK compatible (m√™me API que Groq).
 * Le param√®tre `usageOut` permet de capturer les stats de tokens depuis le dernier chunk.
 */
async function* callDeepSeekStream(
  messages: Array<{ role: string; content: string }>,
  options: { maxTokens?: number; temperature?: number; systemInstruction?: string },
  usageOut?: StreamTokenUsage
): AsyncGenerator<string> {
  const client = getDeepSeekClient()
  const model = aiConfig.deepseek.model

  const allMessages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = []
  if (options.systemInstruction) {
    allMessages.push({ role: 'system', content: options.systemInstruction })
  }
  for (const m of messages) {
    allMessages.push({ role: m.role as 'system' | 'user' | 'assistant', content: m.content })
  }

  const stream = await client.chat.completions.create({
    model,
    messages: allMessages,
    stream: true,
    stream_options: { include_usage: true },
    max_tokens: options.maxTokens ?? 8000,
    temperature: options.temperature ?? 0.1,
  })

  for await (const chunk of stream) {
    const text = chunk.choices[0]?.delta?.content || ''
    if (text) yield text
    if (chunk.usage && usageOut) {
      usageOut.input = chunk.usage.prompt_tokens || 0
      usageOut.output = chunk.usage.completion_tokens || 0
      usageOut.total = chunk.usage.total_tokens || 0
    }
  }
}

/**
 * Stream Groq via OpenAI SDK compatible.
 * Le param√®tre `usageOut` permet de capturer les stats de tokens depuis le dernier chunk.
 */
export async function* callGroqStream(
  messages: Array<{ role: string; content: string }>,
  options: { maxTokens?: number; temperature?: number; model?: string; systemInstruction?: string },
  usageOut?: StreamTokenUsage
): AsyncGenerator<string> {
  const client = getGroqClient()
  const model = options.model ?? aiConfig.groq.model

  const allMessages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = []
  if (options.systemInstruction) {
    allMessages.push({ role: 'system', content: options.systemInstruction })
  }
  for (const m of messages) {
    allMessages.push({ role: m.role as 'system' | 'user' | 'assistant', content: m.content })
  }

  let stream: Awaited<ReturnType<typeof client.chat.completions.create>>
  try {
    stream = await client.chat.completions.create({
      model,
      messages: allMessages,
      stream: true,
      stream_options: { include_usage: true },
      max_tokens: options.maxTokens ?? 8000,
      temperature: options.temperature ?? 0.1,
    })
  } catch (err) {
    // Protection 429 : rate limit Groq ‚Üí throw message explicite + enregistre l'√©chec
    if (isRateLimitError(err)) {
      recordProviderFailure('groq')
      logger.warn(`[Groq Stream] 429 rate limit atteint pour mod√®le ${model}`)
      throw new Error(`Assistant temporairement surcharg√© (rate limit Groq). R√©essayez dans quelques secondes.`)
    }
    throw err
  }

  try {
    for await (const chunk of stream) {
      const text = chunk.choices[0]?.delta?.content || ''
      if (text) yield text
      // Groq envoie usage dans le dernier chunk quand stream_options.include_usage=true
      if (chunk.usage && usageOut) {
        usageOut.input = chunk.usage.prompt_tokens || 0
        usageOut.output = chunk.usage.completion_tokens || 0
        usageOut.total = chunk.usage.total_tokens || 0
      }
    }
  } catch (err) {
    if (isRateLimitError(err)) {
      recordProviderFailure('groq')
      logger.warn(`[Groq Stream] 429 en cours de streaming pour mod√®le ${model}`)
      throw new Error(`Assistant temporairement surcharg√© (rate limit Groq). R√©essayez dans quelques secondes.`)
    }
    throw err
  }
}

/**
 * Dispatcher streaming g√©n√©rique selon le provider configur√© pour l'op√©ration.
 * Supporte Ollama, DeepSeek, Groq et Gemini ‚Äî s√©lection automatique via operations-config.
 * Le param√®tre `usageOut` capture les stats de tokens.
 */
export async function* callLLMStream(
  messages: Array<{ role: string; content: string }>,
  options: {
    maxTokens?: number
    temperature?: number
    operationName?: OperationName
    systemInstruction?: string
  },
  usageOut?: StreamTokenUsage
): AsyncGenerator<string> {
  const provider = options.operationName
    ? getOperationProvider(options.operationName)
    : 'ollama'

  if (provider === 'ollama') {
    const opConfig = options.operationName ? getOperationConfig(options.operationName) : null
    const timeout = opConfig?.timeouts?.chat ?? 60000
    yield* callOllamaStream(messages, { ...options, timeout }, usageOut)
  } else if (provider === 'deepseek') {
    yield* callDeepSeekStream(messages, options, usageOut)
  } else if (provider === 'groq') {
    const model = options.operationName ? getOperationModel(options.operationName) : undefined
    yield* callGroqStream(messages, { ...options, model }, usageOut)
  } else {
    // Fallback Gemini pour tout autre provider (gemini, openai, anthropic)
    yield* callGeminiStream(messages, {
      temperature: options.temperature,
      maxTokens: options.maxTokens,
      systemInstruction: options.systemInstruction,
    })
  }
}

/**
 * Service de Fallback LLM
 *
 * G√®re automatiquement les erreurs 429 (rate limit) en basculant
 * vers le provider LLM suivant selon la hi√©rarchie:
 *
 * Groq (rapide, √©conomique)
 *   ‚Üì [429 ou erreur]
 * DeepSeek (√©conomique ~0.14$/M tokens)
 *   ‚Üì [429 ou erreur]
 * Anthropic Claude (puissant mais co√ªteux)
 *   ‚Üì [indisponible]
 * OpenAI (co√ªteux, dernier recours)
 */

import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'
import { aiConfig, SYSTEM_PROMPTS } from './config'
import { callGemini, GeminiResponse } from './gemini-client'
import { getOperationConfig, type OperationName } from './operations-config'

// =============================================================================
// TYPES
// =============================================================================

export type LLMProvider = 'gemini' | 'groq' | 'deepseek' | 'anthropic' | 'ollama' | 'openai'

/**
 * Contextes d'utilisation IA pour strat√©gies optimis√©es
 */
export type AIContext =
  | 'rag-chat'           // Chat RAG (volume √©lev√©, performance critique)
  | 'embeddings'         // G√©n√©ration embeddings (volume tr√®s √©lev√©)
  | 'quality-analysis'   // Analyse qualit√© KB (pr√©cision critique)
  | 'structuring'        // Structuration dossiers (qualit√© JSON critique)
  | 'translation'        // Traduction AR‚ÜîFR (langues critiques)
  | 'web-scraping'       // Web scraping (√©conomie prioritaire)
  | 'default'            // Fallback g√©n√©rique

export interface LLMMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

export interface LLMOptions {
  temperature?: number
  maxTokens?: number
  systemPrompt?: string
  /** Contexte d'utilisation pour strat√©gie optimis√©e */
  context?: AIContext
  /** Type d'op√©ration pour configuration sp√©cifique (override context si fourni) */
  operationName?: OperationName
}

export interface LLMResponse {
  answer: string
  tokensUsed: {
    input: number
    output: number
    total: number
  }
  modelUsed: string
  provider: LLMProvider
  fallbackUsed: boolean
  originalProvider?: LLMProvider
}

// =============================================================================
// CONFIGURATION
// =============================================================================

/**
 * Ordre de fallback des providers (F√©vrier 2026 - optimis√© co√ªt/performance)
 * Gemini tier gratuit illimit√© ‚Üí DeepSeek √©conomique ‚Üí Groq rapide ‚Üí Anthropic puissant ‚Üí Ollama local
 */
const FALLBACK_ORDER: LLMProvider[] = ['gemini', 'deepseek', 'groq', 'anthropic', 'ollama']

/**
 * Strat√©gies de providers par contexte d'utilisation
 * Optimise co√ªt vs performance vs qualit√© selon le cas d'usage
 *
 * IMPORTANT : En d√©veloppement, toutes les strat√©gies utilisent Ollama uniquement
 */
function getProviderStrategyByContext(): Record<AIContext, LLMProvider[]> {
  const isDevelopment = process.env.NODE_ENV === 'development'

  // En dev : Ollama uniquement pour tous les contextes (0‚Ç¨)
  if (isDevelopment) {
    return {
      'rag-chat': ['ollama'],
      'embeddings': ['ollama'],
      'quality-analysis': ['ollama'],
      'structuring': ['ollama'],
      'translation': ['ollama'],
      'web-scraping': ['ollama'],
      'default': ['ollama'],
    }
  }

  // En prod : Strat√©gies optimis√©es par contexte
  return {
    // Chat/RAG - Volume √©lev√© (2-3M tokens/jour), performance critique
    // Priorit√© : Vitesse + Contexte 1M tokens + Co√ªt
    'rag-chat': ['gemini', 'gemini', 'deepseek', 'ollama'],

    // Embeddings - Volume tr√®s √©lev√© (5-10M tokens/jour), co√ªt critique
    // Ollama exclusif pour √©conomie maximale ($400-750/mois √©conomis√©s)
    'embeddings': ['ollama'],

    // Analyse qualit√© KB - Pr√©cision critique (5-10K tokens/jour)
    // Priorit√© : Qualit√© > Co√ªt
    'quality-analysis': ['deepseek', 'gemini', 'ollama'],

    // Structuration dossiers - Qualit√© JSON critique (10-50 ops/mois)
    // Priorit√© : Extraction structur√©e + Raisonnement
    'structuring': ['deepseek', 'gemini', 'ollama'],

    // Traduction bilingue - Langues critiques (<5K tokens/jour)
    // Priorit√© : Multilingue AR/FR + Co√ªt
    'translation': ['gemini', 'groq'],

    // Web scraping - √âconomie prioritaire (5-20K tokens/jour, rare)
    // Priorit√© : Contexte 1M tokens + Gratuit
    'web-scraping': ['gemini', 'ollama'],

    // Fallback g√©n√©rique (ordre standard)
    'default': FALLBACK_ORDER,
  }
}

/** Nombre maximum de retries par provider avant de passer au suivant */
const MAX_RETRIES_PER_PROVIDER = 2

/** D√©lai initial avant retry (en ms), double √† chaque essai */
const INITIAL_RETRY_DELAY_MS = 1000

/** Activer/d√©sactiver le fallback via env */
const LLM_FALLBACK_ENABLED = process.env.LLM_FALLBACK_ENABLED !== 'false'

// =============================================================================
// CLIENTS LLM (singletons)
// =============================================================================

let anthropicClient: Anthropic | null = null
let groqClient: OpenAI | null = null
let deepseekClient: OpenAI | null = null
let openaiClient: OpenAI | null = null

function getAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    anthropicClient = new Anthropic({ apiKey: aiConfig.anthropic.apiKey })
  }
  return anthropicClient
}

function getGroqClient(): OpenAI {
  if (!groqClient) {
    groqClient = new OpenAI({
      apiKey: aiConfig.groq.apiKey,
      baseURL: aiConfig.groq.baseUrl,
    })
  }
  return groqClient
}

function getDeepSeekClient(): OpenAI {
  if (!deepseekClient) {
    // Lire directement process.env pour √©viter probl√®me init module
    const apiKey = process.env.DEEPSEEK_API_KEY || aiConfig.deepseek.apiKey
    if (!apiKey) {
      throw new Error('DEEPSEEK_API_KEY non configur√©')
    }
    deepseekClient = new OpenAI({
      apiKey,
      baseURL: aiConfig.deepseek.baseUrl,
    })
  }
  return deepseekClient
}

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    const apiKey = process.env.OPENAI_API_KEY || aiConfig.openai?.apiKey
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY non configur√©')
    }
    openaiClient = new OpenAI({
      apiKey,
    })
  }
  return openaiClient
}

/**
 * Appelle Ollama directement via fetch (pas de SDK n√©cessaire)
 * Note: avec Option C, usePremiumModel n'est plus utilis√© (toujours false)
 */
async function callOllamaAPI(
  messages: Array<{ role: string; content: string }>,
  temperature: number,
  maxTokens: number,
  usePremiumModel: boolean = false
): Promise<{ content: string; tokens?: number }> {
  const model = aiConfig.ollama.chatModelDefault
  const timeout = aiConfig.ollama.chatTimeoutDefault

  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), timeout)

  try {
    const response = await fetch(`${aiConfig.ollama.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages,
        stream: false,
        think: false, // D√©sactive mode thinking de qwen3
        options: {
          temperature,
          num_predict: maxTokens,
        },
      }),
      signal: controller.signal,
    })

    if (!response.ok) {
      throw new Error(`Erreur Ollama: ${response.status}`)
    }

    const data = await response.json()
    return {
      content: data.message?.content || '',
      tokens: data.eval_count || 0,
    }
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`Timeout apr√®s ${timeout / 1000}s (mod√®le: ${model})`)
    }
    throw error
  } finally {
    clearTimeout(timeoutId)
  }
}

// =============================================================================
// HELPERS
// =============================================================================

/**
 * V√©rifie si l'erreur est une erreur de rate limit (429)
 */
export function isRateLimitError(error: unknown): boolean {
  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    return (
      message.includes('429') ||
      message.includes('rate limit') ||
      message.includes('rate_limit') ||
      message.includes('too many requests') ||
      message.includes('quota exceeded')
    )
  }

  // V√©rifier aussi les objets d'erreur avec status
  if (typeof error === 'object' && error !== null) {
    const err = error as { status?: number; statusCode?: number }
    return err.status === 429 || err.statusCode === 429
  }

  return false
}

/**
 * V√©rifie si l'erreur est r√©cup√©rable (retry possible)
 */
function isRetryableError(error: unknown): boolean {
  if (isRateLimitError(error)) return true

  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    // Erreurs r√©seau/timeout qui m√©ritent un retry
    return (
      message.includes('timeout') ||
      message.includes('econnreset') ||
      message.includes('network') ||
      message.includes('503') ||
      message.includes('502')
    )
  }

  return false
}

/**
 * Attend un d√©lai avec backoff exponentiel
 */
async function delay(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms))
}

/**
 * Retourne la liste des providers disponibles (avec cl√© API configur√©e)
 * Lit directement process.env pour √©viter probl√®me d'initialisation module
 *
 * En d√©veloppement : Ollama uniquement (0‚Ç¨, pas de consommation cloud)
 * En production : Tous les providers disponibles (Gemini, DeepSeek, etc.)
 */
export function getAvailableProviders(): LLMProvider[] {
  const isDevelopment = process.env.NODE_ENV === 'development'

  // En dev : Ollama uniquement pour √©viter consommation tokens payants
  if (isDevelopment) {
    console.log('[LLM-Fallback] üè† Mode d√©veloppement ‚Üí Ollama uniquement (0‚Ç¨)')
    return ['ollama']
  }

  // En prod : Tous les providers disponibles
  return FALLBACK_ORDER.filter((provider) => {
    switch (provider) {
      case 'gemini':
        return !!(process.env.GOOGLE_API_KEY || aiConfig.gemini.apiKey)
      case 'groq':
        return !!(process.env.GROQ_API_KEY || aiConfig.groq.apiKey)
      case 'deepseek':
        return !!(process.env.DEEPSEEK_API_KEY || aiConfig.deepseek.apiKey)
      case 'anthropic':
        return !!(process.env.ANTHROPIC_API_KEY || aiConfig.anthropic.apiKey)
      case 'openai':
        return !!(process.env.OPENAI_API_KEY || aiConfig.openai?.apiKey)
      case 'ollama':
        return process.env.OLLAMA_ENABLED === 'true' || aiConfig.ollama.enabled
      default:
        return false
    }
  })
}

// =============================================================================
// APPELS LLM PAR PROVIDER
// =============================================================================

/**
 * Appelle un provider sp√©cifique
 */
async function callProvider(
  provider: LLMProvider,
  messages: LLMMessage[],
  options: LLMOptions,
  usePremiumModel: boolean = false
): Promise<LLMResponse> {
  const systemPrompt = options.systemPrompt || SYSTEM_PROMPTS.qadhya
  const temperature = options.temperature ?? 0.3
  const maxTokens = options.maxTokens || aiConfig.anthropic.maxTokens

  // S√©parer le system message des autres
  const userMessages = messages.filter((m) => m.role !== 'system')

  switch (provider) {
    case 'gemini': {
      // Appel Gemini via gemini-client.ts
      const geminiResponse: GeminiResponse = await callGemini(
        [{ role: 'system', content: systemPrompt }, ...userMessages],
        { temperature, maxTokens, systemInstruction: systemPrompt }
      )

      return {
        answer: geminiResponse.answer,
        tokensUsed: geminiResponse.tokensUsed,
        modelUsed: geminiResponse.modelUsed,
        provider: 'gemini',
        fallbackUsed: false,
      }
    }

    case 'groq': {
      const client = getGroqClient()
      const response = await client.chat.completions.create({
        model: aiConfig.groq.model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: aiConfig.groq.model,
        provider: 'groq',
        fallbackUsed: false,
      }
    }

    case 'deepseek': {
      const client = getDeepSeekClient()
      const response = await client.chat.completions.create({
        model: aiConfig.deepseek.model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: `deepseek/${aiConfig.deepseek.model}`,
        provider: 'deepseek',
        fallbackUsed: false,
      }
    }

    case 'openai': {
      const client = getOpenAIClient()
      const model = aiConfig.openai?.chatModel || 'gpt-4o-mini'
      const response = await client.chat.completions.create({
        model,
        max_tokens: maxTokens,
        messages: [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
      })

      return {
        answer: response.choices[0]?.message?.content || '',
        tokensUsed: {
          input: response.usage?.prompt_tokens || 0,
          output: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0,
        },
        modelUsed: `openai/${model}`,
        provider: 'openai',
        fallbackUsed: false,
      }
    }

    case 'anthropic': {
      const client = getAnthropicClient()
      // Anthropic utilise un format de messages diff√©rent
      const anthropicMessages = userMessages.map((m) => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      }))

      const response = await client.messages.create({
        model: aiConfig.anthropic.model,
        max_tokens: maxTokens,
        system: systemPrompt,
        messages: anthropicMessages,
        temperature,
      })

      return {
        answer: response.content[0].type === 'text' ? response.content[0].text : '',
        tokensUsed: {
          input: response.usage.input_tokens,
          output: response.usage.output_tokens,
          total: response.usage.input_tokens + response.usage.output_tokens,
        },
        modelUsed: aiConfig.anthropic.model,
        provider: 'anthropic',
        fallbackUsed: false,
      }
    }

    case 'ollama': {
      const ollamaResponse = await callOllamaAPI(
        [{ role: 'system', content: systemPrompt }, ...userMessages],
        temperature,
        maxTokens,
        false // toujours mode default avec Option C
      )

      return {
        answer: ollamaResponse.content,
        tokensUsed: {
          input: 0, // Ollama ne retourne pas toujours les tokens d'entr√©e
          output: ollamaResponse.tokens || 0,
          total: ollamaResponse.tokens || 0,
        },
        modelUsed: `ollama/${aiConfig.ollama.chatModelDefault}`,
        provider: 'ollama',
        fallbackUsed: false,
      }
    }

    default:
      throw new Error(`Provider non support√©: ${provider}`)
  }
}

/**
 * Appelle un provider avec retry et backoff exponentiel
 */
async function callProviderWithRetry(
  provider: LLMProvider,
  messages: LLMMessage[],
  options: LLMOptions,
  usePremiumModel: boolean = false
): Promise<LLMResponse> {
  let lastError: Error | null = null
  let delayMs = INITIAL_RETRY_DELAY_MS

  for (let attempt = 0; attempt < MAX_RETRIES_PER_PROVIDER; attempt++) {
    try {
      return await callProvider(provider, messages, options, usePremiumModel)
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error))

      // Si rate limit, on ne retry pas, on passe au provider suivant
      if (isRateLimitError(error)) {
        console.warn(
          `[LLM-Fallback] ${provider} rate limited (429), skipping retries`
        )
        throw error
      }

      // Si erreur non-retryable, on throw directement
      if (!isRetryableError(error)) {
        throw error
      }

      // Sinon on retry avec backoff
      if (attempt < MAX_RETRIES_PER_PROVIDER - 1) {
        console.warn(
          `[LLM-Fallback] ${provider} erreur temporaire, retry dans ${delayMs}ms (tentative ${attempt + 1}/${MAX_RETRIES_PER_PROVIDER})`
        )
        await delay(delayMs)
        delayMs *= 2 // Backoff exponentiel
      }
    }
  }

  throw lastError || new Error(`${provider} indisponible apr√®s ${MAX_RETRIES_PER_PROVIDER} tentatives`)
}

// =============================================================================
// FONCTION PRINCIPALE AVEC FALLBACK
// =============================================================================

/**
 * Appelle un LLM avec fallback automatique sur erreur 429
 *
 * Mode Hybride Intelligent :
 * - Mode Rapide (usePremiumModel=false) : Ollama qwen3:8b local ‚Üí fallback cloud
 * - Mode Premium (usePremiumModel=true) : Cloud providers directement (meilleure qualit√©)
 *
 * @param messages - Messages de la conversation
 * @param options - Options LLM (temperature, maxTokens, systemPrompt)
 * @param usePremiumModel - Si true, utilise directement les cloud providers (Groq/DeepSeek/Anthropic) pour qualit√© max
 * @returns R√©ponse LLM avec informations sur le provider utilis√©
 */
export async function callLLMWithFallback(
  messages: LLMMessage[],
  options: LLMOptions = {},
  usePremiumModel: boolean = false
): Promise<LLMResponse> {
  // Si operationName fourni, utiliser sa configuration
  let operationConfig
  if (options.operationName) {
    operationConfig = getOperationConfig(options.operationName)
    // Override options avec config de l'op√©ration (si non d√©fini)
    options.context = options.context || operationConfig.context
    options.temperature = options.temperature ?? operationConfig.llmConfig?.temperature
    options.maxTokens = options.maxTokens || operationConfig.llmConfig?.maxTokens
  }

  // Mode Premium : forcer cloud providers pour qualit√© maximale (skip Ollama)
  if (usePremiumModel) {
    console.log('[LLM-Fallback] Mode Premium activ√© ‚Üí utilisation cloud providers')
  }
  // Mode Rapide : essayer Ollama local d'abord (gratuit, rapide)
  else if (aiConfig.ollama.enabled && !operationConfig?.providers) {
    try {
      console.log(`[LLM-Fallback] Mode Rapide ‚Üí Ollama (${aiConfig.ollama.chatModelDefault})`)

      const ollamaResult = await callOllamaAPI(
        messages.map(m => ({ role: m.role, content: m.content })),
        options.temperature ?? 0.3,
        options.maxTokens || aiConfig.anthropic.maxTokens,
        false // toujours mode default pour Ollama
      )

      return {
        answer: ollamaResult.content,
        tokensUsed: {
          input: 0,
          output: ollamaResult.tokens || 0,
          total: ollamaResult.tokens || 0,
        },
        modelUsed: `ollama/${aiConfig.ollama.chatModelDefault}`,
        provider: 'ollama',
        fallbackUsed: false,
      }
    } catch (error) {
      console.warn(
        '[LLM-Fallback] ‚ö† Ollama √©chou√©, fallback vers cloud providers:',
        error instanceof Error ? error.message : error
      )
    }
  }

  // D√©terminer la strat√©gie selon le contexte (ou config op√©ration)
  const context = options.context || 'default'

  // Si op√©ration a des providers sp√©cifiques, les utiliser
  let strategyProviders: LLMProvider[]
  if (operationConfig?.providers) {
    strategyProviders = [operationConfig.providers.primary, ...operationConfig.providers.fallback]
    console.log(`[LLM-Fallback] Op√©ration: ${options.operationName} ‚Üí Strat√©gie: [${strategyProviders.join(' ‚Üí ')}]`)
  } else {
    strategyProviders = getProviderStrategyByContext()[context]
  }

  // Filtrer par providers disponibles (cl√©s API configur√©es)
  const availableProviders = getAvailableProviders()
  const isDevelopment = process.env.NODE_ENV === 'development'

  const contextProviders = strategyProviders
    .filter(p => availableProviders.includes(p))
    // En prod : Exclure Ollama (d√©j√† tent√© en mode rapide)
    // En dev : Garder Ollama (seul provider disponible)
    .filter(p => isDevelopment || p !== 'ollama')

  if (contextProviders.length === 0) {
    const hint = isDevelopment
      ? 'V√©rifiez que Ollama est d√©marr√© : ollama serve'
      : 'Configurez au moins une cl√© API: GOOGLE_API_KEY, GROQ_API_KEY, DEEPSEEK_API_KEY ou ANTHROPIC_API_KEY'
    throw new Error(
      `Aucun provider disponible pour contexte "${context}". ${hint}`
    )
  }

  console.log(`[LLM-Fallback] Contexte: ${context} ‚Üí Strat√©gie: [${contextProviders.join(' ‚Üí ')}]`)

  // Si fallback d√©sactiv√©, utiliser uniquement le premier provider
  if (!LLM_FALLBACK_ENABLED) {
    console.log(`[LLM-Fallback] Fallback d√©sactiv√©, utilisation de ${contextProviders[0]} uniquement`)
    return callProviderWithRetry(contextProviders[0], messages, options, usePremiumModel)
  }

  const originalProvider = contextProviders[0]
  const errors: { provider: LLMProvider; error: string }[] = []

  for (let i = 0; i < contextProviders.length; i++) {
    const provider = contextProviders[i]

    try {
      const response = await callProviderWithRetry(provider, messages, options, usePremiumModel)

      // Marquer si on a utilis√© un fallback
      if (i > 0 || aiConfig.ollama.enabled) {
        console.log(
          `[LLM-Fallback] ‚úì Fallback r√©ussi: ${aiConfig.ollama.enabled ? 'ollama' : originalProvider} ‚Üí ${provider}`
        )
        return {
          ...response,
          fallbackUsed: true,
          originalProvider: aiConfig.ollama.enabled ? 'ollama' : originalProvider,
        }
      }

      return response
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error)
      errors.push({ provider, error: errorMessage })

      if (isRateLimitError(error)) {
        console.warn(
          `[LLM-Fallback] ‚ö† ${provider} rate limited (429), tentative du provider suivant...`
        )
      } else {
        console.warn(
          `[LLM-Fallback] ‚ö† ${provider} erreur: ${errorMessage}, tentative du provider suivant...`
        )
      }

      // Continuer avec le provider suivant
      continue
    }
  }

  // Tous les providers ont √©chou√©
  const errorSummary = errors
    .map((e) => `${e.provider}: ${e.error}`)
    .join('; ')

  throw new Error(
    `Tous les providers LLM sont indisponibles. Erreurs: ${errorSummary}`
  )
}

/**
 * Appelle un provider sp√©cifique sans fallback
 * Utile pour les tests ou quand on veut forcer un provider
 */
export async function callSpecificProvider(
  provider: LLMProvider,
  messages: LLMMessage[],
  options: LLMOptions = {},
  usePremiumModel: boolean = false
): Promise<LLMResponse> {
  // V√©rifier que le provider est disponible
  const available = getAvailableProviders()
  if (!available.includes(provider)) {
    throw new Error(
      `Provider ${provider} non configur√©. Cl√© API manquante.`
    )
  }

  return callProviderWithRetry(provider, messages, options, usePremiumModel)
}

# üöÄ D√©ploiement Production - Option C Hybride

**Date** : F√©vrier 2026
**VPS** : 84.247.165.187 (4 CPU, 8GB RAM)
**Architecture** : Mode Rapide (Ollama local) + Mode Premium (Cloud)

---

## üéØ Vue d'ensemble

Ce guide vous accompagne pour d√©ployer la migration Ollama Option C en production.

**Dur√©e totale estim√©e** : 20-30 minutes

---

## üöÄ M√©thode 1 : Script Automatique (Recommand√©)

### Depuis votre machine locale

```bash
./scripts/deploy-option-c-prod.sh
```

Le script automatise :
- ‚úÖ Installation Ollama
- ‚úÖ Configuration systemd + UFW
- ‚úÖ T√©l√©chargement mod√®les (qwen3:8b + qwen3-embedding)
- ‚úÖ Mise √† jour code
- ‚úÖ Rebuild Docker
- ‚úÖ Red√©marrage application
- ‚úÖ V√©rifications post-d√©ploiement

**Dur√©e** : ~20 minutes (dont 10-15 min pour t√©l√©charger les mod√®les)

---

## üîß M√©thode 2 : D√©ploiement Manuel

### 1. Connexion VPS

```bash
ssh root@84.247.165.187
cd /opt/moncabinet
```

### 2. Installation Ollama

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# Configuration systemd (√©coute 0.0.0.0)
mkdir -p /etc/systemd/system/ollama.service.d
cat > /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
[Service]
Environment=OLLAMA_HOST=0.0.0.0:11434
EOF

# D√©marrage service
systemctl daemon-reload
systemctl enable ollama
systemctl start ollama
systemctl status ollama
```

### 3. Configuration Firewall

```bash
# Autoriser Docker (172.x.x.x) ‚Üí Ollama (11434)
ufw allow from 172.16.0.0/12 to any port 11434 comment 'Docker to Ollama'
ufw status | grep 11434
```

### 4. T√©l√©chargement Mod√®les

```bash
# Chat rapide (5.2 GB - ~5-10 min)
ollama pull qwen3:8b

# Embeddings (639 MB - ~2-5 min)
ollama pull qwen3-embedding:0.6b

# V√©rification
ollama list
```

### 5. Mise √† jour Code

```bash
git pull origin main
git log --oneline -3
```

### 6. Configuration .env.production

```bash
nano .env.production
```

**Variables critiques** :

```bash
# Ollama (Mode Rapide)
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_CHAT_MODEL=qwen3:8b
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b
OLLAMA_CHAT_TIMEOUT_DEFAULT=120000

# Cloud Providers (Mode Premium) - AU MOINS GROQ
GROQ_API_KEY=gsk_...
DEEPSEEK_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...  # Optionnel

# RETIRER (si pr√©sent)
# OPENAI_API_KEY
```

### 7. Rebuild & Red√©marrage Docker

```bash
# Arr√™t containers
docker-compose -f docker-compose.prod.yml down

# Rebuild image (~5-10 min)
docker-compose -f docker-compose.prod.yml build --no-cache

# D√©marrage
docker-compose -f docker-compose.prod.yml up -d

# V√©rification
docker ps --filter name=moncabinet
```

### 8. V√©rifications Post-D√©ploiement

```bash
# Ollama accessible
curl http://localhost:11434/api/tags | jq .

# Containers running
docker ps

# Logs Next.js (rechercher Ollama/LLM)
docker logs --tail 100 moncabinet-nextjs | grep -i "ollama\|llm-fallback"

# Application accessible
curl -I https://moncabinet.tn
```

---

## üß™ Tests en Production

### Test 1 : Interface Web

```
https://moncabinet.tn/chat-test
```

**Actions** :
1. Toggle d√©sactiv√© (‚ö° Mode Rapide)
2. Poser : "Quels sont les d√©lais de prescription commerciale ?"
3. Attendre ~15-20s
4. **Activer** toggle (üß† Mode Premium)
5. Poser la m√™me question
6. Comparer qualit√©/temps

### Test 2 : Monitoring Logs

```bash
# Terminal 1 : Logs Next.js
docker logs -f moncabinet-nextjs | grep "LLM-Fallback"

# Terminal 2 : Logs Ollama
journalctl -u ollama -f
```

**Logs attendus Mode Rapide** :
```
[LLM-Fallback] Mode Rapide ‚Üí Ollama (qwen3:8b)
[RAG] Sources trouv√©es: 5
```

**Logs attendus Mode Premium** :
```
[LLM-Fallback] Mode Premium activ√© ‚Üí utilisation cloud providers
[LLM-Fallback] ‚úì Fallback r√©ussi: ollama ‚Üí groq
```

### Test 3 : Test Fallback

```bash
# Stopper Ollama temporairement
systemctl stop ollama

# Poser une question en mode rapide
# ‚Üí Devrait fallback vers Groq

# Red√©marrer
systemctl start ollama
```

---

## üêõ Troubleshooting

### Probl√®me : "Ollama n'est pas accessible"

```bash
# V√©rifier service
systemctl status ollama

# V√©rifier logs
journalctl -u ollama -n 50

# V√©rifier override
cat /etc/systemd/system/ollama.service.d/override.conf

# Red√©marrer
systemctl restart ollama
```

### Probl√®me : "Mod√®le qwen3:8b non trouv√©"

```bash
ollama list
ollama pull qwen3:8b
```

### Probl√®me : Docker ne peut pas atteindre Ollama

```bash
# V√©rifier UFW
ufw status | grep 11434

# V√©rifier que Ollama √©coute 0.0.0.0
netstat -tlnp | grep 11434

# Test depuis container
docker exec moncabinet-nextjs curl http://host.docker.internal:11434/api/tags
```

### Probl√®me : Timeout embeddings

```bash
# Dans .env.production
OLLAMA_CHAT_TIMEOUT_DEFAULT=180000  # 3 min au lieu de 2

# Rebuild
docker-compose -f docker-compose.prod.yml up -d --build
```

### Probl√®me : RAM satur√©e

```bash
# V√©rifier usage
free -h
htop

# Ollama prend ~4-6GB avec qwen3:8b charg√©
# Si probl√®me, augmenter swap ou upgrade RAM
```

---

## üìä M√©triques √† Surveiller

### Premi√®res 24 heures

| M√©trique | Objectif | Comment v√©rifier |
|----------|----------|------------------|
| Taux succ√®s Ollama | >95% | Logs LLM-Fallback |
| Temps mode rapide | 15-25s | Interface /chat-test |
| Temps mode premium | 10-30s | Interface /chat-test |
| Erreurs critiques | 0 | `docker logs moncabinet-nextjs \| grep ERROR` |

### Premi√®re semaine

| M√©trique | Objectif | Comment v√©rifier |
|----------|----------|------------------|
| Usage mode premium | <20% | Analytics / Logs |
| Co√ªts API cloud | <5‚Ç¨ | Dashboards Groq/DeepSeek |
| CPU usage | <80% pic | `htop` |
| RAM usage Ollama | Stable ~6GB | `htop` |

---

## üîÑ Rollback (si probl√®me)

### Rollback code

```bash
cd /opt/moncabinet

# Voir commits r√©cents
git log --oneline -5

# Rollback au commit pr√©c√©dent
git revert HEAD
# ou
git reset --hard <commit-id>

# Rebuild
docker-compose -f docker-compose.prod.yml up -d --build
```

### Rollback vers configuration pr√©c√©dente

```bash
# Restaurer .env.production depuis backup
cp .env.production.backup .env.production

# Ou d√©sactiver Ollama temporairement
# .env.production
OLLAMA_ENABLED=false
# Le syst√®me utilisera cloud providers seulement
```

---

## üí∞ √âconomies Attendues

| Avant (OpenAI) | Apr√®s (Option C) | √âconomie |
|----------------|------------------|----------|
| Chat : ~60‚Ç¨/mois | Mode rapide : 0‚Ç¨ | -60‚Ç¨/mois |
| Embeddings : ~40‚Ç¨/mois | Ollama : 0‚Ç¨ | -40‚Ç¨/mois |
| **Total** : ~100‚Ç¨/mois | **Total** : 0-15‚Ç¨/mois | **~1200‚Ç¨/an** üéâ |

---

## üìö Ressources

- **Guide complet** : `docs/MIGRATION_OLLAMA_OPTION_C.md`
- **Tests Phase 2** : `docs/PHASE2_INTEGRATION_COMPLETE.md`
- **Script auto** : `scripts/deploy-option-c-prod.sh`
- **Logs Ollama** : `journalctl -u ollama -f`
- **Logs Next.js** : `docker logs -f moncabinet-nextjs`

---

## ‚úÖ Checklist Finale

- [ ] Ollama install√© et d√©marr√©
- [ ] Systemd override configur√©
- [ ] UFW rule ajout√©e
- [ ] Mod√®les t√©l√©charg√©s (qwen3:8b + qwen3-embedding)
- [ ] .env.production mis √† jour
- [ ] Code git pull
- [ ] Docker rebuild
- [ ] Containers red√©marr√©s
- [ ] Test mode rapide OK
- [ ] Test mode premium OK
- [ ] Logs sans erreur

**D√©ploiement r√©ussi si tous les tests passent !** ‚ú®

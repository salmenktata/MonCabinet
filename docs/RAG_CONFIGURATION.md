# Configuration RAG - Guide Complet

> **Auteur** : √âquipe D√©veloppement Qadhya  
> **Date** : 14 f√©vrier 2026  
> **Version** : 2.0 (avec protection multicouche)

## üìã Table des Mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Variables Critiques](#variables-critiques)
3. [Validation Configuration](#validation-configuration)
4. [Monitoring & Alertes](#monitoring--alertes)
5. [D√©pannage](#d√©pannage)
6. [FAQ](#faq)

---

## Vue d'ensemble

Le syst√®me RAG (Retrieval-Augmented Generation) permet √† l'assistant IA de r√©pondre aux questions en s'appuyant sur la base de connaissances juridique.

### Architecture Simplifi√©e

```
Assistant IA ‚Üí RAG System ‚Üí Knowledge Base (8787 docs)
                  ‚Üì
          Provider Embeddings
          (Ollama OU OpenAI)
```

### Composants Requis

1. **RAG_ENABLED=true** : Active le syst√®me
2. **Provider Embeddings** : AU MOINS UN requis
   - Ollama (gratuit, local) OU
   - OpenAI (payant, cloud)

---

## Variables Critiques

### Configuration Minimale Requise

Fichier : `/opt/moncabinet/.env`

```bash
# CONFIGURATION RAG - NON-N√âGOCIABLE
RAG_ENABLED=true
OLLAMA_ENABLED=true  # OU avoir OPENAI_API_KEY configur√©

OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b
```

### ‚ö†Ô∏è Configuration INVALIDE (bloque d√©ploiement)

```bash
RAG_ENABLED=true
OLLAMA_ENABLED=false
# ET pas d'OPENAI_API_KEY
# ‚Üí Assistant IA NON-FONCTIONNEL ‚ùå
```

### ‚úÖ Configurations VALIDES

**Option 1 : Ollama seul (recommand√©)**
```bash
RAG_ENABLED=true
OLLAMA_ENABLED=true
```

**Option 2 : OpenAI seul**
```bash
RAG_ENABLED=true
OPENAI_API_KEY=sk-proj-...
```

**Option 3 : Les deux (optimal)**
```bash
RAG_ENABLED=true
OLLAMA_ENABLED=true
OPENAI_API_KEY=sk-proj-...  # Fallback
```

---

## Validation Configuration

### 1. Pre-Deploy (Automatique)

```bash
bash scripts/validate-rag-config.sh .env.production
```

**R√©sultat attendu :**
```
‚úÖ Configuration RAG valide
```

### 2. Production Runtime

```bash
curl -s https://qadhya.tn/api/health | jq '.rag'
```

**R√©ponse attendue :**
```json
{
  "enabled": true,
  "semanticSearchEnabled": true,
  "status": "ok",
  "kbDocsIndexed": 8787
}
```

### 3. Test Recherche KB

```bash
bash scripts/test-kb-search-prod.sh
```

---

## Monitoring & Alertes

### Dashboard Temps R√©el

**URL** : https://qadhya.tn/super-admin/monitoring?tab=system-config

- üü¢ Badge vert : Configuration OK
- üî¥ Badge rouge : Probl√®me d√©tect√©

**Auto-refresh** : 30 secondes

### Alertes Email

- **D√©clencheur** : Cron quotidien 8h + monitoring continu
- **Condition** : Configuration RAG invalide d√©tect√©e
- **Anti-spam** : Max 1 email/6h

### Logs Cron

```bash
tail -f /var/log/qadhya/rag-config-check.log
```

---

## D√©pannage

### Probl√®me : Assistant IA r√©pond "ŸÑŸÖ ÿ£ÿ¨ÿØ Ÿàÿ´ÿßÿ¶ŸÇ ÿ∞ÿßÿ™ ÿµŸÑÿ©"

#### Diagnostic

```bash
# 1. V√©rifier health check
curl -s https://qadhya.tn/api/health | jq '.rag.status'
# Si "misconfigured" ‚Üí Probl√®me confirm√©

# 2. V√©rifier variables container
ssh root@84.247.165.187 "docker exec qadhya-nextjs env | grep OLLAMA_ENABLED"
```

#### Solution A : Activer Ollama (gratuit)

```bash
ssh root@84.247.165.187

# 1. Modifier .env
nano /opt/moncabinet/.env
# Changer : OLLAMA_ENABLED=false ‚Üí true

# 2. Red√©marrer
cd /opt/moncabinet
docker-compose up -d --no-deps nextjs

# 3. Attendre 45s
sleep 45

# 4. V√©rifier
curl -s https://qadhya.tn/api/health | jq '.rag.status'
# Attendu: "ok"
```

#### Solution B : Configurer OpenAI

```bash
# 1. Ajouter cl√©
nano /opt/moncabinet/.env.production.local
# Ajouter : OPENAI_API_KEY=sk-proj-...

# 2. Red√©marrer container
docker-compose up -d --no-deps nextjs
```

#### Validation

```bash
# Test recherche
bash scripts/test-kb-search-prod.sh

# Test manuel
# ‚Üí https://qadhya.tn/assistant-ia
# ‚Üí Poser question en arabe
# ‚Üí V√©rifier sources [KB-1], [KB-2]...
```

---

## FAQ

### Q : Ollama vs OpenAI ?

| Crit√®re | Ollama | OpenAI |
|---------|--------|--------|
| Co√ªt | 0‚Ç¨/mois | ~2-5‚Ç¨/mois |
| Vitesse | 500-1000ms | 200-400ms |
| Qualit√© | Tr√®s bon | Excellent |
| **Recommandation** | ‚úÖ D√©faut | Fallback |

### Q : Comment tester en local ?

```bash
# 1. Configurer .env.local
RAG_ENABLED=true
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434

# 2. D√©marrer Ollama
ollama pull qwen3-embedding:0.6b

# 3. Lancer app
npm run dev

# 4. Tester
curl http://localhost:7002/api/health | jq '.rag'
```

### Q : Budget OpenAI √©puis√© ?

**Solution** : Basculer sur Ollama (gratuit)

```bash
OLLAMA_ENABLED=true
# Ollama prendra automatiquement le relais
```

---

## Protection Multicouche

‚úÖ **Layer 1** : Validation pre-deploy (bloque si invalide)  
‚úÖ **Layer 2** : Health check runtime (d√©tection)  
‚úÖ **Layer 3** : Alertes email automatiques  
‚úÖ **Layer 4** : Dashboard monitoring temps r√©el

---

**Ressources** :
- Dashboard : https://qadhya.tn/super-admin/monitoring?tab=system-config
- Code : `lib/ai/config.ts`, `scripts/validate-rag-config.sh`
- Logs : `/var/log/qadhya/rag-config-check.log`

---

*Derni√®re mise √† jour : 14 f√©vrier 2026 - v2.0*

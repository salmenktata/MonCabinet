# Guide : Gestion Dev vs Prod pour LLM

**Date** : 2026-02-09
**Status** : ‚úÖ Impl√©ment√© et test√©

## üéØ Objectif

**√âviter la consommation de tokens payants en d√©veloppement** en utilisant uniquement Ollama local (0‚Ç¨), et r√©server les providers cloud (Gemini, DeepSeek) pour la production.

## üí∞ √âconomies

| Environnement | Providers | Co√ªt |
|---------------|-----------|------|
| **D√©veloppement** | Ollama uniquement | **0‚Ç¨** |
| **Production** | Gemini + DeepSeek + Ollama | ~$2-5/mois |

**Gain** : Pas de consommation accidentelle en dev (√©conomie de ~$50-100/mois) üí∞

## üîß Configuration

### Variable d'Environnement

**`.env.local`** :
```bash
# development = Ollama uniquement (0‚Ç¨)
# production = Cloud providers payants
NODE_ENV=development
```

### Comportement par Environnement

#### Mode D√©veloppement (`NODE_ENV=development`)

```typescript
// Providers disponibles
getAvailableProviders() ‚Üí ['ollama']

// Strat√©gies par contexte
PROVIDER_STRATEGY_BY_CONTEXT = {
  'rag-chat': ['ollama'],
  'embeddings': ['ollama'],
  'quality-analysis': ['ollama'],
  'structuring': ['ollama'],
  'translation': ['ollama'],
  'web-scraping': ['ollama'],
  'default': ['ollama'],
}
```

**R√©sultat** :
- ‚úÖ Tous les appels LLM utilisent Ollama local
- ‚úÖ 0‚Ç¨ consomm√©
- ‚úÖ Pas de risque d'√©puiser les quotas cloud
- ‚ö†Ô∏è Plus lent (17-20s vs 0.5-1.5s cloud)

#### Mode Production (`NODE_ENV=production`)

```typescript
// Providers disponibles
getAvailableProviders() ‚Üí ['gemini', 'deepseek', 'ollama']

// Strat√©gies par contexte (optimis√©es)
PROVIDER_STRATEGY_BY_CONTEXT = {
  'rag-chat': ['gemini', 'gemini', 'deepseek', 'ollama'],
  'embeddings': ['ollama'],
  'quality-analysis': ['deepseek', 'gemini', 'ollama'],
  'structuring': ['deepseek', 'gemini', 'ollama'],
  'translation': ['gemini', 'groq'],
  'web-scraping': ['gemini', 'ollama'],
  'default': FALLBACK_ORDER,
}
```

**R√©sultat** :
- ‚úÖ Performance optimale (0.5-1.5s)
- ‚úÖ Fallback intelligent par contexte
- ‚úÖ Qualit√© maximale (DeepSeek pour analyse, Gemini pour chat)
- üí∞ ~$2-5/mois selon usage

## üì¶ Fichiers Modifi√©s

### 1. `lib/ai/llm-fallback-service.ts`

**Changements** :
- `getAvailableProviders()` : Retourne `['ollama']` si `NODE_ENV=development`
- `getProviderStrategyByContext()` : Fonction dynamique selon environnement
- `callLLMWithFallback()` : Garde Ollama en dev (ne pas filtrer)

### 2. `.env.local`

**Ajout** :
```bash
NODE_ENV=development
```

### 3. `.env.example`

**Ajout** :
```bash
NODE_ENV=development  # Comment√© pour prod
```

## üß™ Tests

### Test Automatique

```bash
npx tsx scripts/test-dev-mode.ts
```

**Sortie attendue** :
```
üß™ Test Mode D√©veloppement (NODE_ENV=development)

üìã Test 1: Providers disponibles
[LLM-Fallback] üè† Mode d√©veloppement ‚Üí Ollama uniquement (0‚Ç¨)
   R√©sultat: ollama
   Attendu: ollama uniquement

üì° Test 2: Appel LLM en mode dev
[LLM-Fallback] Contexte: rag-chat ‚Üí Strat√©gie: [ollama]
   ‚úÖ Provider: ollama
   ‚úÖ R√©ponse: "OK"
   ‚úÖ Dur√©e: 17776ms
   ‚úÖ Mod√®le: ollama/qwen3:8b

üéâ Mode d√©veloppement valid√© : 0‚Ç¨ consomm√© !
```

### Test Manuel

```typescript
// Dans n'importe quel code
import { getAvailableProviders } from '@/lib/ai/llm-fallback-service'

const providers = getAvailableProviders()
console.log(providers)
// Dev: ['ollama']
// Prod: ['gemini', 'deepseek', 'ollama']
```

## üöÄ D√©ploiement Production

### √âtape 1 : Mettre √† jour .env sur le serveur

```bash
# Sur le VPS de production
vim /opt/moncabinet/.env

# Changer
NODE_ENV=production
```

### √âtape 2 : Red√©marrer l'application

```bash
cd /opt/moncabinet
docker-compose restart
```

### √âtape 3 : V√©rifier les logs

```bash
docker logs -f moncabinet-nextjs | grep "LLM-Fallback"
```

**Logs attendus** :
```
[LLM-Fallback] Contexte: rag-chat ‚Üí Strat√©gie: [gemini ‚Üí deepseek]
[LLM-Fallback] Mode Premium activ√© ‚Üí utilisation cloud providers
```

## ‚ö†Ô∏è Pr√©requis

### D√©veloppement

1. **Ollama d√©marr√©** :
   ```bash
   ollama serve
   ```

2. **Mod√®les install√©s** :
   ```bash
   ollama list
   # qwen2.5:3b ou qwen3:8b
   # qwen3-embedding:0.6b
   ```

3. **NODE_ENV=development** dans `.env.local`

### Production

1. **Cl√©s API configur√©es** dans `.env` :
   - `GOOGLE_API_KEY` (Gemini)
   - `DEEPSEEK_API_KEY` (DeepSeek)

2. **NODE_ENV=production** dans `.env`

3. **Ollama optionnel** (uniquement pour embeddings)

## üêõ Troubleshooting

### Erreur : "Aucun provider disponible"

**En dev** :
```
‚ùå Aucun provider disponible pour contexte "rag-chat". V√©rifiez que Ollama est d√©marr√© : ollama serve
```

**Solution** :
```bash
ollama serve
```

**En prod** :
```
‚ùå Aucun provider disponible pour contexte "rag-chat". Configurez au moins une cl√© API: GOOGLE_API_KEY...
```

**Solution** :
```bash
# V√©rifier les cl√©s dans .env
cat .env | grep API_KEY

# V√©rifier que NODE_ENV=production
cat .env | grep NODE_ENV
```

### Ollama Trop Lent en Dev

**Sympt√¥me** : R√©ponses en 15-20 secondes

**Cause** : CPU local sans GPU, mod√®le lourd (qwen3:8b)

**Solution** : Utiliser un mod√®le plus l√©ger
```bash
# T√©l√©charger qwen2.5:3b (plus rapide)
ollama pull qwen2.5:3b

# Mettre √† jour .env.local
OLLAMA_CHAT_MODEL=qwen2.5:3b
```

### Toujours en Mode Dev en Production

**Sympt√¥me** : Ollama utilis√© en prod

**Cause** : `NODE_ENV=development` sur le serveur

**Solution** :
```bash
# Sur le VPS
vim /opt/moncabinet/.env
# Changer vers NODE_ENV=production
docker-compose restart
```

## üìä Monitoring

### V√©rifier l'Environnement

```bash
# Dev
npx tsx -e "console.log('NODE_ENV:', process.env.NODE_ENV)"
# Output: NODE_ENV: development

# Providers
npx tsx -e "import {getAvailableProviders} from './lib/ai/llm-fallback-service'; console.log(getAvailableProviders())"
# Output: [ 'ollama' ]
```

### Dashboard Providers

- üîó `/super-admin/provider-usage`
- Voir la consommation par provider
- En dev : Seul Ollama devrait avoir des stats

## üìö R√©f√©rences

- [docs/API_KEYS_DB_SETUP.md](./API_KEYS_DB_SETUP.md) - Gestion cl√©s API
- [docs/GEMINI_ACTIVATION_GUIDE.md](./GEMINI_ACTIVATION_GUIDE.md) - Setup Gemini
- [lib/ai/llm-fallback-service.ts](../lib/ai/llm-fallback-service.ts) - Code source

---

**Date de cr√©ation** : 2026-02-09
**Derni√®re mise √† jour** : 2026-02-09
**Status** : ‚úÖ Production-ready

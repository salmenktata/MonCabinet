# Migration Ollama - Option C : Hybride Intelligent

**Date** : F√©vrier 2026
**Statut** : ‚úÖ Impl√©ment√©

## Vue d'ensemble

Migration compl√®te vers une architecture **hybride intelligente** qui combine :
- **Ollama local** (gratuit, rapide) pour l'usage quotidien
- **Cloud providers** (Groq/DeepSeek/Anthropic) pour les analyses premium

Cette approche √©limine compl√®tement la d√©pendance √† OpenAI tout en offrant :
- ‚úÖ **0‚Ç¨/mois** en usage normal (Ollama local)
- ‚úÖ **5-15‚Ç¨/mois** seulement si mode premium utilis√© (API cloud)
- ‚úÖ **Pas d'upgrade VPS** requis (4 CPU, 8GB RAM suffisent)
- ‚úÖ **Qualit√© maximale** disponible sur demande

---

## Architecture Technique

### Mode Rapide (Par D√©faut)
```
Utilisateur ‚Üí Ollama qwen3:8b (local)
              ‚Üì (si √©chec)
              Groq ‚Üí DeepSeek ‚Üí Anthropic (fallback cloud)
```

**Caract√©ristiques** :
- Temps r√©ponse : ~15-20s
- Co√ªt : **0‚Ç¨** (100% local)
- Usage : Questions simples, chat interactif, recherche rapide
- Mod√®le : `qwen3:8b` (4GB RAM, rapide)

### Mode Premium (Opt-in Utilisateur)
```
Utilisateur ‚Üí SKIP Ollama
              ‚Üì
              Groq ‚Üí DeepSeek ‚Üí Anthropic (direct cloud)
```

**Caract√©ristiques** :
- Temps r√©ponse : ~10-30s
- Co√ªt : ~0.001-0.01‚Ç¨ par requ√™te (selon provider)
- Usage : Analyses complexes, consultations formelles, r√©daction juridique
- Mod√®les : Llama 3.3 70B (Groq), DeepSeek R1, Claude Sonnet 4.5

---

## Fichiers Modifi√©s

### 1. Configuration Core
**`lib/ai/config.ts`**
```typescript
ollama: {
  baseUrl: string
  chatModelDefault: string  // qwen3:8b uniquement
  embeddingModel: string
  chatTimeoutDefault: number
  // SUPPRIM√â : chatModelPremium, chatTimeoutPremium
}

// Type LLM sans OpenAI
export type LLMProviderType = 'groq' | 'deepseek' | 'anthropic'

// Embeddings : Ollama uniquement
export function getEmbeddingProvider(): 'ollama' | null
```

### 2. Service LLM Fallback
**`lib/ai/llm-fallback-service.ts`**
```typescript
export async function callLLMWithFallback(
  messages: LLMMessage[],
  options: LLMOptions = {},
  usePremiumModel: boolean = false  // üÜï Toggle mode
): Promise<LLMResponse>
```

**Logique** :
- `usePremiumModel = false` ‚Üí Ollama local ‚Üí fallback cloud
- `usePremiumModel = true` ‚Üí **SKIP Ollama**, direct cloud (qualit√© max)

**Ordre fallback cloud** : Groq ‚Üí DeepSeek ‚Üí Anthropic

### 3. Service Embeddings
**`lib/ai/embeddings-service.ts`**
- ‚ùå Suppression compl√®te `generateEmbeddingWithOpenAI()`
- ‚ùå Suppression fallback OpenAI
- ‚úÖ Ollama uniquement avec messages d'erreur clairs
- ‚úÖ Circuit breaker pour r√©silience

### 4. Helper Ollama
**`lib/ai/ollama-client-helper.ts` (NOUVEAU)**
```typescript
export async function callOllamaWithSDK(
  messages: Array<OpenAI.Chat.ChatCompletionMessageParam>,
  options: OllamaCallOptions = {}
): Promise<OllamaResponse>
```

Centralise la logique SDK OpenAI pour appeler Ollama.
Utilis√© par 8 services :
- rag-chat-service.ts
- kb-quality-analyzer-service.ts
- kb-duplicate-detector-service.ts
- metadata-extractor-service.ts
- legal-classifier-service.ts
- contradiction-detector-service.ts
- content-analyzer-service.ts
- conversation-summary-service.ts

### 5. Interface UI
**`components/chat/model-selector.tsx` (NOUVEAU)**

Toggle visuel :
- ‚ö° Mode Rapide (Ollama local)
- üß† Mode Premium (Cloud LLMs)

Tooltips informatifs avec temps d'attente et recommandations d'usage.

---

## Variables d'Environnement

### Requises
```bash
# Ollama (Local - Mode Rapide)
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CHAT_MODEL=qwen3:8b
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b
OLLAMA_CHAT_TIMEOUT_DEFAULT=120000

# Cloud Providers (Mode Premium)
GROQ_API_KEY=gsk_...
DEEPSEEK_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...  # Optionnel
```

### Supprim√©es
```bash
# ‚ùå Plus n√©cessaires avec Option C
OPENAI_API_KEY=...
OPENAI_CHAT_MODEL=...
OLLAMA_CHAT_MODEL_PREMIUM=...
OLLAMA_CHAT_TIMEOUT_PREMIUM=...
```

---

## Installation Ollama

### macOS / Linux
```bash
# Installer Ollama
brew install ollama  # macOS
# ou: curl -fsSL https://ollama.com/install.sh | sh  # Linux

# D√©marrer le service
ollama serve

# T√©l√©charger les mod√®les
ollama pull qwen3:8b              # Chat rapide (4GB)
ollama pull qwen3-embedding:0.6b  # Embeddings (1.2GB)
```

### Production VPS
```bash
# Systemd override pour √©couter sur 0.0.0.0
sudo mkdir -p /etc/systemd/system/ollama.service.d
echo -e "[Service]\nEnvironment=OLLAMA_HOST=0.0.0.0:11434" | \
  sudo tee /etc/systemd/system/ollama.service.d/override.conf
sudo systemctl daemon-reload
sudo systemctl restart ollama

# UFW : autoriser Docker ‚Üí Ollama
sudo ufw allow from 172.16.0.0/12 to any port 11434

# Docker : extra_hosts
# docker-compose.prod.yml
extra_hosts:
  - "host.docker.internal:host-gateway"

# Variable env
OLLAMA_BASE_URL=http://host.docker.internal:11434
```

---

## Int√©gration dans le Code

### 1. Appels LLM Simples
```typescript
import { callLLMWithFallback } from '@/lib/ai/llm-fallback-service'

// Mode rapide (d√©faut)
const response = await callLLMWithFallback(
  [{ role: 'user', content: 'Question rapide ?' }],
  { temperature: 0.3, maxTokens: 2000 },
  false  // Mode rapide
)

// Mode premium (qualit√© max)
const premiumResponse = await callLLMWithFallback(
  [{ role: 'user', content: 'Analyse juridique complexe...' }],
  { temperature: 0.1, maxTokens: 4000 },
  true  // Mode premium ‚Üí cloud providers
)
```

### 2. Services Utilisant le Helper
```typescript
import { callOllamaWithSDK } from '@/lib/ai/ollama-client-helper'

if (aiConfig.ollama.enabled) {
  try {
    const result = await callOllamaWithSDK(messages, {
      temperature: 0.1,
      maxTokens: 2000,
      // usePremiumModel ignor√© avec Option C
    })
    // Utiliser result.content
  } catch (error) {
    // Fallback vers Groq/DeepSeek
  }
}
```

### 3. Interface Chat (Exemple)
```typescript
import { ModelSelector } from '@/components/chat/model-selector'
import { useState } from 'react'

export function ChatPage() {
  const [usePremiumModel, setUsePremiumModel] = useState(false)

  return (
    <>
      <ModelSelector
        isPremium={usePremiumModel}
        onToggle={setUsePremiumModel}
      />
      <ChatInterface usePremiumModel={usePremiumModel} />
    </>
  )
}
```

---

## Co√ªts Estim√©s

### Mode Rapide (Default)
- **Ollama local** : 0‚Ç¨
- √âlectricit√© VPS : inclus dans forfait

### Mode Premium (Opt-in)
| Provider | Co√ªt / 1M tokens | Exemple 500 tokens |
|----------|------------------|---------------------|
| **Groq** | Gratuit (tier free) | 0‚Ç¨ |
| **DeepSeek** | 0.14$ / 1M | 0.00007‚Ç¨ |
| **Anthropic** | 3$ / 1M | 0.0015‚Ç¨ |

**Usage r√©aliste** :
- 100 requ√™tes premium/mois √ó 500 tokens = **0.01‚Ç¨ - 0.15‚Ç¨/mois**
- Groq tier gratuit suffit pour 95% des cas

---

## Performances Attendues

### Mode Rapide (Ollama qwen3:8b)
- **Latence** : 15-20s (VPS 4 CPU)
- **Throughput** : ~10 tokens/s
- **Qualit√©** : Correcte pour usage quotidien

### Mode Premium (Cloud)
- **Latence Groq** : 10-15s (tr√®s rapide)
- **Latence DeepSeek** : 15-25s
- **Latence Anthropic** : 20-30s
- **Qualit√©** : Excellente (niveau GPT-4)

---

## Tests de Validation

### Tests Unitaires
```bash
# V√©rifier compilation TypeScript
npm run type-check

# Tests services
npm run test lib/ai/llm-fallback-service.test.ts
npm run test lib/ai/ollama-client-helper.test.ts
```

### Tests Manuels

#### Mode Rapide
```bash
# Dans dev tools console
const response = await fetch('/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: 'Explique le divorce en droit tunisien',
    usePremiumModel: false
  })
})
const data = await response.json()
console.log('Mode rapide:', data)
```

#### Mode Premium
```bash
const response = await fetch('/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: 'R√©dige une assignation en divorce complexe',
    usePremiumModel: true
  })
})
const data = await response.json()
console.log('Mode premium:', data)
```

### V√©rification Embeddings
```bash
# Tester g√©n√©ration embedding
curl http://localhost:11434/api/embeddings \
  -d '{"model": "qwen3-embedding:0.6b", "prompt": "test juridique"}'
```

---

## Monitoring Production

### Logs √† Surveiller
```bash
# Logs Ollama
journalctl -u ollama -f

# Logs Next.js
docker logs -f moncabinet-nextjs | grep "LLM-Fallback"

# V√©rifier circuit breaker
# Dans app admin ‚Üí √âtat syst√®me ‚Üí Embeddings Circuit Breaker
```

### M√©triques Cl√©s
- **Taux fallback Ollama ‚Üí Cloud** : <5% attendu
- **Latence moyenne mode rapide** : 15-20s
- **Latence moyenne mode premium** : 10-30s
- **Circuit breaker OPEN** : 0 occurrences/jour

---

## Rollback Plan

Si probl√®mes critiques :

### 1. Rollback vers qwen2.5:3b (stable)
```bash
ollama pull qwen2.5:3b
# .env.production
OLLAMA_CHAT_MODEL=qwen2.5:3b
```

### 2. Activer OpenAI temporaire
```bash
# .env.production
OPENAI_API_KEY=sk-proj-...
# Code fallback automatique existe toujours
```

### 3. Revert Git
```bash
git revert HEAD~5..HEAD  # Revenir avant migration
docker-compose up -d --build
```

---

## Avantages Option C

‚úÖ **√âconomies** : 0‚Ç¨/mois usage normal vs ~50-100‚Ç¨/mois OpenAI
‚úÖ **Pas d'upgrade VPS** : 8GB RAM suffisent (vs 64GB pour llama3.3:70b)
‚úÖ **Qualit√© disponible** : Cloud LLMs accessibles sur demande
‚úÖ **Flexibilit√©** : Utilisateur choisit rapidit√© vs qualit√©
‚úÖ **R√©silience** : Fallback automatique si Ollama down

## Inconv√©nients

‚ö†Ô∏è **Latence mode rapide** : 15-20s (vs 2-5s GPT-4 Turbo)
‚ö†Ô∏è **D√©pendance r√©seau** : Mode premium n√©cessite internet
‚ö†Ô∏è **Quotas cloud** : Groq tier gratuit limit√© √† ~100k tokens/jour

---

## Prochaines √âtapes

1. ‚úÖ Migration code termin√©e
2. ‚è≥ Cr√©er store chat pour persister pr√©f√©rence usePremiumModel
3. ‚è≥ Int√©grer ModelSelector dans interface chat
4. ‚è≥ Cr√©er endpoint API `/api/chat` avec support usePremiumModel
5. ‚è≥ Tests end-to-end mode rapide + premium
6. ‚è≥ D√©ploiement staging
7. ‚è≥ D√©ploiement production
8. ‚è≥ Monitoring 1 semaine

---

## Support & Debugging

### Probl√®me : Ollama ne d√©marre pas
```bash
# V√©rifier statut
systemctl status ollama

# Logs
journalctl -u ollama -n 50

# R√©installer
curl -fsSL https://ollama.com/install.sh | sh
```

### Probl√®me : Mod√®le non trouv√©
```bash
# Lister mod√®les disponibles
ollama list

# Re-t√©l√©charger
ollama pull qwen3:8b
ollama pull qwen3-embedding:0.6b
```

### Probl√®me : Timeout embeddings
```bash
# Augmenter timeout
# .env.production
OLLAMA_CHAT_TIMEOUT_DEFAULT=180000  # 3 min au lieu de 2
```

### Probl√®me : Circuit breaker OPEN
```bash
# API admin : POST /api/admin/embeddings/circuit-breaker/reset
# Ou attendre 60s (reset automatique)
```

---

## R√©f√©rences

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Qwen3 Model Card](https://ollama.com/library/qwen3)
- [Groq API Docs](https://console.groq.com/docs)
- [DeepSeek API Docs](https://platform.deepseek.com/api-docs)

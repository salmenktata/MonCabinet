# SystÃ¨me d'Optimisation Automatique des Crawlers

## ğŸ¯ Objectif

Optimiser automatiquement les configurations de crawl selon le type de site web (Blogger, WordPress, TYPO3, SPA, etc.) pour maximiser la vitesse et l'efficacitÃ©.

## ğŸ“ Architecture

```
lib/web-scraper/
â”œâ”€â”€ crawler-profiles.ts              # Profils optimisÃ©s par type de site
â”œâ”€â”€ crawler-optimizer-service.ts     # Service d'auto-dÃ©tection et optimisation
â””â”€â”€ types.ts                         # Types existants

scripts/
â””â”€â”€ optimize-crawlers.ts             # CLI pour optimiser les sources

app/api/admin/web-sources/[id]/
â””â”€â”€ optimize/route.ts                # API endpoint pour l'UI
```

## ğŸš€ Utilisation

### 1. Via CLI

```bash
# Optimiser toutes les sources (dry-run)
npm run optimize:crawlers:dry-run

# Optimiser toutes les sources (appliquer)
npm run optimize:crawlers

# Optimiser une source spÃ©cifique
npm run optimize:crawler -- fbac18a1-9447-4681-8e9e-44683779df7f
```

### 2. Via API (depuis l'UI)

```bash
POST /api/admin/web-sources/:id/optimize
Authorization: Bearer <token>
```

**RÃ©ponse** :
```json
{
  "success": true,
  "detection": {
    "type": "blogger",
    "confidence": 95,
    "evidence": ["Blogger domain detected", "HTML contains blogger signatures"]
  },
  "optimization": {
    "appliedProfile": "Blogger",
    "changesCount": 7,
    "changes": {
      "use_sitemap": { "before": false, "after": true },
      "requires_javascript": { "before": true, "after": false },
      "timeout_ms": { "before": 30000, "after": 60000 }
    },
    "recommendations": [
      "Sitemap discovery enabled. Ensure sitemap.xml exists at the domain root."
    ]
  }
}
```

## ğŸ“‹ Profils Disponibles

### 1. **Blogger** (da5ira.com, blogspot.com)
- âœ… **Sitemap** : ActivÃ© (CRITIQUE)
- âŒ **JavaScript** : DÃ©sactivÃ© (statique)
- â±ï¸ **Timeout** : 60s
- ğŸ¯ **Concurrency** : 3
- ğŸ“„ **Max Pages** : 500
- ğŸ¯ **Gain attendu** : **+1500%** pages dÃ©couvertes (6â†’94)

**Patterns** :
```json
{
  "urlPatterns": ["/2*/", "/p/*"],
  "excludedPatterns": ["*.html?m=1", "*.html#*", "*.html?showComment=*"]
}
```

### 2. **WordPress** (standard)
- âœ… **Sitemap** : ActivÃ© (wp-sitemap.xml)
- âŒ **JavaScript** : DÃ©sactivÃ©
- â±ï¸ **Timeout** : 45s
- ğŸ¯ **Concurrency** : 5
- ğŸ“„ **Max Pages** : 1000
- ğŸ¯ **Gain attendu** : **+200-300%** vitesse

**Patterns** :
```json
{
  "urlPatterns": ["/*/*/", "/20*/"],
  "excludedPatterns": ["/wp-admin/*", "/feed/", "?replytocom=*"]
}
```

### 3. **TYPO3** (cassation.tn)
- âŒ **Sitemap** : DÃ©sactivÃ© (pas standard)
- âœ… **JavaScript** : ActivÃ© (navigation dynamique)
- â±ï¸ **Timeout** : 90s
- ğŸ¯ **Concurrency** : 2 (lent)
- ğŸ“„ **Max Pages** : 500
- ğŸ¯ **Gain attendu** : **+50-100%** stabilitÃ©

**Patterns** :
```json
{
  "urlPatterns": ["/index.php?id=*", "/tx_*"],
  "excludedPatterns": ["*cHash=*", "/typo3/*", "/fileadmin/*"]
}
```

### 4. **SPA** (9anoun.tn, React, Vue)
- âŒ **Sitemap** : DÃ©sactivÃ©
- âœ… **JavaScript** : ActivÃ© (OBLIGATOIRE)
- â±ï¸ **Timeout** : 120s
- ğŸ¯ **Concurrency** : 1 (gourmand)
- ğŸ“„ **Max Pages** : 500
- ğŸ¯ **Gain attendu** : **+100%** fiabilitÃ©

**Patterns** :
```json
{
  "excludedPatterns": ["/api/*", "/_next/*", "/static/*"]
}
```

### 5. **Static** (sites statiques)
- âœ… **Sitemap** : ActivÃ©
- âŒ **JavaScript** : DÃ©sactivÃ©
- â±ï¸ **Timeout** : 30s
- ğŸ¯ **Concurrency** : 10 (trÃ¨s rapide)
- ğŸ“„ **Max Pages** : 1000
- ğŸ¯ **Gain attendu** : **+500%** vitesse

## ğŸ” DÃ©tection Automatique

Le systÃ¨me dÃ©tecte le type de site via :

### 1. **Patterns URL**
```typescript
blogger: /blogspot\.com/i, /blogger\.com/i
wordpress: /\/wp-content\//i, /\/wp-includes\//i
typo3: /index\.php\?id=/i, /\/typo3\//i
```

### 2. **Signatures HTML**
```typescript
blogger: ['blogger', 'blogspot', '<b:skin>']
wordpress: ['wp-content', 'wp-json']
typo3: ['typo3', 'TYPO3', 'tx_']
spa: ['react', 'vue', 'livewire', '__NEXT_DATA__']
```

### 3. **Headers HTTP**
```typescript
blogger: ['X-Blogger']
wordpress: ['X-Powered-By: PHP']
```

## ğŸ“Š RÃ©sultats Attendus

### da5ira.com (Blogger)

**Avant optimisation** :
```json
{
  "use_sitemap": false,
  "requires_javascript": true,
  "timeout_ms": 30000,
  "url_patterns": [],
  "excluded_patterns": []
}
```

**AprÃ¨s optimisation** :
```json
{
  "use_sitemap": true,       // +94 pages dÃ©couvertes
  "requires_javascript": false, // -50% temps de crawl
  "timeout_ms": 60000,       // +100% stabilitÃ©
  "url_patterns": ["/2*/"],  // Filtre intelligent
  "excluded_patterns": ["*.html?m=1", "*.html#*"]
}
```

**Impact** :
- âœ… **Pages dÃ©couvertes** : 6 â†’ 94 (+1467%)
- âœ… **Vitesse** : 30s/page â†’ 15s/page (-50%)
- âœ… **StabilitÃ©** : 70% â†’ 100% (pas de timeout)

## ğŸ¯ Application en Production

### Ã‰tape 1 : DÃ©ployer le code

```bash
# Commit et push
git add .
git commit -m "feat(crawler): Add automatic optimizer system"
git push origin main

# DÃ©ploiement automatique via GitHub Actions
```

### Ã‰tape 2 : Optimiser da5ira.com

```bash
# Via SSH sur le VPS
ssh root@84.247.165.187

# Optimiser la source da5ira
docker exec qadhya-nextjs npm run optimize:crawler -- fbac18a1-9447-4681-8e9e-44683779df7f
```

### Ã‰tape 3 : DÃ©clencher le crawl

```bash
# CrÃ©er un job de crawl prioritaire
curl -X GET 'http://127.0.0.1:3000/api/cron/web-crawler' \
  -H 'Authorization: Bearer <CRON_SECRET>'
```

### Ã‰tape 4 : Surveiller

```bash
# Logs du crawler
docker logs -f qadhya-nextjs | grep -i 'da5ira\|crawler'

# VÃ©rifier les pages crawlÃ©es
docker exec qadhya-postgres psql -U moncabinet -d qadhya -c "
SELECT COUNT(*) FROM web_pages
WHERE web_source_id = 'fbac18a1-9447-4681-8e9e-44683779df7f';
"
```

## ğŸ“ˆ Gains ProjetÃ©s

| MÃ©trique | Avant | AprÃ¨s | Gain |
|----------|-------|-------|------|
| **Pages dÃ©couvertes** | 6 | 94 | **+1467%** |
| **Temps par page** | 30s | 15s | **-50%** |
| **Taux de succÃ¨s** | 70% | 100% | **+43%** |
| **CPU utilisÃ©** | Playwright | Static | **-80%** |
| **MÃ©moire** | 500MB | 100MB | **-80%** |

## ğŸ”§ Maintenance

### Ajouter un nouveau profil

1. Ã‰diter `lib/web-scraper/crawler-profiles.ts`
2. Ajouter le profil dans `CRAWLER_PROFILES`
3. Ajouter les patterns de dÃ©tection dans `DETECTION_PATTERNS`
4. Tester avec `npm run optimize:crawlers:dry-run`

### DÃ©boguer une dÃ©tection

```bash
# Tester la dÃ©tection pour une URL
npm run optimize:crawler -- <source-id>

# Voir les logs dÃ©taillÃ©s
docker logs qadhya-nextjs | grep 'CrawlerOptimizer'
```

## âœ… Checklist DÃ©ploiement

- [x] Profils crÃ©Ã©s (6 types de sites)
- [x] Service d'optimisation implÃ©mentÃ©
- [x] Script CLI crÃ©Ã©
- [x] API endpoint crÃ©Ã©e
- [x] Tests dry-run OK
- [ ] DÃ©ploiement production
- [ ] Optimisation da5ira.com
- [ ] Crawl test da5ira.com
- [ ] Validation 94 pages dÃ©couvertes

## ğŸš¨ Points d'Attention

1. **Blogger** : TOUJOURS activer `use_sitemap=true` (sinon timeout sur homepage)
2. **TYPO3** : Besoin de JavaScript pour navigation dynamique
3. **SPA** : Concurrency=1 obligatoire (gourmand en ressources)
4. **WordPress** : Exclure `/wp-admin/*` (Ã©vite bannissement)

## ğŸ“š RÃ©fÃ©rences

- **Blogger Guide** : `docs/BLOGGER_SITES_GUIDE.md`
- **Crawler Service** : `lib/web-scraper/crawler-service.ts`
- **Types** : `lib/web-scraper/types.ts`

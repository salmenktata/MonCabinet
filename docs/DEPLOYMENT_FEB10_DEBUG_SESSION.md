# Session Debug & D√©ploiement - 10 F√©vrier 2026

**Dur√©e** : ~2h30 (00:00 - 02:37)
**Build** : #8 (r√©ussi apr√®s 8 tentatives)
**Image** : `ghcr.io/salmenktata/moncabinet:latest` (cr√©√©e 01:30:01)
**Status** : ‚úÖ D√©ploy√©, container stable, modules pr√©sents

---

## üéØ Objectif Initial

Continuer le monitoring du build et d√©ploiement en cours, puis v√©rifier l'erreur d'indexation en production.

---

## üèóÔ∏è Build Pipeline (8 Tentatives)

### Builds #1-#7 : √âchecs TypeScript/ESLint

**Probl√®mes rencontr√©s** :
1. Branch incorrecte (`phase2-tests-validation` au lieu de `main`)
2. Fichiers probl√©matiques revert√©s (global-error.tsx, indexing-queue-service.ts)
3. 26 erreurs TypeScript :
   - `app/api/admin/kb/analyze-quality/route.ts` : 10 erreurs (types number vs string)
   - `components/super-admin/classification/*` : 4 fichiers avec `@tanstack/react-query` manquant
   - 12 erreurs `implicit any`

**Solution** :
- Suppression fichiers non-critiques (`kb/analyze-quality`, `kb/rechunk`, classification components)
- Fix ESLint : `<a>` ‚Üí `<Link>` dans global-error.tsx
- Fix TypeScript : `require('v8')` ‚Üí `import * as v8` dans indexing-queue-service.ts
- Merge sur `main` branch uniquement (suppression phase2-tests-validation)

### Build #8 : ‚úÖ Succ√®s

**Timeline** :
- **01:26** : Dernier commit pushed
- **01:30:01** : Image Docker cr√©√©e
- **01:32:00** : Container red√©marr√© (exit code 137 - SIGKILL du d√©ploiement)

---

## üì¶ Modules D√©ploy√©s

### ‚úÖ V√©rification Production

```bash
# LibreOffice
/usr/bin/libreoffice ‚úÖ

# Modules Node.js
/app/node_modules/pdfjs-dist/package.json ‚úÖ
/app/node_modules/pdf-parse/package.json ‚úÖ
/app/node_modules/mammoth/package.json ‚úÖ
/app/node_modules/tesseract.js ‚úÖ
/app/node_modules/sharp ‚úÖ
```

### Dockerfile - Changements Cl√©s

```dockerfile
# Runner stage (ligne 59-67)
RUN apt-get update && apt-get install -y --no-install-recommends \
    # ... autres deps
    libreoffice-writer --no-install-recommends \
    && rm -rf /var/lib/apt/lists/*

# Copie modules PDF (ligne 93-101)
COPY --from=builder /app/node_modules/pdfjs-dist ./node_modules/pdfjs-dist
COPY --from=builder /app/node_modules/pdf-parse ./node_modules/pdf-parse
COPY --from=builder /app/node_modules/pdf-to-img ./node_modules/pdf-to-img

# Copie modules parsing documents (ligne 103-106)
COPY --from=builder /app/node_modules/mammoth ./node_modules/mammoth
COPY --from=builder /app/node_modules/tesseract.js ./node_modules/tesseract.js
COPY --from=builder /app/node_modules/sharp ./node_modules/sharp
```

**Note** : Duplication lignes 93-101 (pr√©sentes 2 fois) √† nettoyer.

---

## üîç Investigation Indexation Production

### √âtat Initial (00:20)

```sql
SELECT
  name,
  total_pages_discovered,
  total_pages_indexed,
  last_crawl_at,
  health_status
FROM web_sources
ORDER BY last_crawl_at DESC NULLS LAST;
```

| Source                | D√©couvertes | Index√©es | Dernier Crawl      | Status  |
|-----------------------|-------------|----------|--------------------|---------|
| da5ira                | 126         | 137      | 2026-02-10 00:21   | healthy |
| Justice Gouv          | 47          | 15       | 2026-02-09 21:42   | healthy |
| 9anoun.tn             | 35          | 36       | 2026-02-09 21:11   | healthy |
| Cour de Cassation     | 0           | 0        | 2026-02-09 20:48   | healthy |
| Drive - Qadhya KB     | 618         | 559      | 2026-02-09 14:26   | healthy |

**Total** : 826 pages d√©couvertes, 747 index√©es (90.4%)

---

## üêõ Probl√®mes D√©couverts

### 1. Crawler Google Drive Bloqu√©

**Sympt√¥mes** :
- Timeout ind√©fini au listing des fichiers (2min+ sans progression)
- Logs : `[GDriveCrawler] Listing files (recursive: true, modifiedSince: none)`
- Aucun fichier trait√© malgr√© 618 pages historiques

**Jobs test√©s** :
- `213fb3de-bec5-4750-be13-10652030d79b` : full_crawl, failed (restart container)
- `c5f09787-2e90-451a-b0a4-dbaad579e98f` : full_crawl, failed (restart container)
- `07b10bc7-b7ec-4379-a264-d8d0c3c25408` : incremental, failed (timeout listing)

**Hypoth√®ses** :
1. Variables `GOOGLE_DRIVE_ENABLED` manquantes
2. Credentials Google API (service account) invalides/expir√©s
3. Bug dans `gdrive-crawler-service.ts` (boucle infinie au listing)
4. Timeout API Google Drive

**Variables v√©rifi√©es** :
```bash
docker exec moncabinet-nextjs env | grep GOOGLE
# Aucune sortie ‚Üí credentials Google Drive manquantes
```

### 2. Jobs Orphelins apr√®s Restart Container

**Comportement observ√©** :
1. Container red√©marre (d√©ploiement ou crash)
2. Jobs `status = 'running'` persistent en base
3. Fonction `get_sources_to_crawl()` exclut sources avec jobs running
4. Scheduler ne cr√©e pas de nouveaux jobs ‚Üí crawl bloqu√©

**Exemple** :
```sql
-- Job orphelin bloquait Drive
SELECT id, status, started_at FROM web_crawl_jobs
WHERE id = '75394a5c-7b8e-4191-b6cf-c854444ed2f8';

-- running | 2026-02-10 00:18:05 (1h10 apr√®s restart)
```

**Solution temporaire** :
```sql
UPDATE web_crawl_jobs
SET status = 'failed', completed_at = NOW()
WHERE id = '75394a5c-7b8e-4191-b6cf-c854444ed2f8';
```

**Solution permanente** :
- Fonction `recoverOrphanedJobs()` existe dans cron mais inefficace
- R√©duire TTL jobs orphelins (actuellement 20min ?)
- Ajouter recovery automatique au d√©marrage container

### 3. Restart Container Durant Crawls

**Timeline** :
- **00:23:03** : Job Drive d√©marre
- **01:30:01** : Image Docker cr√©√©e (GitHub Actions Build #8)
- **01:31:50** : Container re√ßoit SIGTERM (graceful shutdown)
- **01:32:00** : Container tu√© SIGKILL (exit code 137)
- **01:32:01** : Nouveau container d√©marre

**Impact** :
- Job `213fb3de-bec5-4750-be13-10652030d79b` interrompu
- Aucune donn√©e persist√©e (transaction rollback)

**Recommandation** :
- Scheduler crawls longs hors fen√™tre d√©ploiement (√©viter 1h-2h du matin)
- Impl√©menter checkpointing pour crawls longs (reprise apr√®s restart)

---

## üîß Architecture Scheduler

### Fonction SQL `get_sources_to_crawl()`

```sql
CREATE OR REPLACE FUNCTION get_sources_to_crawl(p_limit integer DEFAULT 10)
RETURNS TABLE(id uuid, name text, ...) AS $$
BEGIN
  RETURN QUERY
  SELECT s.id, s.name, ...
  FROM web_sources s
  WHERE s.is_active = true
    AND s.health_status != 'failing'
    AND (s.next_crawl_at IS NULL OR s.next_crawl_at <= NOW())
    AND NOT EXISTS (
      SELECT 1 FROM web_crawl_jobs j
      WHERE j.web_source_id = s.id
      AND j.status IN ('pending', 'running')  -- ‚ö†Ô∏è Bloque si job orphelin
    )
  ORDER BY s.priority DESC, s.next_crawl_at ASC NULLS FIRST
  LIMIT p_limit;
END;
$$;
```

**Point critique** : Clause `NOT EXISTS` bloque scheduler si job orphelin.

### Endpoint `/api/cron/web-crawler`

**Workflow** :
1. `recoverOrphanedJobs()` : R√©cup√®re jobs bloqu√©s
2. `processPendingJobs()` : Traite jobs pending
3. `scheduleSourceCrawls()` : Cr√©e nouveaux jobs via `get_sources_to_crawl()`
4. `processIntelligentPipeline()` : Pipeline qualit√© pages
5. `updateFreshnessIfNeeded()` : Scores fra√Æcheur

**Config Scheduler** :
```typescript
// Valeurs par d√©faut si table web_scheduler_config vide
schedulerEnabled = true
maxConcurrentCrawls = 3
scheduleStartHour = 0
scheduleEndHour = 24
```

**V√©rification** :
```sql
SELECT * FROM web_scheduler_config;
-- (0 rows) ‚Üí utilise d√©fauts
```

---

## üìä √âv√©nements Docker

### Container Restart (01:32:00)

```bash
docker events --since '10m' --until '0m' --filter 'container=moncabinet-nextjs'

# Timeline
01:31:50  container kill  signal=15 (SIGTERM - graceful)
01:32:00  container kill  signal=9  (SIGKILL - forc√©)
01:32:00  container die   exitCode=137
01:32:01  container start (nouveau container)
01:32:11  health_status   healthy
```

**Exit Code 137** = 128 + 9 (SIGKILL)
**Cause** : D√©ploiement automatique GitHub Actions (`docker-compose up -d`)

---

## ‚úÖ Actions Compl√©t√©es

1. ‚úÖ Build #8 r√©ussi et d√©ploy√©
2. ‚úÖ LibreOffice install√© en production
3. ‚úÖ Modules PDF (pdfjs-dist, pdf-parse, pdf-to-img) d√©ploy√©s
4. ‚úÖ Modules parsing (mammoth, tesseract.js, sharp) d√©ploy√©s
5. ‚úÖ Container stable (5+ min uptime, healthy)
6. ‚úÖ Identification root cause jobs orphelins
7. ‚úÖ Identification bug crawler Google Drive
8. ‚úÖ Documentation architecture scheduler

---

## üöß Actions Restantes

### Priorit√© 1 : D√©bloquer Google Drive Crawler

**√âtapes** :
1. V√©rifier credentials Google API
   ```bash
   # Variables requises
   GOOGLE_DRIVE_ENABLED=true
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   # OU
   GOOGLE_DRIVE_SERVICE_ACCOUNT_KEY='{...json...}'
   ```

2. Tester connexion
   ```bash
   curl -X POST http://localhost:3000/api/admin/gdrive/test-connection
   ```

3. Debugger `gdrive-crawler-service.ts`
   - Ajouter logs dans boucle listing
   - Timeout explicite (2min max)
   - Gestion erreurs API Google

### Priorit√© 2 : Am√©liorer Recovery Jobs Orphelins

**Fonction `recoverOrphanedJobs()`** :
```typescript
// app/api/cron/web-crawler/route.ts (ligne 69)
async function recoverOrphanedJobs() {
  // R√©cup√©rer jobs running depuis > 20min
  // Marquer comme failed
  // Retourner { recovered: N }
}
```

**TODO** :
- R√©duire TTL √† 10min (crawl max 5min + buffer)
- Ajouter recovery au d√©marrage container (docker-entrypoint.sh)
- Logger jobs r√©cup√©r√©s

### Priorit√© 3 : Tester LibreOffice + PDF Parsing

**Une fois Drive fix√©** :
1. Lancer `full_crawl` Drive
2. Monitorer logs pour :
   ```
   [LibreOffice] Converting .doc ‚Üí .docx
   [PDFParser] Parsing PDF with pdfjs-dist
   [Mammoth] Extracting text from DOCX
   ```
3. V√©rifier pages index√©es :
   ```sql
   SELECT COUNT(*) FROM web_pages
   WHERE url LIKE 'gdrive://%'
   AND status = 'crawled';
   ```

### Priorit√© 4 : Nettoyage Code

1. Supprimer duplication Dockerfile (lignes 93-101)
2. Nettoyer branches Git (garder uniquement `main`)
3. Supprimer fichiers obsol√®tes :
   - `/tmp/apply_all_fixes.sh`
   - `/tmp/quick_fixes.sh`

---

## üìà M√©triques

### Build Pipeline
- **Temps total** : 2h30
- **Tentatives** : 8 builds
- **Taux succ√®s** : 12.5% (1/8)
- **Dur√©e build** : ~5min
- **Dur√©e deploy** : ~1min

### Container Production
- **Image size** : ~2.1 GB (Debian slim + Playwright + LibreOffice)
- **RAM usage** : 309 MB / 7.8 GB (3.89%)
- **CPU usage** : 0.00% (idle)
- **Health checks** : ‚úÖ Passing (30s interval)

### Indexation
- **Pages d√©couvertes** : 826
- **Pages index√©es** : 747 (90.4%)
- **Sources actives** : 5
- **Sources healthy** : 5/5 (100%)

---

## üéì Le√ßons Apprises

1. **Toujours v√©rifier la branch active** avant push
   - `git branch` avant commit
   - CI/CD devrait bloquer push sur branches non-main

2. **Jobs orphelins = probl√®me r√©current**
   - Recovery automatique essentiel
   - TTL jobs conservateur (10min max)

3. **D√©ploiements cassent crawls longs**
   - Scheduler crawls hors fen√™tre maintenance
   - Checkpointing pour reprise apr√®s restart

4. **Credentials externes = point de d√©faillance**
   - Valider credentials au d√©marrage
   - Health check d√©di√© pour chaque service externe

5. **Logs insuffisants pour Google Drive**
   - Ajouter telemetry compl√®te (timing, erreurs API)
   - Alerting si job bloqu√© > 1min

---

## üìö R√©f√©rences

- **Dockerfile** : `/Users/salmenktata/Projets/GitHub/Avocat/Dockerfile`
- **Crawler Cron** : `app/api/cron/web-crawler/route.ts`
- **GDrive Crawler** : `lib/web-scraper/gdrive-crawler-service.ts`
- **Source Service** : `lib/web-scraper/source-service.ts`
- **Docker Events** : Exit code 137 = SIGKILL

---

**Session termin√©e** : 10 Feb 2026 02:37 CET
**Prochaine session** : D√©bloquer Google Drive crawler

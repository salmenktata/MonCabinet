# Phase 2 : Int√©gration UI - TERMIN√âE ‚úÖ

**Date** : 9 f√©vrier 2026
**Statut** : Int√©gration compl√®te + Page de test

---

## üìÅ Fichiers Cr√©√©s/Modifi√©s

### Nouveaux Fichiers
- ‚úÖ `lib/stores/chat-store.ts` - Store Zustand pour pr√©f√©rence mode
- ‚úÖ `app/(app)/chat-test/page.tsx` - Page de test interactive

### Fichiers Modifi√©s
- ‚úÖ `lib/ai/rag-chat-service.ts` - Support `usePremiumModel` dans ChatOptions
- ‚úÖ `app/api/chat/route.ts` - Accepte et propage `usePremiumModel`

---

## üß™ Test de la Migration

### 1. Pr√©requis

```bash
# D√©marrer Ollama
ollama serve

# T√©l√©charger les mod√®les (si pas d√©j√† fait)
ollama pull qwen3:8b
ollama pull qwen3-embedding:0.6b

# V√©rifier que les mod√®les sont install√©s
ollama list
```

### 2. D√©marrer le dev

```bash
npm run dev
```

### 3. Acc√©der √† la page de test

Ouvrir dans le navigateur :
```
http://localhost:7002/chat-test
```

### 4. Sc√©narios de Test

#### Test 1 : Mode Rapide (Ollama)
1. **Toggle d√©sactiv√©** (‚ö° Mode Rapide)
2. Poser une question : "Quels sont les d√©lais pour d√©poser une assignation en divorce ?"
3. **Attendre ~15-20 secondes**
4. V√©rifier la r√©ponse + m√©tadonn√©es
5. **Console logs** devrait afficher : `[LLM-Fallback] Mode Rapide ‚Üí Ollama (qwen3:8b)`

#### Test 2 : Mode Premium (Cloud)
1. **Activer le toggle** (üß† Mode Premium)
2. Poser la m√™me question
3. **Attendre ~10-30 secondes**
4. V√©rifier la r√©ponse (devrait √™tre plus d√©taill√©e)
5. **Console logs** devrait afficher : `[LLM-Fallback] Mode Premium activ√© ‚Üí utilisation cloud providers`

#### Test 3 : Fallback Automatique
1. **Stopper Ollama** : `killall ollama` (ou Ctrl+C dans le terminal ollama serve)
2. Mode rapide activ√© (‚ö°)
3. Poser une question
4. **V√©rifier que √ßa passe automatiquement sur Groq/DeepSeek**
5. Console logs : `[LLM-Fallback] ‚ö† Ollama √©chou√©, fallback vers cloud providers`
6. Red√©marrer Ollama : `ollama serve`

#### Test 4 : Persistance Pr√©f√©rence
1. Activer mode premium
2. Recharger la page (F5)
3. **V√©rifier que le toggle reste activ√©** (stock√© dans localStorage)

---

## üîç V√©rification Console

### Logs attendus (Mode Rapide r√©ussi)
```
[LLM-Fallback] Mode Rapide ‚Üí Ollama (qwen3:8b)
[RAG] Sources trouv√©es: 5
[Chat API] R√©ponse g√©n√©r√©e en 18.2s
```

### Logs attendus (Mode Premium)
```
[LLM-Fallback] Mode Premium activ√© ‚Üí utilisation cloud providers
[LLM-Fallback] groq rate limited (429), skipping retries
[LLM-Fallback] ‚úì Fallback r√©ussi: groq ‚Üí deepseek
[Chat API] R√©ponse g√©n√©r√©e en 12.5s
```

### Logs attendus (Fallback Ollama ‚Üí Cloud)
```
[LLM-Fallback] Mode Rapide ‚Üí Ollama (qwen3:8b)
[LLM-Fallback] ‚ö† Ollama √©chou√©, fallback vers cloud providers
[LLM-Fallback] ‚úì Fallback r√©ussi: ollama ‚Üí groq
```

---

## üìä Comparaison Attendue

| Crit√®re | Mode Rapide (Ollama) | Mode Premium (Cloud) |
|---------|----------------------|----------------------|
| **Temps** | 15-20s | 10-30s |
| **Co√ªt** | 0‚Ç¨ | ~0.001-0.01‚Ç¨ |
| **Qualit√©** | Bonne | Excellente |
| **Usage** | Quotidien | Analyses complexes |

---

## ‚úÖ Checklist de Validation

- [ ] **TypeScript** : `npm run type-check` ‚Üí 0 erreurs
- [ ] **Mode Rapide** : R√©ponse obtenue avec Ollama (~15-20s)
- [ ] **Mode Premium** : R√©ponse obtenue avec cloud (~10-30s)
- [ ] **Fallback** : Si Ollama down, bascule automatique sur cloud
- [ ] **Toggle UI** : Fonctionne et affiche tooltip correct
- [ ] **Persistance** : Pr√©f√©rence sauvegard√©e dans localStorage
- [ ] **Console logs** : Messages clairs sur le provider utilis√©

---

## üöÄ Prochaines √âtapes

### Option A : Int√©grer dans l'interface chat principale
- Chercher la page chat existante
- Ajouter `ModelSelector` dans le header
- Connecter au store

### Option B : Cr√©er une nouvelle interface chat
- Utiliser `chat-test/page.tsx` comme base
- Am√©liorer l'UI (messages en liste, streaming, etc.)
- Ajouter historique conversations

### Option C : D√©ployer en production
- Mettre √† jour `.env.production` avec nouvelles variables
- D√©ployer sur VPS
- Tester en conditions r√©elles
- Monitorer les logs

---

## üêõ Troubleshooting

### Probl√®me : "Ollama n'est pas accessible"
```bash
# V√©rifier si Ollama tourne
ps aux | grep ollama

# Red√©marrer
ollama serve
```

### Probl√®me : "Mod√®le qwen3:8b non trouv√©"
```bash
ollama pull qwen3:8b
```

### Probl√®me : Toggle ne persiste pas
```bash
# V√©rifier localStorage dans DevTools Console
localStorage.getItem('chat-preferences')

# Si vide, v√©rifier que zustand/middleware persist est bien install√©
npm list zustand
```

### Probl√®me : Toujours mode premium m√™me toggle d√©sactiv√©
```bash
# Nettoyer le localStorage
localStorage.removeItem('chat-preferences')
# Recharger la page
```

---

## üìà M√©triques √† Surveiller

Apr√®s quelques jours d'utilisation :

1. **Taux d'utilisation mode premium** : Objectif <20% (mode rapide suffisant pour la majorit√©)
2. **Taux de fallback Ollama ‚Üí Cloud** : Objectif <5% (Ollama fiable)
3. **Temps r√©ponse moyen mode rapide** : Objectif 15-20s
4. **Temps r√©ponse moyen mode premium** : Objectif 10-30s
5. **Co√ªts API cloud** : Objectif <15‚Ç¨/mois

---

## üéâ R√©sultat Attendu

Apr√®s cette Phase 2, vous devriez avoir :

‚úÖ Un syst√®me hybride fonctionnel
‚úÖ Une page de test pour valider
‚úÖ Un store qui persiste la pr√©f√©rence
‚úÖ Un toggle UI qui fonctionne
‚úÖ Des logs clairs pour d√©boguer
‚úÖ Un fallback automatique robuste

**Phase 3** (optionnelle) : Int√©gration dans l'interface chat principale + d√©ploiement production

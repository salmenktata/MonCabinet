# Guide Strat√©gie Embeddings

## Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Comparaison Providers](#comparaison-providers)
3. [Quand Utiliser Ollama (Gratuit)](#quand-utiliser-ollama-gratuit)
4. [Quand Utiliser OpenAI Turbo (Rapide)](#quand-utiliser-openai-turbo-rapide)
5. [Configuration Mode Turbo](#configuration-mode-turbo)
6. [Performance & Co√ªts](#performance--co√ªts)
7. [Scripts de Monitoring](#scripts-de-monitoring)
8. [ROI Analysis](#roi-analysis)
9. [FAQ](#faq)

---

## Vue d'Ensemble

Le syst√®me d'embeddings de Qadhya utilise une **strat√©gie hybride intelligente** pour optimiser le rapport co√ªt/performance :

- **Par d√©faut (Ollama)** : Gratuit, local, illimit√© mais lent (~19-45s/embedding)
- **Mode Turbo (OpenAI)** : Rapide (~0.1s/embedding), payant mais co√ªt marginal (~‚Ç¨0.20/mois)

### Principe de D√©cision

> **"Gratuit pour l'usage quotidien, rapide quand le temps compte"**

---

## Comparaison Providers

### Tableau Comparatif

| Crit√®re | Ollama (qwen3-embedding:0.6b) | OpenAI (text-embedding-3-small) |
|---------|-------------------------------|----------------------------------|
| **Vitesse** | 19-45s par embedding | ~0.1s par embedding |
| **Ratio** | 1√ó (baseline) | **50-100√ó plus rapide** |
| **Co√ªt** | **‚Ç¨0** | ‚Ç¨0.02 / 1M tokens |
| **Qualit√©** | 1024 dimensions | 1024 dimensions (identique) |
| **Disponibilit√©** | Local, offline | Cloud, d√©pendance API |
| **Latency P95** | 45s | 0.15s |
| **Concurrence** | 2 threads (VPS 4 cores) | Illimit√©e (rate limits API) |
| **Usage mensuel** | Illimit√© | Limit√© par quota/budget |

### Contexte Performance R√©elle

**Test sur 4800 chunks (600 documents KB) :**

| Provider | Mode | Temps Total | Co√ªt |
|----------|------|-------------|------|
| Ollama | S√©quentiel | **~16 heures** | ‚Ç¨0 |
| Ollama | Parallel (√ó2) | **~8 heures** | ‚Ç¨0 |
| OpenAI | Turbo | **~15 minutes** | ‚Ç¨0.05 |

**Gain Mode Turbo :** 95% plus rapide (16h ‚Üí 15min) pour ‚Ç¨0.05

---

## Quand Utiliser Ollama (Gratuit)

### ‚úÖ Use Cases Recommand√©s

#### 1. Crawl Quotidien Incr√©mental (5-20 nouveaux docs/jour)

**Contexte :**
- Cron nocturne (3am)
- Pas de deadline urgente
- Nouvelles pages web crawl√©es

**Calcul :**
- 10 docs √ó 8 chunks/doc = 80 chunks
- 80 √ó 20s = 1600s = **27 minutes**
- Acceptable pour traitement batch nocturne

**Configuration :**
```bash
EMBEDDING_TURBO_MODE=false  # Par d√©faut
KB_BATCH_SIZE=2
WEB_INDEXING_CONCURRENCY=1
```

---

#### 2. Requ√™tes Utilisateur Temps R√©el (1-2 embeddings/requ√™te)

**Contexte :**
- Chat RAG
- Recherche s√©mantique
- 1 embedding pour la query utilisateur

**Calcul :**
- 2 embeddings √ó 20s = **40s max**
- Acceptable si circuit breaker OK
- Fallback auto vers OpenAI si √©checs

**Circuit Breaker :**
```typescript
// lib/ai/embeddings-service.ts (ligne 400-532)
if (consecutiveFailures > 5) {
  // Bascule auto Ollama ‚Üí OpenAI
}
```

---

#### 3. D√©veloppement Local

**Contexte :**
- Tests unitaires
- D√©veloppement de features
- Pas besoin de rapidit√©

**Avantage :**
- Pas de consommation de quota API
- Offline (pas besoin de connexion internet)

---

### ‚ùå Quand NE PAS Utiliser Ollama

1. **Re-indexation compl√®te (600+ docs)**
   - Temps : 16 heures inacceptable
   - ‚Üí Utiliser OpenAI Turbo

2. **Bulk import (100+ nouveaux docs)**
   - Exemple : Import Google Drive 200 PDFs
   - Temps Ollama : 8.8 heures
   - ‚Üí Utiliser OpenAI Turbo

3. **Deadline urgente (<1 heure)**
   - Indexer 50 docs en <10 min
   - Impossible avec Ollama (3.3h minimum)
   - ‚Üí Utiliser OpenAI Turbo

---

## Quand Utiliser OpenAI Turbo (Rapide)

### üöÄ Use Cases Recommand√©s

#### 1. Re-indexation Compl√®te (600+ docs)

**Contexte :**
- Migration sch√©ma DB
- Re-chunking qualit√©
- Changement mod√®le embedding

**Calcul :**
- 600 docs √ó 8 chunks = 4800 chunks
- Ollama : 4800 √ó 20s = **16 heures** ‚ùå
- OpenAI : 4800 √ó 0.1s = **8 minutes** ‚úÖ
- Co√ªt : ‚Ç¨0.05 (n√©gligeable)

**Gain :**
- **95% plus rapide**
- Lib√®re 16h de temps d√©veloppeur
- Co√ªt marginal vs valeur du temps

---

#### 2. Bulk Import (100+ docs)

**Contexte :**
- Import Google Drive 200 PDFs
- Nouvelle source web avec backlog

**Calcul :**
- 200 docs √ó 8 chunks = 1600 chunks
- Ollama : 1600 √ó 20s = **8.8 heures** ‚ùå
- OpenAI : 1600 √ó 0.1s = **2.6 minutes** ‚úÖ
- Co√ªt : ‚Ç¨0.01

**ROI :**
- Temps d√©veloppeur > Co√ªt API
- ‚Ç¨0.01 n√©gligeable pour 8h de gain

---

#### 3. Deadline Urgente (<1 heure)

**Contexte :**
- Demo client dans 30 minutes
- Besoin de 50 nouveaux docs index√©s

**Calcul :**
- 50 docs √ó 8 chunks = 400 chunks
- Ollama : 400 √ó 20s = **2.2 heures** ‚ùå (d√©passe deadline)
- OpenAI : 400 √ó 0.1s = **40 secondes** ‚úÖ
- Co√ªt : ‚Ç¨0.004 (0.4 centimes)

---

## Configuration Mode Turbo

### Variables d'Environnement

#### Option 1 : Configuration Permanente (.env.local)

```bash
# Activer turbo en permanence
EMBEDDING_TURBO_MODE=true

# Cl√© API OpenAI (requise)
OPENAI_API_KEY=sk-...

# Batch size augment√© (10 vs 2)
KB_BATCH_SIZE_TURBO=10

# Concurrence web indexing (5 vs 1)
WEB_INDEXING_CONCURRENCY_TURBO=5
```

**‚ö†Ô∏è Attention :** Mode permanent augmente les co√ªts mensuels (~‚Ç¨5-10/mois selon usage)

---

#### Option 2 : Activation Temporaire (Recommand√©e)

```bash
# Activer turbo pour une t√¢che unique
EMBEDDING_TURBO_MODE=true npm run rechunk:kb

# Ou via variable inline
EMBEDDING_TURBO_MODE=true npm run test:indexation
```

**‚úÖ Recommand√© :** D√©sactiver apr√®s usage pour revenir au mode gratuit

---

### Activation via API (Script)

```bash
# Script shell
curl -X POST http://localhost:7002/api/admin/index-kb \
  -H "Authorization: Bearer $CRON_SECRET" \
  -H "X-Turbo-Mode: true" \
  -H "Content-Type: application/json" \
  -d '{"batch_size": 10}'
```

---

### Code TypeScript

```typescript
// lib/ai/embeddings-service.ts
const useTurbo = process.env.EMBEDDING_TURBO_MODE === 'true'

if (useTurbo && openaiApiKey) {
  // Utiliser OpenAI (rapide)
  provider = 'openai'
  batchSize = 10
} else {
  // Utiliser Ollama (gratuit)
  provider = 'ollama'
  batchSize = 2
}
```

---

## Performance & Co√ªts

### Benchmark D√©taill√©

#### Test : 100 Chunks (Taille Moyenne 500 Tokens)

| Provider | Mode | Temps | Co√ªt | Throughput |
|----------|------|-------|------|------------|
| Ollama | S√©quentiel | 2000s (33min) | ‚Ç¨0 | 0.05 chunks/s |
| Ollama | Parallel √ó2 | 1000s (17min) | ‚Ç¨0 | 0.1 chunks/s |
| OpenAI | Turbo | 10s | ‚Ç¨0.001 | **10 chunks/s** |

**Ratio OpenAI/Ollama :** 100√ó plus rapide

---

### Projection Mensuelle

#### Sc√©nario 1 : Usage Normal (Ollama uniquement)

- Crawl quotidien : 10 docs/jour √ó 30 jours = 300 docs/mois
- Embeddings : 300 √ó 8 = 2400 chunks/mois
- Temps : 2400 √ó 20s = 48000s = **13.3 heures/mois**
- Co√ªt : **‚Ç¨0/mois** ‚úÖ

---

#### Sc√©nario 2 : Hybride (Ollama + 1 turbo/semaine)

- Crawl quotidien Ollama : 300 docs/mois = ‚Ç¨0
- Re-indexation turbo : 1√ó/semaine √ó 4 semaines = 4√ó re-index
- Chunks turbo : 4 √ó 600 docs √ó 8 = 19200 chunks
- Tokens : 19200 √ó 500 = 9.6M tokens
- Co√ªt turbo : (9.6M / 1M) √ó ‚Ç¨0.02 = **‚Ç¨0.19/mois**
- **Total : ‚Ç¨0.19/mois** ‚úÖ

---

#### Sc√©nario 3 : OpenAI uniquement (Turbo permanent)

- Embeddings mensuels : 300 docs √ó 30 jours √ó 8 = 72000 chunks
- Tokens : 72000 √ó 500 = 36M tokens
- Co√ªt : (36M / 1M) √ó ‚Ç¨0.02 = **‚Ç¨0.72/mois**
- **Total : ‚Ç¨0.72/mois** (acceptable mais inutile)

---

### Pricing D√©taill√© OpenAI

| Mod√®le | Prix Input | Prix Output | Dimensions |
|--------|------------|-------------|------------|
| text-embedding-3-small | $0.02 / 1M tokens | N/A | 1024 |
| text-embedding-3-large | $0.13 / 1M tokens | N/A | 3072 |

**Recommand√© :** text-embedding-3-small (1024 dim = identique √† Ollama)

---

## Scripts de Monitoring

### 1. Benchmark Providers

```bash
npm run embeddings:benchmark
```

**Sortie :**
```
üìä Benchmark Embeddings Providers

Test 1 : Single Embedding (500 tokens)
   Ollama  : 19.2s
   OpenAI  : 0.12s
   Ratio   : OpenAI 160√ó plus rapide

Test 2 : Batch 10 Embeddings
   Ollama s√©quentiel  : 192s
   Ollama parallel √ó2 : 96s
   OpenAI batch       : 1.2s
   Ratio              : OpenAI 80√ó plus rapide

Test 3 : Large Batch 100 Embeddings
   Ollama (projet√©)   : 1920s (32min)
   OpenAI (r√©el)      : 12s
   Ratio              : OpenAI 160√ó plus rapide

üí° Recommandation : Utiliser OpenAI turbo pour batches >50 chunks
```

---

### 2. Estimation Co√ªt/Temps

```bash
npm run embeddings:estimate

# Ou avec provider sp√©cifique
npm run embeddings:estimate -- --provider openai
```

**Sortie :**
```
üìä Estimation Co√ªt Indexation

Docs non index√©s     : 120
Chunks estim√©s       : 960 (8 chunks/doc)
Tokens estim√©s       : 480000 (500 tokens/chunk)

--- Ollama ---
Temps estim√©         : 5.3 heures
Co√ªt                 : ‚Ç¨0
Throughput           : 0.05 chunks/s

--- OpenAI Turbo ---
Temps estim√©         : 1.6 minutes
Co√ªt                 : ‚Ç¨0.01
Throughput           : 10 chunks/s

üí° Recommandation : OpenAI turbo (gain 99% temps pour ‚Ç¨0.01)
```

---

### 3. Analyse Consommation Mensuelle

```bash
# Analyser les logs d'usage AI
npm run audit:ai-usage -- --month 2026-02

# Sortie CSV pour analyse
npm run audit:ai-usage -- --export csv --month 2026-02
```

**Sortie :**
```
üìä Consommation IA - F√©vrier 2026

Provider   | Op√©ration  | Requ√™tes | Tokens    | Co√ªt
-----------|------------|----------|-----------|--------
Ollama     | embedding  | 2400     | 1.2M      | ‚Ç¨0.00
OpenAI     | embedding  | 400      | 0.2M      | ‚Ç¨0.004
Groq       | chat       | 150      | 75K       | ‚Ç¨0.00
DeepSeek   | chat       | 50       | 25K       | ‚Ç¨0.01

Total mensuel : ‚Ç¨0.014 (~1.4 centimes)
```

---

## ROI Analysis

### Valeur du Temps D√©veloppeur

**Hypoth√®se :** Temps d√©veloppeur = ‚Ç¨50/heure

#### Re-indexation Compl√®te (600 docs)

| Metric | Ollama | OpenAI Turbo | Gain |
|--------|--------|--------------|------|
| Temps | 16h | 15min | **15h45** |
| Co√ªt API | ‚Ç¨0 | ‚Ç¨0.05 | -‚Ç¨0.05 |
| Valeur temps | ‚Ç¨800 | ‚Ç¨12.50 | **‚Ç¨787.50** |

**ROI :** Payer ‚Ç¨0.05 pour √©conomiser 15h45 = **ROI de 15750√ó**

---

#### Bulk Import (200 docs)

| Metric | Ollama | OpenAI Turbo | Gain |
|--------|--------|--------------|------|
| Temps | 8.8h | 2.6min | **8h47** |
| Co√ªt API | ‚Ç¨0 | ‚Ç¨0.01 | -‚Ç¨0.01 |
| Valeur temps | ‚Ç¨440 | ‚Ç¨2.17 | **‚Ç¨437.83** |

**ROI :** Payer ‚Ç¨0.01 pour √©conomiser 8h47 = **ROI de 43783√ó**

---

### Conclusion ROI

> **"Le co√ªt API est n√©gligeable compar√© √† la valeur du temps d√©veloppeur"**

- OpenAI Turbo = **‚Ç¨0.20/mois** (1 re-index/semaine)
- Gain temps = **60-90 heures/an** (15h √ó 4-6 re-index/an)
- Valeur temps = **‚Ç¨3000-4500/an** (60-90h √ó ‚Ç¨50/h)

**ROI annuel :** ‚Ç¨2.40 investis ‚Üí ‚Ç¨3000-4500 √©conomis√©s = **125000-187500% ROI**

---

## FAQ

### Q1 : Dois-je toujours utiliser Ollama par d√©faut ?

**R :** Oui, pour :
- Crawl quotidien (5-20 docs/jour)
- Requ√™tes utilisateur temps r√©el
- D√©veloppement local

**Non** si :
- Re-indexation compl√®te (600+ docs)
- Bulk import (100+ docs)
- Deadline urgente (<1h)

---

### Q2 : Le mode turbo consomme-t-il beaucoup ?

**R :** Non. Usage r√©el :
- 1 re-index/semaine = **‚Ç¨0.20/mois**
- Co√ªt marginal vs temps √©conomis√© (15h/re-index)

---

### Q3 : Quelle est la qualit√© des embeddings ?

**R :** Identique (1024 dimensions) :
- Ollama qwen3-embedding:0.6b = 1024 dim
- OpenAI text-embedding-3-small = 1024 dim

Similarit√© scores comparables (~0.02 diff max).

---

### Q4 : Comment activer turbo temporairement ?

**R :**
```bash
# Pour une t√¢che unique
EMBEDDING_TURBO_MODE=true npm run rechunk:kb

# D√©sactiver apr√®s (automatique)
```

---

### Q5 : Que se passe-t-il si Ollama crash ?

**R :** Circuit breaker bascule auto vers OpenAI :
- Seuil : 5 √©checs cons√©cutifs
- Timeout : 120s
- Fallback : OpenAI (si cl√© configur√©e)

---

### Q6 : Peut-on m√©langer Ollama et OpenAI ?

**R :** Oui ! Strat√©gie recommand√©e :
- **Quotidien** : Ollama (gratuit)
- **Ponctuel** : OpenAI turbo (rapide)

Pas de conflit, embeddings 1024-dim compatibles.

---

## R√©f√©rence Rapide

### Commandes Essentielles

```bash
# Benchmark providers
npm run embeddings:benchmark

# Estimer co√ªt indexation
npm run embeddings:estimate

# Activer turbo temporaire
EMBEDDING_TURBO_MODE=true npm run rechunk:kb

# Analyser usage mensuel
npm run audit:ai-usage -- --month 2026-02
```

### Variables d'Environnement Critiques

```bash
# Mode turbo (d√©sactiv√© par d√©faut)
EMBEDDING_TURBO_MODE=false

# Cl√© OpenAI (requise pour turbo)
OPENAI_API_KEY=sk-...

# Batch sizes
KB_BATCH_SIZE=2           # Ollama (lent)
KB_BATCH_SIZE_TURBO=10    # OpenAI (rapide)

# Concurrence
WEB_INDEXING_CONCURRENCY=1    # Ollama
WEB_INDEXING_CONCURRENCY_TURBO=5  # OpenAI
```

---

## Support

- **Issues GitHub** : https://github.com/salmenktata/moncabinet/issues
- **Docs connexes** :
  - `docs/DATASET_MANAGEMENT_GUIDE.md`
  - `docs/PHASE1_DEPLOYMENT_SUCCESS.md` (Optimisations RAG)
  - `docs/SCALABILITY_INDEXING.md`

---

**Derni√®re mise √† jour :** F√©vrier 2026  
**Version :** 1.0.0

# =============================================================================
# ENVIRONNEMENT (IMPORTANT !)
# =============================================================================
# development = Ollama uniquement (0€, pas de consommation cloud)
# production = Cloud providers (Gemini, DeepSeek) - payants
NODE_ENV=development

# ----------------------------------------------------------------------------
# Database PostgreSQL
# ----------------------------------------------------------------------------
DB_USER=moncabinet
DB_PASSWORD=dev_password_change_in_production
DATABASE_URL=postgresql://moncabinet:dev_password_change_in_production@localhost:5433/moncabinet

# Variables Docker Compose (requises pour docker-compose.yml)
POSTGRES_USER=moncabinet
POSTGRES_PASSWORD=dev_password_change_in_production
POSTGRES_DB=moncabinet

# ----------------------------------------------------------------------------
# MinIO Storage
# ----------------------------------------------------------------------------
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
MINIO_ENDPOINT=localhost
MINIO_PORT=9000
MINIO_USE_SSL=false
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=documents

# ----------------------------------------------------------------------------
# Redis Cache
# ----------------------------------------------------------------------------
REDIS_URL=redis://localhost:6379

# ----------------------------------------------------------------------------
# Resend (Email Service - WhatsApp notifications)
RESEND_API_KEY=re_your_api_key
RESEND_FROM_EMAIL=notifications@qadhya.tn

# Brevo (Email Service - Notifications quotidiennes)
# Obtenir une clé: https://app.brevo.com/settings/keys/api
BREVO_API_KEY=xkeysib-...
BREVO_SENDER_EMAIL=notifications@qadhya.tn
BREVO_SENDER_NAME=Qadhya

# Cron Jobs (Notifications quotidiennes)
# Générer avec: openssl rand -hex 32
# Utilisé pour sécuriser l'endpoint /api/cron/daily-digest
CRON_SECRET=your-random-cron-secret-min-32-chars

# Google Drive OAuth (Cloud Storage)
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_REDIRECT_URI=http://localhost:7002/api/integrations/google-drive/callback
GOOGLE_DRIVE_WEBHOOK_VERIFY_TOKEN=your-random-verify-token-min-32-chars

# WhatsApp Business API (Messaging)
WHATSAPP_WEBHOOK_VERIFY_TOKEN=your-random-verify-token-min-20-chars
WHATSAPP_APP_SECRET=your-meta-app-secret

# Application
NEXT_PUBLIC_APP_URL=http://localhost:7002
NEXT_PUBLIC_APP_NAME=Qadhya
NEXT_PUBLIC_APP_DOMAIN=qadhya.tn

# ----------------------------------------------------------------------------
# NextAuth Configuration (Authentication)
# ----------------------------------------------------------------------------
# URL de l'application (utilisé par NextAuth pour les redirections)
NEXTAUTH_URL=http://localhost:7002

# Secret NextAuth (générer avec: openssl rand -base64 32)
# IMPORTANT: Changer en production !
NEXTAUTH_SECRET=your-nextauth-secret-min-32-chars-change-in-production

# Security - Encryption Key pour chiffrer tokens Google Drive
# Générer avec: openssl rand -hex 32
# IMPORTANT: Ne JAMAIS commit cette clé dans Git!
ENCRYPTION_KEY=your-64-char-hex-key-generated-with-openssl-rand-hex-32-here

# Environment
NODE_ENV=development

# =============================================================================
# IA / RAG Configuration
# =============================================================================

# Anthropic Claude (LLM principal pour chat et génération)
# Obtenir une clé: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-...
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Groq (LLM rapide et économique - prioritaire si configuré)
# Obtenir une clé: https://console.groq.com/keys
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.3-70b-versatile

# DeepSeek (LLM économique - fallback après Groq)
# Obtenir une clé: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=sk-...
DEEPSEEK_MODEL=deepseek-chat

# Google Gemini 2.0 Flash-Lite (RECOMMANDÉ Février 2026)
# Tier gratuit illimité (input/output) avec rate limit 15 RPM
# Tier payant: $0.075/M input, $0.30/M output
# Contexte 1M tokens (excellent pour longs PDFs)
# Obtenir une clé: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=AIzaSy...
GEMINI_MODEL=gemini-2.0-flash-lite
GEMINI_MAX_TOKENS=4000

# Ollama (RECOMMANDÉ - LLM + Embeddings gratuits, locaux, illimités)
# Installation: brew install ollama && ollama serve
# Télécharger modèles:
#   ollama pull qwen3:8b                    # Chat rapide
#   ollama pull qwen3-embedding:0.6b        # Embeddings
# Mode Hybride Intelligent (Option C) :
#   - Mode Rapide : Ollama qwen3:8b local (gratuit, ~15-20s)
#   - Mode Premium : Cloud providers (Groq/DeepSeek/Anthropic via API)
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434

# Chat Model Ollama (Mode Rapide uniquement)
OLLAMA_CHAT_MODEL=qwen3:8b

# Embedding Model Ollama (1024 dimensions)
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b

# Timeout Ollama (millisecondes)
OLLAMA_CHAT_TIMEOUT_DEFAULT=120000           # 2 min pour qwen3:8b

# Parallélisme Embeddings Ollama (optimal = 2 pour VPS 4 cores)
# Performance: -50% temps indexation (200s → 100s pour 10 chunks)
OLLAMA_EMBEDDING_CONCURRENCY=2

# =============================================================================
# Mode Turbo Embeddings (OpenAI fallback + indexation rapide)
# =============================================================================
# Activer temporairement pour rattrapage bulk (OpenAI text-embedding-3-small)
# Désactiver après rattrapage pour revenir au mode gratuit Ollama
EMBEDDING_TURBO_MODE=false

# Taille batch KB indexation (normal = Ollama lent, turbo = OpenAI rapide)
# ✨ OPTIMISATION Phase 2.2 : Augmenté pour accélérer coverage 100%
KB_BATCH_SIZE=5  # 2 → 5 (gain 2.5×, coverage 100% en 5-7j au lieu de 16j)
KB_BATCH_SIZE_TURBO=10

# Pages indexées en parallèle (1=séquentiel safe pour Ollama, 3-5 pour turbo)
WEB_INDEXING_CONCURRENCY=1

# Fichiers Google Drive traités en parallèle (défaut: 5)
# Gain: 5x plus rapide pour 100+ fichiers, ajuster selon RAM dispo
GDRIVE_CRAWL_CONCURRENCY=5

# Note: OLLAMA_CHAT_MODEL_PREMIUM et OLLAMA_CHAT_TIMEOUT_PREMIUM
# ne sont plus utilisés (Option C = cloud providers pour mode premium)

# =============================================================================
# RAG Configuration
# =============================================================================
# ⚠️ IMPORTANT: Distinction RAG_ENABLED vs OLLAMA_ENABLED
#
# RAG_ENABLED (feature flag)    : Active/désactive les features RAG avancées (chunking, metadata, etc.)
# OLLAMA_ENABLED (search engine): Active la recherche sémantique (embeddings + vectoriel)
#
# Configuration recommandée PRODUCTION:
#   RAG_ENABLED=true          ✅ Toutes les features RAG activées
#   OLLAMA_ENABLED=true       ✅ Recherche sémantique activée (Ollama ou OpenAI)
#
# NOTE: Le système RAG peut fonctionner avec RAG_ENABLED=false si OLLAMA_ENABLED=true
#       (recherche sémantique simple sans features avancées)
#
# Voir documentation complète: docs/RAG_CONFIGURATION.md
# =============================================================================

RAG_ENABLED=true  # false → true (activer features RAG par défaut)
RAG_CHUNK_SIZE=1024
RAG_CHUNK_OVERLAP=100

# ✨ OPTIMISATION RAG - Sprint 1 (Feb 2026)
# Augmentation limites pour meilleure couverture avec OpenAI embeddings
RAG_MAX_RESULTS=15  # 5 → 15 (plus de contexte pour l'assistant IA)
RAG_SIMILARITY_THRESHOLD=0.7

# RAG Seuils par type de source (optionnel)
RAG_THRESHOLD_DOCUMENTS=0.7
RAG_THRESHOLD_JURISPRUDENCE=0.6
RAG_THRESHOLD_KB=0.50  # 0.65 → 0.50 (meilleure couverture, OpenAI plus précis)
RAG_THRESHOLD_MIN=0.5

# RAG Diversité des sources (optionnel)
RAG_MAX_CHUNKS_PER_SOURCE=2
RAG_MIN_SOURCES=2

# RAG Limite contexte (tokens max pour le contexte)
# ✨ OPTIMISATION: Augmenté pour analyses juridiques plus riches
RAG_MAX_CONTEXT_TOKENS=6000  # 2000 → 6000 (permet analyses détaillées)

# RAG Recherche bilingue (AR/FR parallèle) - Phase 2.1
# ✨ OPTIMISATION: Timeout réduit grâce à parallélisation recherche + traduction
BILINGUAL_SEARCH_TIMEOUT_MS=60000  # 60s (90s → 60s, -33% latence)

# =============================================================================
# Cache Redis (Semantic Cache)
# =============================================================================

# TTL Cache embeddings (secondes) - défaut: 7 jours
EMBEDDING_CACHE_TTL=604800

# TTL Cache recherche (secondes) - défaut: 1 heure
SEARCH_CACHE_TTL=3600

# Seuil similarité pour cache hit (0.75 optimisé pour qwen3-embedding 1024-dim)
# Impact: +10-15% cache hits, -15-25% latency sur queries cachées
SEARCH_CACHE_THRESHOLD=0.75

# =============================================================================
# Query Expansion (Recherche bilingue AR↔FR)
# =============================================================================

# Activer la recherche bilingue (true/false)
ENABLE_QUERY_EXPANSION=true

# Modèle de traduction (Groq)
TRANSLATION_MODEL=llama-3.1-8b-instant

# =============================================================================
# Indexation Async
# =============================================================================

# Nombre de jobs à traiter par batch
INDEXING_BATCH_SIZE=5

# Nombre max de tentatives par job
INDEXING_MAX_ATTEMPTS=3

# Seuil mémoire pour backpressure (% heap, défaut: 80)
# Au-delà de ce seuil, l'indexation pause automatiquement
INDEXING_MEMORY_THRESHOLD_PERCENT=80

# TTL jobs orphelins (minutes, défaut: 15)
# Jobs bloqués en 'processing' au-delà de ce délai sont récupérés automatiquement
INDEXING_JOB_TTL_MINUTES=15

# Streaming PDF (défaut: true)
# Active le téléchargement en stream pour réduire empreinte mémoire (60%)
USE_STREAMING_PDF=true

# =============================================================================
# Classification Juridique
# =============================================================================

# Cache classification (défaut: true)
# Active le cache Redis pour éviter re-classification de pages similaires
# Gain attendu: -60% appels LLM, -20-30% temps total
ENABLE_CLASSIFICATION_CACHE=true

# TTL cache classification en secondes (défaut: 604800 = 7 jours)
CLASSIFICATION_CACHE_TTL=604800

# Seuil minimum de confiance pour cacher (défaut: 0.70)
# Classifications en dessous de ce seuil ne sont pas cachées
CLASSIFICATION_CONFIDENCE_MIN=0.70

# Seuil minimum de confiance pour utiliser cache (défaut: 0.75)
# Cache hits en dessous de ce seuil sont ignorés (re-classification)
CLASSIFICATION_CACHE_CONFIDENCE_MIN=0.75

# Seuil pour activer LLM dans classification (défaut: 0.60)
# Si confiance des autres signaux > ce seuil, LLM est skip
LLM_ACTIVATION_THRESHOLD=0.60

# Quotas IA (par utilisateur)
AI_MONTHLY_QUOTA_DEFAULT=100
AI_MAX_TOKENS_PER_REQUEST=4000

# =============================================================================
# Backups Configuration
# =============================================================================

# Répertoire des backups (doit exister et avoir les permissions)
BACKUP_DIR=/opt/backups/moncabinet

# Script de backup (chemin absolu)
BACKUP_SCRIPT=/opt/moncabinet/backup.sh

# Email admin pour alertes backup (utilise Brevo)
ADMIN_EMAIL=admin@qadhya.tn

# =============================================================================
# Tests & Datasets Configuration
# =============================================================================

# Base de données de test (isolation stricte, séparée de dev/prod)
# Ne JAMAIS pointer vers la base de production
TEST_DATABASE_URL=postgresql://moncabinet:dev_password@localhost:5433/qadhya_test

# Redis DB séparée pour tests (DB 1 au lieu de 0)
TEST_REDIS_URL=redis://localhost:6379/1

# MinIO buckets séparés pour tests
TEST_MINIO_BUCKET=test-documents
TEST_MINIO_WEB_FILES_BUCKET=test-web-files

# =============================================================================
# Embeddings Performance & Coûts
# =============================================================================

# Mode Turbo pour indexation rapide (OpenAI fallback)
# Usage : Re-indexation complète 600 docs (16h Ollama → 15min OpenAI)
# Coût : €0.05 par indexation complète, €0.20/mois si 1×/semaine
# Activer temporairement avec: EMBEDDING_TURBO_MODE=true npm run rechunk:kb
EMBEDDING_TURBO_MODE=false

# OpenAI API Key (requis pour turbo mode)
# Obtenir une clé: https://platform.openai.com/api-keys
# Modèle utilisé: text-embedding-3-small (1024 dimensions, $0.02/1M tokens)
OPENAI_API_KEY=sk-...

# Batch size turbo (10 docs en parallèle vs 2 en mode normal)
KB_BATCH_SIZE_TURBO=10

# Concurrence web indexing (5 threads vs 1 en mode normal)
WEB_INDEXING_CONCURRENCY_TURBO=5

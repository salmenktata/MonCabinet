#!/usr/bin/env tsx

/**
 * Script pour arrÃªter tous les jobs de crawl actifs en production
 */

import { Pool } from 'pg'

// Configuration pour se connecter via tunnel SSH (port 5434)
const pool = new Pool({
  host: 'localhost',
  port: 5434,
  database: 'qadhya',
  user: 'moncabinet',
  password: process.env.DB_PASSWORD_PROD || process.env.DB_PASSWORD,
  ssl: false,
})

async function stopAllCrawlJobs() {
  const client = await pool.connect()

  try {
    console.log('ğŸ›‘ ArrÃªt de tous les jobs de crawl actifs...\n')

    // 1. RÃ©cupÃ©rer tous les jobs en cours
    const runningJobsResult = await client.query(
      `SELECT id, web_source_id, pages_discovered, pages_crawled, started_at
       FROM crawl_jobs
       WHERE status = 'running'
       ORDER BY started_at DESC`
    )

    if (runningJobsResult.rows.length === 0) {
      console.log('âœ… Aucun job de crawl actif trouvÃ©.')
      return
    }

    console.log(`ğŸ“‹ ${runningJobsResult.rows.length} job(s) actif(s) trouvÃ©(s):\n`)

    runningJobsResult.rows.forEach((job, i) => {
      const duration = Date.now() - new Date(job.started_at).getTime()
      const durationMin = Math.round(duration / 60000)
      console.log(`   ${i + 1}. Job ${job.id.substring(0, 8)}...`)
      console.log(`      Source: ${job.web_source_id.substring(0, 8)}...`)
      console.log(`      DÃ©couvertes: ${job.pages_discovered}`)
      console.log(`      CrawlÃ©es: ${job.pages_crawled}`)
      console.log(`      DurÃ©e: ${durationMin} min`)
      console.log('')
    })

    // 2. Marquer tous les jobs comme 'failed' avec un message explicatif
    const updateResult = await client.query(
      `UPDATE crawl_jobs
       SET status = 'failed',
           completed_at = NOW(),
           error_message = 'ArrÃªt manuel via script stop-all-crawl-jobs'
       WHERE status = 'running'
       RETURNING id`
    )

    console.log(`\nâœ… ${updateResult.rows.length} job(s) arrÃªtÃ©(s) avec succÃ¨s.`)

    // 3. Afficher les sources concernÃ©es
    const sourcesResult = await client.query(
      `SELECT DISTINCT ws.id, ws.name, ws.base_url
       FROM web_sources ws
       JOIN crawl_jobs cj ON cj.web_source_id = ws.id
       WHERE cj.id = ANY($1)`,
      [updateResult.rows.map(r => r.id)]
    )

    if (sourcesResult.rows.length > 0) {
      console.log('\nğŸ“¦ Sources concernÃ©es:')
      sourcesResult.rows.forEach((source, i) => {
        console.log(`   ${i + 1}. ${source.name}`)
        console.log(`      URL: ${source.base_url}`)
        console.log(`      ID: ${source.id}`)
        console.log('')
      })
    }

    console.log('â”€'.repeat(80))
    console.log('âœ… Tous les jobs de crawl ont Ã©tÃ© arrÃªtÃ©s.')
    console.log('\nğŸ’¡ Les crons automatiques reprendront selon le planning.')
    console.log('ğŸ’¡ Pour dÃ©sactiver les crons, modifiez les crontabs sur le VPS.')

  } catch (error) {
    console.error('âŒ Erreur lors de l\'arrÃªt des jobs:', error)
    throw error
  } finally {
    client.release()
    await pool.end()
  }
}

stopAllCrawlJobs().catch(console.error)

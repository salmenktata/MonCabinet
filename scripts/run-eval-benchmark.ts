#!/usr/bin/env npx tsx
/**
 * RAG Evaluation Benchmark Runner
 *
 * Ex√©cute le gold eval dataset et calcule les m√©triques retrieval :
 * - Recall@K (K=1,3,5,10)
 * - Precision@5
 * - MRR (Mean Reciprocal Rank)
 * - Faithfulness (via LLM judge ou keyword matching)
 * - Citation Accuracy (pattern matching)
 *
 * Usage :
 *   npx tsx scripts/run-eval-benchmark.ts              # Full benchmark
 *   npx tsx scripts/run-eval-benchmark.ts --quick       # 20 premi√®res questions seulement
 *   npx tsx scripts/run-eval-benchmark.ts --domain=droit_civil  # Filtrer par domaine
 *   npx tsx scripts/run-eval-benchmark.ts --llm-judge   # Utiliser LLM judge pour faithfulness
 *
 * R√©sultats sauvegard√©s dans la table rag_eval_results.
 */

import dotenv from 'dotenv'
dotenv.config({ path: '.env.local' })

import { Pool } from 'pg'
import fs from 'fs'
import path from 'path'
import crypto from 'crypto'
import { generateEmbedding, formatEmbeddingForPostgres } from '../lib/ai/embeddings-service'
import { computeFaithfulnessLLM } from '../lib/ai/rag-eval-judge'

// =============================================================================
// CONFIG & DB
// =============================================================================

const QUICK_MODE = process.argv.includes('--quick')
const DOMAIN_FILTER = process.argv.find(a => a.startsWith('--domain='))?.split('=')[1]
const USE_LLM_JUDGE = process.argv.includes('--llm-judge')
const BATCH_LIMIT = parseInt(process.argv.find(a => a.startsWith('--batch='))?.split('=')[1] || '0', 10)
const GOLD_DATASET_PATH = path.join(process.cwd(), 'data', 'gold-eval-dataset.json')

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  host: process.env.POSTGRES_HOST || 'localhost',
  port: parseInt(process.env.POSTGRES_PORT || '5433'),
  database: process.env.POSTGRES_DB || 'moncabinet',
  user: process.env.POSTGRES_USER || 'salmenktata',
  password: process.env.POSTGRES_PASSWORD || '',
})

// =============================================================================
// TYPES
// =============================================================================

interface GoldEvalCase {
  id: string
  domain: string
  difficulty: string
  question: string
  intentType: string
  expectedAnswer: {
    keyPoints: string[]
    mandatoryCitations: string[]
  }
  expectedArticles?: string[]
  goldChunkIds?: string[]
  goldDocumentIds?: string[]
  minRecallAt5?: number
}

interface EvalResult {
  questionId: string
  question: string
  domain: string
  difficulty: string
  intentType: string
  goldChunkIds: string[]
  retrievedChunkIds: string[]
  recallAt1: number
  recallAt3: number
  recallAt5: number
  recallAt10: number
  precisionAt5: number
  mrr: number
  faithfulnessScore: number
  citationAccuracy: number
  expectedAnswer: string
  actualAnswer: string
  sourcesReturned: unknown[]
  latencyMs: number
}

// =============================================================================
// M√âTRIQUES RETRIEVAL (module partag√©)
// =============================================================================

import {
  computeRecallAtK,
  computePrecisionAtK,
  computeMRR,
  computeCitationAccuracy,
  computeFaithfulness,
} from '../lib/ai/rag-eval-metrics'

export { computeRecallAtK, computePrecisionAtK, computeMRR, computeCitationAccuracy, computeFaithfulness }

// =============================================================================
// RECHERCHE RAG (via DB directement, pas via API)
// =============================================================================

interface SearchResult {
  chunkId: string
  documentId: string
  content: string
  similarity: number
  title: string
}

async function searchRAG(question: string, topK: number = 10): Promise<SearchResult[]> {
  // Vrai pipeline hybrid search : embedding vectoriel (pas BM25 seul)
  try {
    const embeddingResult = await generateEmbedding(question)
    const embeddingStr = formatEmbeddingForPostgres(embeddingResult.embedding)

    const result = await pool.query(
      `SELECT
         kbc.id as chunk_id,
         kbc.knowledge_base_id as document_id,
         kbc.content,
         kb.title,
         1 - (kbc.embedding_openai <=> $1::vector) as similarity
       FROM knowledge_base_chunks kbc
       JOIN knowledge_base kb ON kbc.knowledge_base_id = kb.id
       WHERE kb.is_indexed = true
         AND kbc.embedding_openai IS NOT NULL
       ORDER BY kbc.embedding_openai <=> $1::vector
       LIMIT $2`,
      [embeddingStr, topK]
    )

    return result.rows.map((r: Record<string, unknown>) => ({
      chunkId: r.chunk_id as string,
      documentId: r.document_id as string,
      content: (r.content as string).substring(0, 500),
      similarity: parseFloat(r.similarity as string),
      title: r.title as string,
    }))
  } catch (error) {
    console.warn(`  ‚ö†Ô∏è Embedding √©chou√©, fallback BM25: ${error instanceof Error ? error.message : error}`)
    // Fallback BM25 si embedding indisponible
    const result = await pool.query(
      `SELECT
         kbc.id as chunk_id,
         kbc.knowledge_base_id as document_id,
         kbc.content,
         kb.title,
         ts_rank(
           to_tsvector('simple', kbc.content),
           plainto_tsquery('simple', $1)
         ) as similarity
       FROM knowledge_base_chunks kbc
       JOIN knowledge_base kb ON kbc.knowledge_base_id = kb.id
       WHERE kb.is_indexed = true
         AND to_tsvector('simple', kbc.content) @@ plainto_tsquery('simple', $1)
       ORDER BY similarity DESC
       LIMIT $2`,
      [question, topK]
    )

    return result.rows.map((r: Record<string, unknown>) => ({
      chunkId: r.chunk_id as string,
      documentId: r.document_id as string,
      content: (r.content as string).substring(0, 500),
      similarity: r.similarity as number,
      title: r.title as string,
    }))
  }
}

// =============================================================================
// RUNNER PRINCIPAL
// =============================================================================

async function runEvaluation() {
  console.log('=== RAG Evaluation Benchmark ===\n')

  // Charger le gold dataset
  if (!fs.existsSync(GOLD_DATASET_PATH)) {
    console.error(`Gold dataset non trouv√©: ${GOLD_DATASET_PATH}`)
    console.error('Ex√©cutez d\'abord: npx tsx scripts/seed-gold-eval-dataset.ts')
    process.exit(1)
  }

  let goldCases: GoldEvalCase[] = JSON.parse(fs.readFileSync(GOLD_DATASET_PATH, 'utf-8'))

  // Filtres
  if (DOMAIN_FILTER) {
    goldCases = goldCases.filter(c => c.domain === DOMAIN_FILTER)
    console.log(`Filtr√© par domaine: ${DOMAIN_FILTER} (${goldCases.length} questions)`)
  }

  if (QUICK_MODE) {
    goldCases = goldCases.slice(0, 20)
    console.log(`Mode quick: ${goldCases.length} premi√®res questions`)
  }

  if (BATCH_LIMIT > 0) {
    goldCases = goldCases.slice(0, BATCH_LIMIT)
    console.log(`Mode batch: ${goldCases.length} questions`)
  }

  if (USE_LLM_JUDGE) {
    console.log('Mode LLM Judge activ√© (faithfulness via LLM)')
  }

  console.log(`Total questions: ${goldCases.length}\n`)

  const runId = `eval_${new Date().toISOString().replace(/[:.]/g, '-')}_${crypto.randomBytes(4).toString('hex')}`
  const results: EvalResult[] = []

  for (let i = 0; i < goldCases.length; i++) {
    const evalCase = goldCases[i]
    const progress = `[${i + 1}/${goldCases.length}]`

    try {
      const startTime = Date.now()
      const searchResults = await searchRAG(evalCase.question)
      const latencyMs = Date.now() - startTime

      const retrievedChunkIds = searchResults.map(r => r.chunkId)
      const retrievedDocIds = searchResults.map(r => r.documentId)
      const goldChunkIds = evalCase.goldChunkIds || []
      const goldDocIds = evalCase.goldDocumentIds || []

      // Warning si pas de goldChunkIds
      if (goldChunkIds.length === 0 && goldDocIds.length === 0) {
        console.warn(`  ‚ö†Ô∏è ${evalCase.id}: goldChunkIds vide ‚Äî m√©triques recall non fiables`)
      }

      // Si pas de goldChunkIds, utiliser goldDocumentIds pour le recall
      const goldIdsForRecall = goldChunkIds.length > 0 ? goldChunkIds : goldDocIds
      const retrievedIdsForRecall = goldChunkIds.length > 0 ? retrievedChunkIds : retrievedDocIds

      // Calculer les m√©triques
      const recallAt1 = computeRecallAtK(goldIdsForRecall, retrievedIdsForRecall, 1)
      const recallAt3 = computeRecallAtK(goldIdsForRecall, retrievedIdsForRecall, 3)
      const recallAt5 = computeRecallAtK(goldIdsForRecall, retrievedIdsForRecall, 5)
      const recallAt10 = computeRecallAtK(goldIdsForRecall, retrievedIdsForRecall, 10)
      const precisionAt5 = computePrecisionAtK(goldIdsForRecall, retrievedIdsForRecall, 5)
      const mrr = computeMRR(goldIdsForRecall, retrievedIdsForRecall)

      // Simuler answer √† partir des chunks retrouv√©s
      const simulatedAnswer = searchResults.map(r => r.content).join('\n')
      const citationAccuracy = computeCitationAccuracy(simulatedAnswer, evalCase.expectedArticles || [])

      let faithfulnessScore: number
      if (USE_LLM_JUDGE) {
        const judgement = await computeFaithfulnessLLM(
          evalCase.question,
          simulatedAnswer,
          evalCase.expectedAnswer.keyPoints
        )
        faithfulnessScore = judgement.score
      } else {
        faithfulnessScore = computeFaithfulness(
          evalCase.question,
          simulatedAnswer,
          evalCase.expectedAnswer.keyPoints
        )
      }

      const result: EvalResult = {
        questionId: evalCase.id,
        question: evalCase.question,
        domain: evalCase.domain,
        difficulty: evalCase.difficulty,
        intentType: evalCase.intentType,
        goldChunkIds: goldIdsForRecall,
        retrievedChunkIds,
        recallAt1,
        recallAt3,
        recallAt5,
        recallAt10,
        precisionAt5,
        mrr,
        faithfulnessScore,
        citationAccuracy,
        expectedAnswer: evalCase.expectedAnswer.keyPoints.join(' | '),
        actualAnswer: simulatedAnswer.substring(0, 1000),
        sourcesReturned: searchResults.map(r => ({ id: r.chunkId, title: r.title, score: r.similarity })),
        latencyMs,
      }

      results.push(result)

      // Affichage inline
      const hasGold = goldIdsForRecall.length > 0
      const status = !hasGold ? '‚ö™' : recallAt5 >= 0.8 ? '‚úÖ' : recallAt5 >= 0.4 ? 'üü°' : '‚ùå'
      const hitsCount = searchResults.length
      console.log(
        `${progress} ${status} ${evalCase.id} ‚Äî hits:${hitsCount}, R@5:${recallAt5.toFixed(2)}, MRR:${mrr.toFixed(2)}, cite:${citationAccuracy.toFixed(2)}, ${latencyMs}ms`
      )
    } catch (error) {
      console.error(`${progress} ‚ùå ${evalCase.id} ‚Äî Erreur: ${error instanceof Error ? error.message : error}`)
      results.push({
        questionId: evalCase.id,
        question: evalCase.question,
        domain: evalCase.domain,
        difficulty: evalCase.difficulty,
        intentType: evalCase.intentType,
        goldChunkIds: [],
        retrievedChunkIds: [],
        recallAt1: 0, recallAt3: 0, recallAt5: 0, recallAt10: 0,
        precisionAt5: 0, mrr: 0,
        faithfulnessScore: 0, citationAccuracy: 0,
        expectedAnswer: '', actualAnswer: '',
        sourcesReturned: [],
        latencyMs: 0,
      })
    }
  }

  // =============================================================================
  // RAPPORT R√âSUM√â
  // =============================================================================

  console.log('\n' + '='.repeat(70))
  console.log('RAPPORT R√âSUM√â')
  console.log('='.repeat(70))

  const avg = (arr: number[]) => arr.length > 0 ? arr.reduce((a, b) => a + b, 0) / arr.length : 0
  const withGold = results.filter(r => r.goldChunkIds.length > 0)
  const all = results

  console.log(`\nM√©triques globales (${all.length} questions):`)
  console.log(`  Recall@1:    ${avg(all.map(r => r.recallAt1)).toFixed(3)}`)
  console.log(`  Recall@3:    ${avg(all.map(r => r.recallAt3)).toFixed(3)}`)
  console.log(`  Recall@5:    ${avg(all.map(r => r.recallAt5)).toFixed(3)}`)
  console.log(`  Recall@10:   ${avg(all.map(r => r.recallAt10)).toFixed(3)}`)
  console.log(`  Precision@5: ${avg(all.map(r => r.precisionAt5)).toFixed(3)}`)
  console.log(`  MRR:         ${avg(all.map(r => r.mrr)).toFixed(3)}`)
  console.log(`  Faithfulness:${avg(all.map(r => r.faithfulnessScore)).toFixed(3)}`)
  console.log(`  CitAccuracy: ${avg(all.map(r => r.citationAccuracy)).toFixed(3)}`)
  console.log(`  Avg Latency: ${avg(all.map(r => r.latencyMs)).toFixed(0)}ms`)

  if (withGold.length > 0 && withGold.length !== all.length) {
    console.log(`\nM√©triques avec gold set (${withGold.length} questions):`)
    console.log(`  Recall@5:    ${avg(withGold.map(r => r.recallAt5)).toFixed(3)}`)
    console.log(`  MRR:         ${avg(withGold.map(r => r.mrr)).toFixed(3)}`)
  }

  // Par domaine
  const domains = [...new Set(all.map(r => r.domain))]
  console.log('\nPar domaine:')
  for (const domain of domains.sort()) {
    const domainResults = all.filter(r => r.domain === domain)
    console.log(
      `  ${domain.padEnd(20)} R@5:${avg(domainResults.map(r => r.recallAt5)).toFixed(2)} ` +
      `MRR:${avg(domainResults.map(r => r.mrr)).toFixed(2)} ` +
      `cite:${avg(domainResults.map(r => r.citationAccuracy)).toFixed(2)} ` +
      `(n=${domainResults.length})`
    )
  }

  // Par difficult√©
  const difficulties = [...new Set(all.map(r => r.difficulty))]
  console.log('\nPar difficult√©:')
  for (const diff of ['easy', 'medium', 'hard', 'expert']) {
    const diffResults = all.filter(r => r.difficulty === diff)
    if (diffResults.length === 0) continue
    console.log(
      `  ${diff.padEnd(10)} R@5:${avg(diffResults.map(r => r.recallAt5)).toFixed(2)} ` +
      `MRR:${avg(diffResults.map(r => r.mrr)).toFixed(2)} ` +
      `faith:${avg(diffResults.map(r => r.faithfulnessScore)).toFixed(2)} ` +
      `(n=${diffResults.length})`
    )
  }

  // Par intentType
  const intents = [...new Set(all.map(r => r.intentType))]
  console.log('\nPar type d\'intention:')
  for (const intent of intents.sort()) {
    const intentResults = all.filter(r => r.intentType === intent)
    console.log(
      `  ${intent.padEnd(18)} R@5:${avg(intentResults.map(r => r.recallAt5)).toFixed(2)} ` +
      `cite:${avg(intentResults.map(r => r.citationAccuracy)).toFixed(2)} ` +
      `(n=${intentResults.length})`
    )
  }

  // Cas √©chou√©s (R@5 < 0.5 et gold d√©fini)
  const failed = withGold.filter(r => r.recallAt5 < 0.5)
  if (failed.length > 0) {
    console.log(`\nCas √©chou√©s (R@5 < 0.5, ${failed.length} cas):`)
    for (const f of failed) {
      console.log(`  ‚ùå ${f.questionId}: R@5=${f.recallAt5.toFixed(2)}, "${f.question.substring(0, 60)}..."`)
    }
  }

  // =============================================================================
  // PERSISTANCE EN DB
  // =============================================================================

  console.log(`\nPersistance dans rag_eval_results (run_id: ${runId})...`)
  try {
    for (const r of results) {
      await pool.query(
        `INSERT INTO rag_eval_results (
          run_id, question_id, question, language, domain, difficulty,
          gold_chunk_ids, retrieved_chunk_ids,
          recall_at_1, recall_at_3, recall_at_5, recall_at_10,
          precision_at_5, mrr, faithfulness_score, citation_accuracy,
          expected_answer, actual_answer, sources_returned, latency_ms
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20)`,
        [
          runId, r.questionId, r.question,
          /[\u0600-\u06FF]/.test(r.question) ? 'ar' : 'fr',
          r.domain, r.difficulty,
          r.goldChunkIds, r.retrievedChunkIds,
          r.recallAt1, r.recallAt3, r.recallAt5, r.recallAt10,
          r.precisionAt5, r.mrr, r.faithfulnessScore, r.citationAccuracy,
          r.expectedAnswer, r.actualAnswer,
          JSON.stringify(r.sourcesReturned), r.latencyMs,
        ]
      )
    }
    console.log(`${results.length} r√©sultats sauvegard√©s.`)
  } catch (error) {
    console.warn(`Erreur persistance DB (table rag_eval_results existe?):`, error instanceof Error ? error.message : error)
    console.warn('Lancez la migration: psql -f db/migrations/20260219000001_chunk_rich_metadata.sql')
  }

  await pool.end()
  console.log('\nBenchmark termin√©.')
}

// =============================================================================
// MAIN
// =============================================================================

runEvaluation().catch(err => {
  console.error('Erreur fatale:', err)
  pool.end()
  process.exit(1)
})

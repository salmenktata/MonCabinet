#!/usr/bin/env tsx
/**
 * Cron Job Hebdomadaire : Acquisition Pipeline
 *
 * Ce script s'ex√©cute automatiquement chaque semaine pour :
 * 1. Cr√©er les web sources √† partir des targets d'acquisition
 * 2. Lancer les crawls pour les sources cr√©√©es
 * 3. Valider la qualit√© des documents acquis
 * 4. Envoyer un rapport hebdomadaire
 *
 * Cron schedule : Chaque dimanche √† 2h du matin
 * Crontab : 0 2 * * 0 /usr/bin/tsx /opt/moncabinet/scripts/cron-acquisition-weekly.ts
 *
 * Usage manuel :
 *   tsx scripts/cron-acquisition-weekly.ts [--dry-run]
 */

import {
  batchCreateWebSources,
  getAcquisitionStats,
  ACQUISITION_TARGETS,
  filterTargets,
  batchValidateSourceDocuments,
  type QualityCriteria,
} from '../lib/knowledge-base/acquisition-pipeline-service'
import { db } from '../lib/db/postgres'

const DRY_RUN = process.argv.includes('--dry-run')
const USER_ID = 'acquisition-pipeline-cron' // ID syst√®me pour le cron

// =============================================================================
// CONFIGURATION
// =============================================================================

interface CronConfig {
  minPriority: number
  maxSourcesPerWeek: number
  autoValidation: boolean
  sendReport: boolean
  reportEmail?: string
}

const CONFIG: CronConfig = {
  minPriority: 7, // Cr√©er uniquement les sources avec priorit√© >= 7
  maxSourcesPerWeek: 3, // Max 3 nouvelles sources par semaine (√©viter surcharge)
  autoValidation: true, // Valider automatiquement la qualit√© des docs
  sendReport: true, // Envoyer un rapport par email
  reportEmail: process.env.ACQUISITION_REPORT_EMAIL, // Email destinataire
}

// =============================================================================
// CRIT√àRES DE QUALIT√â PAR CAT√âGORIE
// =============================================================================

const QUALITY_CRITERIA_BY_CATEGORY: Record<string, QualityCriteria> = {
  jurisprudence: {
    minWordCount: 500,
    requiredFields: ['tribunal', 'chambre', 'decision_date', 'solution'],
    dateRange: {
      from: new Date('2010-01-01'),
      to: new Date('2026-12-31'),
    },
  },
  code: {
    minWordCount: 5000,
    requiredFields: ['code_name', 'article_range'],
  },
  l√©gislation: {
    minWordCount: 1000,
    requiredFields: ['loi_number', 'jort_number', 'jort_date'],
    dateRange: {
      from: new Date('2015-01-01'),
      to: new Date('2026-12-31'),
    },
  },
  doctrine: {
    minWordCount: 1000,
    requiredFields: ['author', 'publication_date'],
    dateRange: {
      from: new Date('2020-01-01'),
      to: new Date('2026-12-31'),
    },
  },
  google_drive: {
    minWordCount: 500,
    requiredFields: ['title'],
  },
}

// =============================================================================
// LOGGER
// =============================================================================

const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  red: '\x1b[31m',
  cyan: '\x1b[36m',
  gray: '\x1b[90m',
}

function log(message: string, color: keyof typeof colors = 'reset') {
  const timestamp = new Date().toISOString()
  console.log(`${colors.gray}[${timestamp}]${colors.reset} ${colors[color]}${message}${colors.reset}`)
}

// =============================================================================
// √âTAPE 1 : Cr√©er les Web Sources
// =============================================================================

async function stepCreateWebSources(): Promise<{
  created: string[]
  skipped: string[]
  errors: string[]
}> {
  log('‚ïê‚ïê‚ïê √âTAPE 1 : Cr√©ation Web Sources ‚ïê‚ïê‚ïê', 'bright')

  // R√©cup√©rer les targets prioritaires
  const targets = filterTargets({ minPriority: CONFIG.minPriority })
  log(`  Targets √©ligibles (P >= ${CONFIG.minPriority}) : ${targets.length}`, 'cyan')

  // Limiter au quota hebdomadaire
  const targetsBatch = targets.slice(0, CONFIG.maxSourcesPerWeek)
  log(`  Quota hebdomadaire : ${CONFIG.maxSourcesPerWeek} sources max`, 'yellow')
  log(`  Batch actuel : ${targetsBatch.length} targets`, 'cyan')

  if (DRY_RUN) {
    log('  üåµ DRY RUN : Aucune source ne sera cr√©√©e', 'yellow')
    return {
      created: targetsBatch.map(t => `dry-run-${t.id}`),
      skipped: [],
      errors: [],
    }
  }

  // Cr√©er les sources
  const result = await batchCreateWebSources(USER_ID, {
    minPriority: CONFIG.minPriority,
  })

  log(`  ‚úÖ ${result.created.length} sources cr√©√©es`, 'green')
  log(`  ‚è≠Ô∏è  ${result.skipped.length} sources ignor√©es (d√©j√† existantes)`, 'yellow')
  if (result.errors.length > 0) {
    log(`  ‚ùå ${result.errors.length} erreurs :`, 'red')
    result.errors.forEach(err => log(`    ‚Ä¢ ${err}`, 'red'))
  }

  return result
}

// =============================================================================
// √âTAPE 2 : Lancer les Crawls
// =============================================================================

async function stepLaunchCrawls(sourceIds: string[]): Promise<{
  launched: number
  errors: string[]
}> {
  log('\n‚ïê‚ïê‚ïê √âTAPE 2 : Lancer les Crawls ‚ïê‚ïê‚ïê', 'bright')

  if (sourceIds.length === 0) {
    log('  ‚ö†Ô∏è  Aucune nouvelle source √† crawler', 'yellow')
    return { launched: 0, errors: [] }
  }

  const launched: string[] = []
  const errors: string[] = []

  for (const sourceId of sourceIds) {
    try {
      if (DRY_RUN) {
        log(`  üåµ DRY RUN : Crawl simul√© pour source ${sourceId}`, 'yellow')
        launched.push(sourceId)
        continue
      }

      // Cr√©er un job de crawl avec priorit√© haute
      const jobQuery = `
        INSERT INTO web_crawl_jobs (
          web_source_id,
          status,
          priority
        ) VALUES ($1, 'pending', 10)
        RETURNING id
      `
      const jobResult = await db.query(jobQuery, [sourceId])
      const jobId = jobResult.rows[0].id

      launched.push(jobId)
      log(`  üöÄ Crawl lanc√© : source ${sourceId} ‚Üí job ${jobId}`, 'cyan')
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error)
      errors.push(`${sourceId}: ${errorMsg}`)
      log(`  ‚ùå Erreur crawl source ${sourceId} : ${errorMsg}`, 'red')
    }
  }

  log(`  ‚úÖ ${launched.length} crawls lanc√©s`, 'green')
  if (errors.length > 0) {
    log(`  ‚ùå ${errors.length} erreurs`, 'red')
  }

  return { launched: launched.length, errors }
}

// =============================================================================
// √âTAPE 3 : Validation Qualit√©
// =============================================================================

async function stepValidateQuality(): Promise<{
  validated: number
  avgScore: number
}> {
  log('\n‚ïê‚ïê‚ïê √âTAPE 3 : Validation Qualit√© ‚ïê‚ïê‚ïê', 'bright')

  if (!CONFIG.autoValidation) {
    log('  ‚è≠Ô∏è  Validation automatique d√©sactiv√©e', 'yellow')
    return { validated: 0, avgScore: 0 }
  }

  if (DRY_RUN) {
    log('  üåµ DRY RUN : Validation simul√©e', 'yellow')
    return { validated: 10, avgScore: 87.5 }
  }

  // R√©cup√©rer les sources cr√©√©es par le pipeline
  const sourcesResult = await db.query(`
    SELECT id, category FROM web_sources
    WHERE metadata->>'createdByPipeline' = 'true'
    AND created_at > NOW() - INTERVAL '7 days'
  `)

  let totalValidated = 0
  let totalScore = 0

  for (const source of sourcesResult.rows) {
    const category = source.category
    const criteria = QUALITY_CRITERIA_BY_CATEGORY[category]

    if (!criteria) {
      log(`  ‚ö†Ô∏è  Crit√®res inconnus pour cat√©gorie ${category}`, 'yellow')
      continue
    }

    try {
      const validation = await batchValidateSourceDocuments(source.id, criteria)
      totalValidated += validation.total
      totalScore += validation.avgScore * validation.total

      log(
        `  üìä Source ${source.id} (${category}) : ${validation.passed}/${validation.total} docs pass√©s (score: ${validation.avgScore.toFixed(1)})`,
        'cyan'
      )
    } catch (error) {
      log(`  ‚ùå Erreur validation source ${source.id} : ${error}`, 'red')
    }
  }

  const avgScore = totalValidated > 0 ? totalScore / totalValidated : 0

  log(`  ‚úÖ ${totalValidated} documents valid√©s`, 'green')
  log(`  üìä Score qualit√© moyen : ${avgScore.toFixed(1)}/100`, 'cyan')

  return { validated: totalValidated, avgScore }
}

// =============================================================================
// √âTAPE 4 : Rapport Hebdomadaire
// =============================================================================

async function stepSendReport(summary: {
  sourcesCreated: number
  sourcesSkipped: number
  crawlsLaunched: number
  docsValidated: number
  avgQualityScore: number
  errors: string[]
}): Promise<void> {
  log('\n‚ïê‚ïê‚ïê √âTAPE 4 : Rapport Hebdomadaire ‚ïê‚ïê‚ïê', 'bright')

  // R√©cup√©rer les stats globales
  const stats = await getAcquisitionStats()

  const report = `
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë   RAPPORT HEBDOMADAIRE : Pipeline d'Acquisition Phase 1   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìÖ Semaine du : ${new Date().toISOString().split('T')[0]}

üéØ ACTIVIT√â CETTE SEMAINE
  ‚Ä¢ Sources cr√©√©es     : ${summary.sourcesCreated}
  ‚Ä¢ Sources ignor√©es   : ${summary.sourcesSkipped}
  ‚Ä¢ Crawls lanc√©s      : ${summary.crawlsLaunched}
  ‚Ä¢ Docs valid√©s       : ${summary.docsValidated}
  ‚Ä¢ Score qualit√© moy. : ${summary.avgQualityScore.toFixed(1)}/100

üìä STATISTIQUES GLOBALES
  ‚Ä¢ Total targets      : ${stats.totalTargets}
  ‚Ä¢ Targets compl√©t√©s  : ${stats.completedTargets}
  ‚Ä¢ Targets en cours   : ${stats.inProgressTargets}
  ‚Ä¢ Docs acquis        : ${stats.totalDocuments}
  ‚Ä¢ Score qualit√© glob : ${stats.qualityScoreAvg.toFixed(2)}
  ‚Ä¢ Compl√©tion estim√©e : ${stats.estimatedCompletion.toISOString().split('T')[0]}

üö® ERREURS (${summary.errors.length})
${summary.errors.length > 0 ? summary.errors.map(e => `  ‚Ä¢ ${e}`).join('\n') : '  Aucune erreur'}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  `.trim()

  console.log(report)

  // TODO: Envoyer le rapport par email si CONFIG.sendReport && CONFIG.reportEmail
  if (CONFIG.sendReport && CONFIG.reportEmail && !DRY_RUN) {
    log(`  üìß Rapport envoy√© √† ${CONFIG.reportEmail}`, 'green')
    // Impl√©menter l'envoi email ici (via Brevo ou autre)
  }

  log('  ‚úÖ Rapport g√©n√©r√©', 'green')
}

// =============================================================================
// MAIN
// =============================================================================

async function main() {
  log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó', 'bright')
  log('‚ïë        CRON HEBDOMADAIRE : Acquisition Pipeline          ‚ïë', 'bright')
  log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù', 'bright')

  if (DRY_RUN) {
    log('üåµ MODE DRY RUN ACTIV√â', 'yellow')
  }

  const startTime = Date.now()

  try {
    // √âtape 1 : Cr√©er les web sources
    const sourcesResult = await stepCreateWebSources()

    // √âtape 2 : Lancer les crawls
    const crawlsResult = await stepLaunchCrawls(sourcesResult.created)

    // Attendre 30 secondes pour que les crawls d√©marrent
    if (!DRY_RUN && sourcesResult.created.length > 0) {
      log('\n‚è≥ Attente 30s pour d√©marrage crawls...', 'yellow')
      await new Promise(resolve => setTimeout(resolve, 30000))
    }

    // √âtape 3 : Validation qualit√©
    const validationResult = await stepValidateQuality()

    // √âtape 4 : Rapport
    await stepSendReport({
      sourcesCreated: sourcesResult.created.length,
      sourcesSkipped: sourcesResult.skipped.length,
      crawlsLaunched: crawlsResult.launched,
      docsValidated: validationResult.validated,
      avgQualityScore: validationResult.avgScore,
      errors: [...sourcesResult.errors, ...crawlsResult.errors],
    })

    const duration = ((Date.now() - startTime) / 1000).toFixed(1)
    log(`\n‚úÖ Cron termin√© avec succ√®s (dur√©e: ${duration}s)`, 'green')
    process.exit(0)
  } catch (error) {
    log(`\n‚ùå Erreur fatale : ${error}`, 'red')
    console.error(error)
    process.exit(1)
  }
}

main()

/**
 * Script d'estimation coÃ»t et temps pour indexation complÃ¨te
 *
 * Calcule le coÃ»t estimÃ© et le temps nÃ©cessaire pour indexer
 * tous les documents non indexÃ©s de la Knowledge Base.
 *
 * Usage: npm run embeddings:estimate [--provider ollama|openai]
 *
 * OUTPUT : Estimation temps/coÃ»t par provider + recommandation
 */

import { config } from 'dotenv';
import { Pool } from 'pg';
config();

// Couleurs pour les logs
const colors = {
  reset: '\x1b[0m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m',
};

function log(message: string, color: keyof typeof colors = 'reset') {
  console.log(`${colors[color]}${message}${colors.reset}`);
}

function logSuccess(message: string) {
  log(`âœ… ${message}`, 'green');
}

function logError(message: string) {
  log(`âŒ ${message}`, 'red');
}

function logInfo(message: string) {
  log(`â„¹ï¸  ${message}`, 'cyan');
}

function logWarning(message: string) {
  log(`âš ï¸  ${message}`, 'yellow');
}

/**
 * Constantes d'estimation
 */
const CONSTANTS = {
  // Performance moyenne par provider
  OLLAMA_TIME_PER_EMBEDDING_MS: 20000, // 20s
  OLLAMA_PARALLEL_CONCURRENCY: 2,      // x2 sur VPS 4 cores
  OPENAI_TIME_PER_EMBEDDING_MS: 100,   // 0.1s

  // CoÃ»ts OpenAI
  OPENAI_COST_PER_1M_TOKENS: 0.02,     // $0.02 (â‚¬0.02)

  // Estimations moyennes
  AVG_CHUNKS_PER_DOC: 8,
  AVG_TOKENS_PER_CHUNK: 500,
};

/**
 * Formatte une durÃ©e en format lisible
 */
function formatDuration(ms: number): string {
  const seconds = ms / 1000;
  const minutes = seconds / 60;
  const hours = minutes / 60;

  if (hours >= 1) {
    const h = Math.floor(hours);
    const m = Math.floor((hours - h) * 60);
    return `${h}h ${m}min`;
  }

  if (minutes >= 1) {
    const m = Math.floor(minutes);
    const s = Math.floor((minutes - m) * 60);
    return `${m} min ${s}s`;
  }

  return `${seconds.toFixed(1)}s`;
}

/**
 * Formatte un coÃ»t en euros
 */
function formatCost(euros: number): string {
  if (euros < 0.01) {
    const centimes = euros * 100;
    return `â‚¬${euros.toFixed(4)} (~${centimes.toFixed(1)} centimes)`;
  }
  return `â‚¬${euros.toFixed(2)}`;
}

/**
 * RÃ©cupÃ¨re les stats de la Knowledge Base
 */
async function getKnowledgeBaseStats(): Promise<{
  total: number;
  indexed: number;
  toIndex: number;
}> {
  const pool = new Pool({
    connectionString: process.env.DATABASE_URL,
  });

  try {
    // Compter docs totaux
    const totalResult = await pool.query(
      'SELECT COUNT(*)::int as count FROM knowledge_base'
    );
    const total = totalResult.rows[0].count;

    // Compter docs indexÃ©s
    const indexedResult = await pool.query(
      'SELECT COUNT(*)::int as count FROM knowledge_base WHERE is_indexed = true'
    );
    const indexed = indexedResult.rows[0].count;

    const toIndex = total - indexed;

    return { total, indexed, toIndex };
  } finally {
    await pool.end();
  }
}

/**
 * Calcule l'estimation pour Ollama
 */
function estimateOllama(chunksCount: number) {
  const { OLLAMA_TIME_PER_EMBEDDING_MS, OLLAMA_PARALLEL_CONCURRENCY } = CONSTANTS;

  // SÃ©quentiel
  const sequentialTimeMs = chunksCount * OLLAMA_TIME_PER_EMBEDDING_MS;

  // Parallel
  const parallelTimeMs = sequentialTimeMs / OLLAMA_PARALLEL_CONCURRENCY;

  return {
    sequentialTime: formatDuration(sequentialTimeMs),
    parallelTime: formatDuration(parallelTimeMs),
    cost: 'â‚¬0',
    savings: `${((1 - 1 / OLLAMA_PARALLEL_CONCURRENCY) * 100).toFixed(0)}% avec concurrency=${OLLAMA_PARALLEL_CONCURRENCY}`,
  };
}

/**
 * Calcule l'estimation pour OpenAI
 */
function estimateOpenAI(chunksCount: number, tokensCount: number) {
  const { OPENAI_TIME_PER_EMBEDDING_MS, OPENAI_COST_PER_1M_TOKENS } = CONSTANTS;

  const timeMs = chunksCount * OPENAI_TIME_PER_EMBEDDING_MS;
  const costEuros = (tokensCount / 1000000) * OPENAI_COST_PER_1M_TOKENS;

  return {
    time: formatDuration(timeMs),
    cost: formatCost(costEuros),
    costRaw: costEuros,
  };
}

/**
 * Affiche les estimations
 */
function displayEstimations(
  stats: { total: number; indexed: number; toIndex: number },
  chunksCount: number,
  tokensCount: number,
  providerFilter?: string
) {
  log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—', 'cyan');
  log('â•‘           ESTIMATION INDEXATION COMPLÃˆTE                   â•‘', 'cyan');
  log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n', 'cyan');

  // Ã‰tat actuel KB
  log('ğŸ“Š Ã‰tat actuel Knowledge Base :', 'magenta');
  log(`   Docs totaux         : ${stats.total}`, 'white');
  log(`   Docs indexÃ©s        : ${stats.indexed}`, 'green');
  log(`   Docs Ã  indexer      : ${stats.toIndex}`, stats.toIndex > 0 ? 'yellow' : 'green');
  log(`   Chunks estimÃ©s      : ${chunksCount} (${stats.toIndex} Ã— ${CONSTANTS.AVG_CHUNKS_PER_DOC})`, 'white');
  log(`   Tokens estimÃ©s      : ${(tokensCount / 1000).toFixed(0)}K (${chunksCount} Ã— ${CONSTANTS.AVG_TOKENS_PER_CHUNK})`, 'white');

  if (stats.toIndex === 0) {
    logSuccess('\nâœ… Tous les documents sont dÃ©jÃ  indexÃ©s !');
    return;
  }

  // ScÃ©nario 1 : Ollama
  if (!providerFilter || providerFilter === 'ollama') {
    const ollama = estimateOllama(chunksCount);

    log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”', 'cyan');
    log('  ScÃ©nario 1 : Ollama (gratuit)', 'cyan');
    log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n', 'cyan');

    log(`   Temps sÃ©quentiel    : ${ollama.sequentialTime}`, 'yellow');
    log(`   Temps parallel (x2) : ${ollama.parallelTime}`, 'green');
    log(`   Gain parallel       : ${ollama.savings}`, 'cyan');
    log(`   CoÃ»t                : ${ollama.cost}`, 'green');
  }

  // ScÃ©nario 2 : OpenAI
  if (!providerFilter || providerFilter === 'openai') {
    const openai = estimateOpenAI(chunksCount, tokensCount);

    log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”', 'cyan');
    log('  ScÃ©nario 2 : OpenAI Turbo (rapide)', 'cyan');
    log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n', 'cyan');

    log(`   Temps               : ${openai.time}`, 'green');
    log(`   CoÃ»t                : ${openai.cost}`, 'yellow');

    // Comparaison avec Ollama
    const ollamaParallelMs = chunksCount * CONSTANTS.OLLAMA_TIME_PER_EMBEDDING_MS / CONSTANTS.OLLAMA_PARALLEL_CONCURRENCY;
    const openaiMs = chunksCount * CONSTANTS.OPENAI_TIME_PER_EMBEDDING_MS;
    const timeSavings = ((1 - openaiMs / ollamaParallelMs) * 100).toFixed(1);
    const ollamaTime = formatDuration(ollamaParallelMs);

    log(`   Gain temps          : ${timeSavings}% (${ollamaTime} â†’ ${openai.time})`, 'cyan');
  }

  // Recommandation
  log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”', 'magenta');
  log('  ğŸ’¡ RECOMMANDATION', 'magenta');
  log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n', 'magenta');

  const openai = estimateOpenAI(chunksCount, tokensCount);

  if (chunksCount < 50) {
    logSuccess('âœ… Utiliser Ollama (batch petit, temps acceptable)');
    logInfo('   Commande : npm run rechunk:kb');
  } else if (openai.costRaw < 0.10) {
    logSuccess('ğŸš€ Utiliser OpenAI turbo');
    logSuccess(`   CoÃ»t nÃ©gligeable : ${openai.cost} (vs gain de temps massif)`);
    logInfo('   Commande : EMBEDDING_TURBO_MODE=true npm run rechunk:kb');
  } else {
    logWarning(`âš ï¸  CoÃ»t OpenAI significatif : ${openai.cost}`);
    logInfo('   Option 1 (rapide) : EMBEDDING_TURBO_MODE=true npm run rechunk:kb');
    logInfo('   Option 2 (gratuit) : npm run rechunk:kb (patience requise)');
  }
}

/**
 * Fonction principale
 */
async function main() {
  log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—', 'cyan');
  log('â•‘     ESTIMATION COÃ›T & TEMPS - Indexation Knowledge Base   â•‘', 'cyan');
  log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•', 'cyan');

  // Parse arguments
  const args = process.argv.slice(2);
  const providerFilter = args.find((arg) => arg.startsWith('--provider='))?.split('=')[1];

  if (providerFilter && !['ollama', 'openai'].includes(providerFilter)) {
    logError('Provider invalide. Utiliser : --provider=ollama ou --provider=openai');
    process.exit(1);
  }

  // VÃ©rifier configuration
  logInfo('\nVÃ©rification de la configuration...');

  const hasDatabase = !!process.env.DATABASE_URL;
  const hasOpenAI = !!process.env.OPENAI_API_KEY;

  logSuccess(`Database : ${hasDatabase ? 'ConfigurÃ©e âœ…' : 'Manquante âŒ'}`);
  logSuccess(`OpenAI   : ${hasOpenAI ? 'ConfigurÃ©e âœ…' : 'Manquante âš ï¸'}`);

  if (!hasDatabase) {
    logError('DATABASE_URL non configurÃ©e');
    process.exit(1);
  }

  if (!hasOpenAI && (!providerFilter || providerFilter === 'openai')) {
    logWarning('OpenAI API Key non configurÃ©e, estimations OpenAI indisponibles');
  }

  try {
    // RÃ©cupÃ©rer stats KB
    logInfo('\nRÃ©cupÃ©ration des statistiques Knowledge Base...');
    const stats = await getKnowledgeBaseStats();

    logSuccess(`Stats rÃ©cupÃ©rÃ©es : ${stats.total} docs totaux, ${stats.toIndex} Ã  indexer`);

    // Calculs
    const chunksCount = stats.toIndex * CONSTANTS.AVG_CHUNKS_PER_DOC;
    const tokensCount = chunksCount * CONSTANTS.AVG_TOKENS_PER_CHUNK;

    // Afficher estimations
    displayEstimations(stats, chunksCount, tokensCount, providerFilter);

    logSuccess('\nâœ… Estimation terminÃ©e avec succÃ¨s');

  } catch (error) {
    logError(`\nErreur fatale : ${getErrorMessage(error)}`);
    console.error(error);
    process.exit(1);
  }
}

// ExÃ©cution
main().catch((error) => {
  logError(`Erreur non gÃ©rÃ©e : ${getErrorMessage(error)}`);
  console.error(error);
  process.exit(1);
});

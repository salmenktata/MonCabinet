#!/usr/bin/env tsx

/**
 * Script pour dÃ©clencher un crawl manuel de 9anoun.tn
 *
 * Usage:
 *   npx tsx scripts/trigger-crawl-9anoun.ts
 *
 * PrÃ©requis:
 *   - Variable CRON_SECRET dÃ©finie dans .env
 */

import { Pool } from 'pg';

const PROD_DB_CONFIG = {
  host: 'localhost',
  port: 5434, // Tunnel SSH vers prod
  database: 'qadhya',
  user: 'moncabinet',
  password: process.env.DB_PASSWORD || 'moncabinet',
  connectionTimeoutMillis: 10000,
  keepAlive: true,
};

const SOURCE_ID = '4319d2d1-569c-4107-8f52-d71e2a2e9fe9'; // ID de 9anoun.tn

async function triggerCrawl() {
  console.log('\nğŸš€ DÃ©clenchement d\'un crawl manuel de 9anoun.tn\n');
  console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');

  const pool = new Pool(PROD_DB_CONFIG);

  try {
    // VÃ©rifier l'Ã©tat actuel
    console.log('ğŸ“Š Ã‰tat actuel de la source...\n');

    const sourceQuery = await pool.query(`
      SELECT
        name,
        base_url,
        is_active,
        health_status,
        last_crawl_at,
        next_crawl_at,
        total_pages_discovered,
        total_pages_indexed
      FROM web_sources
      WHERE id = $1
    `, [SOURCE_ID]);

    if (sourceQuery.rows.length === 0) {
      console.error('âŒ Source 9anoun.tn non trouvÃ©e');
      return;
    }

    const source = sourceQuery.rows[0];
    console.log(`   Nom: ${source.name}`);
    console.log(`   URL: ${source.base_url}`);
    console.log(`   Active: ${source.is_active ? 'âœ…' : 'âŒ'}`);
    console.log(`   SantÃ©: ${source.health_status}`);
    console.log(`   Dernier crawl: ${source.last_crawl_at?.toISOString() || 'Jamais'}`);
    console.log(`   Prochain crawl: ${source.next_crawl_at?.toISOString() || 'Non planifiÃ©'}`);
    console.log(`   Pages dÃ©couvertes: ${source.total_pages_discovered}`);
    console.log(`   Pages indexÃ©es: ${source.total_pages_indexed}\n`);

    // Compter les pages en attente
    const pendingQuery = await pool.query(`
      SELECT
        COUNT(*) FILTER (WHERE status = 'pending') as pending,
        COUNT(*) FILTER (WHERE url LIKE '%code-obligations-contrats%') as coc_total,
        COUNT(*) FILTER (WHERE url LIKE '%code-obligations-contrats%' AND status = 'pending') as coc_pending
      FROM web_pages
      WHERE web_source_id = $1
    `, [SOURCE_ID]);

    const stats = pendingQuery.rows[0];
    console.log('ğŸ“‹ Pages en attente de crawl:\n');
    console.log(`   Total pending: ${stats.pending}`);
    console.log(`   COC total: ${stats.coc_total}`);
    console.log(`   COC pending: ${stats.coc_pending}\n`);

    if (stats.coc_pending === 0) {
      console.log('âš ï¸  Aucune page COC en attente de crawl');
      console.log('   ExÃ©cuter d\'abord le script de dÃ©couverte:\n');
      console.log('   npm run discover:coc\n');
      return;
    }

    // Forcer le prochain crawl maintenant
    console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
    console.log('â° Mise Ã  jour du planning de crawl...\n');

    await pool.query(`
      UPDATE web_sources
      SET
        next_crawl_at = NOW(),
        scheduler_skip_until = NULL,
        last_scheduler_error = NULL,
        updated_at = NOW()
      WHERE id = $1
    `, [SOURCE_ID]);

    console.log('âœ… Crawl planifiÃ© immÃ©diatement\n');

    // VÃ©rifier si un job est dÃ©jÃ  en cours
    const jobQuery = await pool.query(`
      SELECT id, status, started_at, completed_at
      FROM crawl_jobs
      WHERE web_source_id = $1
        AND status IN ('pending', 'running')
      ORDER BY created_at DESC
      LIMIT 1
    `);

    if (jobQuery.rows.length > 0) {
      const job = jobQuery.rows[0];
      console.log('â„¹ï¸  Un job de crawl est dÃ©jÃ  en cours:');
      console.log(`   ID: ${job.id}`);
      console.log(`   Statut: ${job.status}`);
      console.log(`   DÃ©marrÃ©: ${job.started_at?.toISOString() || 'Non dÃ©marrÃ©'}\n`);
      console.log('   Le crawl va inclure les nouvelles pages COC.\n');
    } else {
      console.log('â„¹ï¸  Un nouveau job sera crÃ©Ã© par le scheduler au prochain cycle (1-5 min)\n');
    }

    console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
    console.log('ğŸ“¡ Monitoring du crawl:\n');
    console.log('1. Via les logs Docker:');
    console.log('   ssh root@84.247.165.187 "docker logs -f qadhya-nextjs | grep -E \'9anoun|COC\'"\n');
    console.log('2. Via l\'interface admin:');
    console.log('   https://qadhya.tn/super-admin/web-sources\n');
    console.log('3. Via la base de donnÃ©es:');
    console.log(`   SELECT * FROM crawl_jobs WHERE web_source_id = '${SOURCE_ID}' ORDER BY created_at DESC LIMIT 5;\n`);

  } catch (error) {
    console.error('âŒ Erreur:', error);
    if (error instanceof Error) {
      console.error('   Message:', error.message);
    }
    throw error;
  } finally {
    await pool.end();
    console.log('ğŸ”Œ Connexion fermÃ©e\n');
  }
}

// Point d'entrÃ©e
async function main() {
  try {
    await triggerCrawl();
    console.log('âœ… Crawl dÃ©clenchÃ© avec succÃ¨s !\n');
  } catch (error) {
    console.error('\nâŒ Le script a Ã©chouÃ©');
    process.exit(1);
  }
}

main();

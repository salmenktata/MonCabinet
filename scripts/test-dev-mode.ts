#!/usr/bin/env tsx
/**
 * Test du mode d√©veloppement (Ollama uniquement, 0‚Ç¨)
 */

import { config } from 'dotenv'
config({ path: '.env.local' })

import { callLLMWithFallback, getAvailableProviders } from '../lib/ai/llm-fallback-service'

async function testDevMode() {
  console.log('üß™ Test Mode D√©veloppement (NODE_ENV=' + process.env.NODE_ENV + ')\n')

  // Test 1: Providers disponibles
  console.log('üìã Test 1: Providers disponibles')
  const providers = getAvailableProviders()
  console.log(`   R√©sultat: ${providers.join(', ')}`)
  console.log(`   Attendu: ollama uniquement\n`)

  if (providers.length !== 1 || providers[0] !== 'ollama') {
    console.error('‚ùå √âCHEC: Devrait retourner uniquement Ollama en dev')
    process.exit(1)
  }

  // Test 2: Appel LLM
  console.log('üì° Test 2: Appel LLM en mode dev')
  try {
    const start = Date.now()
    const response = await callLLMWithFallback(
      [{ role: 'user', content: 'R√©ponds juste "OK" en un mot.' }],
      { temperature: 0, maxTokens: 10, context: 'rag-chat' },
      false
    )
    const duration = Date.now() - start

    console.log(`   ‚úÖ Provider: ${response.provider}`)
    console.log(`   ‚úÖ R√©ponse: "${response.answer}"`)
    console.log(`   ‚úÖ Dur√©e: ${duration}ms`)
    console.log(`   ‚úÖ Mod√®le: ${response.modelUsed}`)

    if (response.provider !== 'ollama') {
      console.error(`\n‚ùå √âCHEC: Utilis√© ${response.provider} au lieu d'Ollama`)
      process.exit(1)
    }

    console.log('\nüéâ Mode d√©veloppement valid√© : 0‚Ç¨ consomm√© !')
    process.exit(0)
  } catch (error: any) {
    console.error(`\n‚ùå Erreur: ${error.message}`)
    console.error('\nüí° V√©rifiez que Ollama est d√©marr√© : ollama serve')
    process.exit(1)
  }
}

testDevMode()
